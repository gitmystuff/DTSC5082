{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC5082/blob/main/Interview_Prep_6/interview_prep_simulation_scenarios_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85448461",
      "metadata": {
        "id": "85448461"
      },
      "source": [
        "# ðŸ“ˆ Time Series, Forecasting & Agentic Pipelines\n",
        "\n",
        "Your Name\n",
        "\n",
        "NOT YOUR ID\n",
        "\n",
        "**Topics Covered:** Time Series Analysis Â· Autoregression Â· ARIMA Â· Prophet Â· Sklearn Lag Models Â· Smolagents Â· GROQ Â· HuggingFace\n",
        "\n",
        "**Libraries:** `yfinance` Â· `statsmodels` Â· `sklearn` Â· `prophet` Â· `smolagents` Â· `groq` Â· `huggingface_hub`\n",
        "\n",
        "> ðŸŽ¯ **Learning Goal:** By the end of this notebook you will have built a full forecasting pipeline â€” from raw financial data to a working AI agent that analyzes and narrates your results â€” skills directly applicable in industry interviews and production systems.\n",
        "\n",
        "---\n",
        "### ðŸ—ºï¸ Notebook Roadmap\n",
        "| Section | Topic | Style |\n",
        "|---------|-------|-------|\n",
        "| 1 | Environment Setup | Guided |\n",
        "| 2 | Data Acquisition & EDA | Fill-in-the-blank |\n",
        "| 3 | Stationarity, Decomposition & Autocorrelation | Fill-in-the-blank |\n",
        "| 4 | ARIMA with `statsmodels` | Fill-in-the-blank â†’ Complete |\n",
        "| 5 | Lag-Feature Regression with `sklearn` | Guided â†’ Complete |\n",
        "| 6 | Forecasting with `Prophet` | Complete + Discussion |\n",
        "| 7 | Agentic Pipeline with Smolagents + GROQ + HuggingFace | Complete + Discussion |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "* Edit Your Name\n",
        "* Automatic 0 if you include your student id or any other id\n",
        "* Name the file Your_Name.ipynb\n",
        "* Share the link with Anyone with link and Edit privileges\n",
        "* Get your Groq API key and HuggingFace token and put them in Colab secrets\n",
        "\n",
        "> **Setup task:** Sign up at [console.groq.com](https://console.groq.com) and [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) for free API keys."
      ],
      "metadata": {
        "id": "1OFnw9R6rsJc"
      },
      "id": "1OFnw9R6rsJc"
    },
    {
      "cell_type": "markdown",
      "id": "60507961",
      "metadata": {
        "id": "60507961"
      },
      "source": [
        "---\n",
        "## Section 1 â€” Environment Setup ðŸ› ï¸\n",
        "\n",
        "Before we do any data science, we need a clean, reproducible environment. This mirrors industry practice â€” every production ML project starts with dependency management.\n",
        "\n",
        "Run the cell below to install all required libraries. If you are on a shared server or Colab, this may take a minute.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28091980",
      "metadata": {
        "id": "28091980"
      },
      "outputs": [],
      "source": [
        "# Install all required packages\n",
        "# Run this cell first â€” then restart the kernel if prompted\n",
        "\n",
        "import subprocess, sys\n",
        "\n",
        "packages = [\n",
        "    \"yfinance\",\n",
        "    \"statsmodels\",\n",
        "    \"scikit-learn\",\n",
        "    \"prophet\",\n",
        "    \"smolagents\",\n",
        "    \"groq\",\n",
        "    \"huggingface_hub\",\n",
        "    \"plotly\",\n",
        "    \"pandas\",\n",
        "    \"numpy\",\n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "]\n",
        "\n",
        "for pkg in packages:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
        "\n",
        "print(\"âœ… All packages installed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24172327",
      "metadata": {
        "id": "24172327"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ Core Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Time series specific\n",
        "import yfinance as yf\n",
        "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Prophet\n",
        "from prophet import Prophet\n",
        "from prophet.plot import plot_plotly\n",
        "\n",
        "# API clients\n",
        "from groq import Groq\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# Smolagents\n",
        "from smolagents import tool, CodeAgent, InferenceClientModel\n",
        "\n",
        "# Display settings\n",
        "pd.set_option(\"display.float_format\", \"{:.4f}\".format)\n",
        "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"âœ… All imports successful.\")\n",
        "print(f\"   pandas  : {pd.__version__}\")\n",
        "print(f\"   numpy   : {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69470603",
      "metadata": {
        "id": "69470603"
      },
      "source": [
        "### ðŸ”‘ API Key Setup\n",
        "\n",
        "We are using two free API providers:\n",
        "\n",
        "- **GROQ** â€” blazing-fast LLM inference (free tier, no credit card required). Sign up at [console.groq.com](https://console.groq.com)\n",
        "- **HuggingFace** â€” free inference API for open-source models. Get your token at [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "\n",
        "> âš ï¸ **Never hardcode API keys in a notebook you share.** Use environment variables or Colab secrets in production. For this workshop we will enter them interactively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17478783",
      "metadata": {
        "id": "17478783"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "# â”€â”€ Set your API keys â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Option A: Enter interactively (recommended for class)\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    # os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your GROQ API key: \")\n",
        "    os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "if \"HF_TOKEN\" not in os.environ:\n",
        "    # os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your HuggingFace token: \")\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Option B: Use environment variables\n",
        "\n",
        "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
        "HF_TOKEN     = os.environ[\"HF_TOKEN\"]\n",
        "\n",
        "print(\"âœ… API keys loaded.\")\n",
        "print(\"   GROQ key starts with:\", GROQ_API_KEY[:8], \"...\")\n",
        "print(\"   HF   key starts with:\", HF_TOKEN[:8], \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71830855",
      "metadata": {
        "id": "71830855"
      },
      "source": [
        "---\n",
        "## Section 2 â€” Data Acquisition & Exploratory Analysis ðŸ“Š\n",
        "\n",
        "We will work with **Apple Inc. (AAPL)** daily closing prices â€” a real financial time series that is non-stationary, seasonally influenced, and rich with structure. This is exactly the kind of data you will encounter in quantitative finance, data engineering, and ML engineering roles.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Download the Data\n",
        "\n",
        "`yfinance` is a widely used open-source library that fetches market data from Yahoo Finance. It returns a `pandas DataFrame` â€” the standard data structure you will use in almost every data science role."
      ],
      "metadata": {
        "id": "BLMV-s9Pk0Tx"
      },
      "id": "BLMV-s9Pk0Tx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28370445",
      "metadata": {
        "id": "28370445"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 2.1 Download financial data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "TICKER    = \"AAPL\"       # Try \"SPY\", \"MSFT\", \"TSLA\" for variety\n",
        "START     = \"2018-01-01\"\n",
        "END       = \"2024-12-31\"\n",
        "FALLBACK  = \"aapl_fallback.csv\"   # Will be created below as backup\n",
        "\n",
        "try:\n",
        "    raw = yf.download(TICKER, start=START, end=END, auto_adjust=True, progress=False)\n",
        "    if raw.empty:\n",
        "        raise ValueError(\"Empty response from yfinance\")\n",
        "    raw.to_csv(FALLBACK)           # Cache locally as fallback\n",
        "    print(f\"âœ… Downloaded {len(raw)} rows from Yahoo Finance.\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  yfinance failed ({e}). Loading fallback CSV.\")\n",
        "    raw = pd.read_csv(FALLBACK, index_col=0, parse_dates=True)\n",
        "\n",
        "# Work with the closing price only\n",
        "df = raw[[\"Close\"]].copy()\n",
        "df.columns = [\"price\"]\n",
        "df.index = pd.to_datetime(df.index)\n",
        "df.index.name = \"date\"\n",
        "\n",
        "print(f\"\\nDataset shape : {df.shape}\")\n",
        "print(f\"Date range    : {df.index.min().date()} â†’ {df.index.max().date()}\")\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28822056",
      "metadata": {
        "id": "28822056"
      },
      "source": [
        "### 2.2 Fill-in-the-Blank: Basic Exploration\n",
        "\n",
        "> ðŸ“ **Exercise** â€” Complete the cells below. Replace every `___` with the correct value or method call.  \n",
        "> Hint: use pandas methods like `.describe()`, `.isnull()`, `.resample()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60244929",
      "metadata": {
        "id": "60244929"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 2.2a Summary statistics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# FILL IN: call the pandas method that returns count, mean, std, min, max, etc.\n",
        "\n",
        "summary = df[\"price\"].___()\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ 2.2b Check for missing values â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# FILL IN: count how many nulls are in the price column\n",
        "missing = df[\"price\"].___().___()\n",
        "df[\"price\"] = df[\"price\"].___()\n",
        "print(f\"Missing values: {missing}\")\n",
        "print(\"After fill:\", df[\"price\"].isnull().sum(), \"missing values remain.\")"
      ],
      "metadata": {
        "id": "dB8AsHewhKaY"
      },
      "id": "dB8AsHewhKaY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63623038",
      "metadata": {
        "id": "63623038"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 2.2c Monthly average price â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# FILL IN: resample to monthly frequency and compute the mean\n",
        "# Hint: df[\"price\"].resample(\"___\").mean()\n",
        "\n",
        "monthly = df[\"price\"].resample(___).___()\n",
        "print(monthly.tail(12))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18697085",
      "metadata": {
        "id": "18697085"
      },
      "source": [
        "### 2.3 Visualization\n",
        "\n",
        "Good visualization is the first thing any analyst does with a new time series. We want to see trend, volatility, and any obvious structural breaks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40183182",
      "metadata": {
        "id": "40183182"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 2.3 Full price history + 50-day & 200-day moving averages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "df[\"MA50\"]  = df[\"price\"].rolling(window=50).mean()\n",
        "df[\"MA200\"] = df[\"price\"].rolling(window=200).mean()\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
        "\n",
        "# Top panel â€” price with moving averages\n",
        "axes[0].plot(df.index, df[\"price\"], label=\"Daily Close\", alpha=0.7, linewidth=1)\n",
        "axes[0].plot(df.index, df[\"MA50\"],  label=\"50-day MA\",   linewidth=1.5, linestyle=\"--\")\n",
        "axes[0].plot(df.index, df[\"MA200\"], label=\"200-day MA\",  linewidth=1.5, linestyle=\"-.\")\n",
        "axes[0].set_title(f\"{TICKER} Daily Closing Price with Moving Averages\", fontsize=14)\n",
        "axes[0].set_ylabel(\"Price (USD)\")\n",
        "axes[0].legend()\n",
        "\n",
        "# Bottom panel â€” daily returns\n",
        "df[\"returns\"] = df[\"price\"].pct_change()\n",
        "axes[1].bar(df.index, df[\"returns\"], alpha=0.5, color=\"steelblue\", width=1)\n",
        "axes[1].axhline(0, color=\"black\", linewidth=0.8)\n",
        "axes[1].set_title(\"Daily Returns (%)\", fontsize=14)\n",
        "axes[1].set_ylabel(\"Return\")\n",
        "axes[1].xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"aapl_overview.png\", dpi=150)\n",
        "plt.show()\n",
        "print(\"Chart saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88490246",
      "metadata": {
        "id": "88490246"
      },
      "source": [
        "### ðŸ’¬ Discussion Question 2A\n",
        "> Look at the chart above.\n",
        "> 1. Can you identify any obvious trend? Is this series stationary? What does that mean for modeling?\n",
        "> 2. What do you notice about the distribution of daily returns â€” does it look normal? Why does this matter for financial models?\n",
        "> 3. The 50-day and 200-day moving average crossover is a classic trading signal (the \"Golden Cross\"). Can you spot one in the chart? Would you use this as a feature in a predictive model?\n",
        "\n",
        "**Write your answers below (double-click this cell to edit):**\n",
        "\n",
        "1.\n",
        "2.\n",
        "3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15068945",
      "metadata": {
        "id": "15068945"
      },
      "source": [
        "---\n",
        "## Section 3 â€” Stationarity, Decomposition & Autocorrelation ðŸ”¬\n",
        "\n",
        "This section covers the **diagnostic layer** of time series analysis â€” the work that happens before any model is fit. These concepts come up constantly in technical interviews.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "| Concept | Definition | Why It Matters |\n",
        "|---------|-----------|----------------|\n",
        "| **Stationarity** | Statistical properties (mean, variance) are constant over time | Most classical models assume it |\n",
        "| **Decomposition** | Splitting a series into Trend + Seasonality + Residual | Reveals structure, aids model selection |\n",
        "| **Autocorrelation (ACF)** | Correlation of a series with its own lagged values | Tells you how far back to look |\n",
        "| **Partial Autocorrelation (PACF)** | ACF after removing shorter-lag effects | Helps select AR order |\n",
        "| **ADF Test** | Augmented Dickey-Fuller â€” formal test for unit root (non-stationarity) | p < 0.05 â†’ stationary |\n",
        "\n",
        "### 3.1 Fill-in-the-Blank: Stationarity Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57343191",
      "metadata": {
        "id": "57343191"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 3.1 Augmented Dickey-Fuller Test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# The ADF test checks: H0 = series has a unit root (non-stationary)\n",
        "#                      H1 = series is stationary\n",
        "# If p-value < 0.05 â†’ reject H0 â†’ series is stationary\n",
        "\n",
        "def adf_report(series, label=\"Series\"):\n",
        "    \"\"\"Run ADF test and print a clean report.\"\"\"\n",
        "    result = adfuller(series.dropna())\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"ADF Test: {label}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"  Test Statistic : {result[0]:.4f}\")\n",
        "    print(f\"  p-value        : {result[1]:.6f}\")\n",
        "    print(f\"  Lags Used      : {result[2]}\")\n",
        "    print(f\"  Critical Values:\")\n",
        "    for key, val in result[4].items():\n",
        "        print(f\"    {key}: {val:.4f}\")\n",
        "    conclusion = \"âœ… STATIONARY (reject H0)\" if result[1] < 0.05 else \"âŒ NON-STATIONARY (fail to reject H0)\"\n",
        "    print(f\"  Conclusion     : {conclusion}\")\n",
        "    return result[1]\n",
        "\n",
        "# FILL IN: test the raw price series\n",
        "p_raw = adf_report(df[___], label=\"AAPL Raw Price\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42067724",
      "metadata": {
        "id": "42067724"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 3.2 Make the series stationary through differencing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# First-order differencing: removes trend by computing day-over-day change\n",
        "# This is the \"I\" in ARIMA (Integrated)\n",
        "\n",
        "# FILL IN: compute the first difference of the price column\n",
        "df[\"price_diff\"] = df[\"price\"].___(periods=___)   # Hint: .diff(1)\n",
        "\n",
        "p_diff = adf_report(df[\"price_diff\"].dropna(), label=\"AAPL First Difference\")\n",
        "\n",
        "print(f\"\\nConclusion: Raw series p={p_raw:.4f} vs Differenced series p={p_diff:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47312339",
      "metadata": {
        "id": "47312339"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 3.3 Seasonal Decomposition â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# We use an additive model: price = Trend + Seasonality + Residual\n",
        "# period=252 â‰ˆ trading days in a year\n",
        "\n",
        "decomp = seasonal_decompose(df[\"price\"].dropna(), model=\"additive\", period=252)\n",
        "\n",
        "fig, axes = plt.subplots(4, 1, figsize=(14, 10), sharex=True)\n",
        "components = {\n",
        "    \"Observed\"   : decomp.observed,\n",
        "    \"Trend\"      : decomp.trend,\n",
        "    \"Seasonal\"   : decomp.seasonal,\n",
        "    \"Residual\"   : decomp.resid,\n",
        "}\n",
        "\n",
        "colors = [\"steelblue\", \"darkorange\", \"green\", \"red\"]\n",
        "for ax, (name, data), color in zip(axes, components.items(), colors):\n",
        "    ax.plot(data, color=color, linewidth=0.8)\n",
        "    ax.set_ylabel(name)\n",
        "    ax.set_title(name, fontsize=11)\n",
        "\n",
        "axes[-1].xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
        "plt.suptitle(f\"{TICKER} Seasonal Decomposition (Additive, period=252)\", fontsize=13, y=1.01)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10755036",
      "metadata": {
        "id": "10755036"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 3.4 ACF and PACF Plots â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# ACF  â†’ tells us how many MA (q) terms to include\n",
        "# PACF â†’ tells us how many AR (p) terms to include\n",
        "# Rule of thumb: cut-off lags are where bars cross the confidence band\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "plot_acf(df[\"price_diff\"].dropna(),  lags=40, ax=axes[0],\n",
        "         title=\"ACF â€” First-Differenced Price\", alpha=0.05)\n",
        "plot_pacf(df[\"price_diff\"].dropna(), lags=40, ax=axes[1],\n",
        "         title=\"PACF â€” First-Differenced Price\", alpha=0.05, method=\"ywm\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50975910",
      "metadata": {
        "id": "50975910"
      },
      "source": [
        "### ðŸ’¬ Discussion Question 3A\n",
        "> 1. The raw price series failed the ADF test but the differenced series passed. Explain in plain English why differencing makes a series stationary and what information is lost when you do it.\n",
        "> 2. Looking at the ACF and PACF plots of the differenced series, what values of p (AR order) and q (MA order) would you propose for an ARIMA model? Justify your answer.\n",
        "> 3. The seasonal decomposition used `period=252`. Why 252? What would you change if you were working with hourly data instead of daily data?\n",
        "\n",
        "**Write your answers below:**\n",
        "\n",
        "1.\n",
        "2.\n",
        "3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36769972",
      "metadata": {
        "id": "36769972"
      },
      "source": [
        "---\n",
        "## Section 4 â€” ARIMA Modeling with `statsmodels` ðŸ“\n",
        "\n",
        "ARIMA stands for **AutoRegressive Integrated Moving Average**. It is the backbone of classical time series forecasting and is almost universally tested in quantitative data science interviews.\n",
        "\n",
        "### The ARIMA(p, d, q) Parameters\n",
        "\n",
        "| Parameter | Name | Meaning |\n",
        "|-----------|------|---------|\n",
        "| **p** | AR order | How many lagged values of the series to use |\n",
        "| **d** | Differencing | How many times to difference to achieve stationarity |\n",
        "| **q** | MA order | How many lagged forecast errors to use |\n",
        "\n",
        "> From Section 3, we know `d=1` (one round of differencing made the series stationary). We will start with `p=1, q=1` and evaluate.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Train / Test Split\n",
        "\n",
        "âš ï¸ **Critical Interview Concept:** You must **never** randomly shuffle time series data for train/test splitting. Order matters. The standard approach is a chronological cut â€” all data before a date is training, everything after is test."
      ],
      "metadata": {
        "id": "KtBYIrGVkwv5"
      },
      "id": "KtBYIrGVkwv5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57554969",
      "metadata": {
        "id": "57554969"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 4.1 Chronological Train/Test Split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "SPLIT_DATE = \"2023-01-01\"   # Last ~2 years = test set\n",
        "\n",
        "train = df[\"price\"][:SPLIT_DATE]\n",
        "test  = df[\"price\"][SPLIT_DATE:]\n",
        "\n",
        "print(f\"Training set : {train.index.min().date()} â†’ {train.index.max().date()} ({len(train)} days)\")\n",
        "print(f\"Test set     : {test.index.min().date()}  â†’ {test.index.max().date()} ({len(test)} days)\")\n",
        "\n",
        "# Visualise the split\n",
        "fig, ax = plt.subplots(figsize=(14, 5))\n",
        "ax.plot(train.index, train, label=\"Training\", color=\"steelblue\")\n",
        "ax.plot(test.index,  test,  label=\"Test\",     color=\"darkorange\")\n",
        "ax.axvline(pd.Timestamp(SPLIT_DATE), color=\"red\", linestyle=\"--\", label=\"Split Date\")\n",
        "ax.set_title(\"Train / Test Split\", fontsize=14)\n",
        "ax.set_ylabel(\"Price (USD)\")\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33866864",
      "metadata": {
        "id": "33866864"
      },
      "source": [
        "### 4.2 Fill-in-the-Blank: Fit ARIMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43198293",
      "metadata": {
        "id": "43198293"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 4.2 Fit ARIMA(1,1,1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# FILL IN: complete the ARIMA call with order=(p, d, q)\n",
        "# We determined d=1 from our ADF test. Start with p=1, q=1.\n",
        "\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import warnings\n",
        "from statsmodels.tools.sm_exceptions import ValueWarning\n",
        "\n",
        "# Suppress the date frequency warning â€” yfinance indices are irregular\n",
        "# due to market holidays and cannot be assigned a strict frequency\n",
        "warnings.filterwarnings(\"ignore\", category=ValueWarning)\n",
        "\n",
        "arima_model = ARIMA(train, order=(1,1,1))\n",
        "arima_result = arima_model.fit()\n",
        "\n",
        "print(arima_result.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43144955",
      "metadata": {
        "id": "43144955"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 4.3 Residual Diagnostics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# A well-fit ARIMA leaves residuals that look like white noise (random, no pattern)\n",
        "\n",
        "arima_result.plot_diagnostics(figsize=(14, 8))\n",
        "plt.suptitle(\"ARIMA(1,1,1) Residual Diagnostics\", fontsize=13)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Formal test: Ljung-Box test for residual autocorrelation\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "lb_result = acorr_ljungbox(arima_result.resid, lags=[10, 20], return_df=True)\n",
        "print(\"\\nLjung-Box Test (H0: residuals are white noise):\")\n",
        "print(lb_result)\n",
        "print(\"\\nâ†’ p > 0.05 at all lags = âœ… residuals look like white noise (good fit)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44085994",
      "metadata": {
        "id": "44085994"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 4.4 Walk-Forward Validation (Rolling Forecast) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# This is the gold-standard evaluation for time series.\n",
        "# At each step: fit on all available history, predict one step ahead, move forward.\n",
        "# This avoids look-ahead bias â€” a common mistake in industry.\n",
        "\n",
        "print(\"Running walk-forward validation (this may take a minute)...\")\n",
        "\n",
        "history     = list(train)\n",
        "predictions = []\n",
        "actuals     = list(test)\n",
        "\n",
        "for i, actual in enumerate(actuals):\n",
        "    model = ARIMA(history, order=(1, 1, 1))\n",
        "    fit   = model.fit()\n",
        "    yhat  = fit.forecast(steps=1)[0]\n",
        "    predictions.append(yhat)\n",
        "    history.append(actual)   # Add real observation to history\n",
        "    if i % 50 == 0:\n",
        "        print(f\"  Step {i+1}/{len(actuals)} â€” predicted: {yhat:.2f}, actual: {actual:.2f}\")\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "actuals     = np.array(actuals)\n",
        "\n",
        "# â”€â”€ Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "mae  = mean_absolute_error(actuals, predictions)\n",
        "rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
        "mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100\n",
        "\n",
        "print(f\"\\n{'='*40}\")\n",
        "print(f\"ARIMA(1,1,1) Walk-Forward Results\")\n",
        "print(f\"{'='*40}\")\n",
        "print(f\"  MAE  : ${mae:.2f}\")\n",
        "print(f\"  RMSE : ${rmse:.2f}\")\n",
        "print(f\"  MAPE : {mape:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64210171",
      "metadata": {
        "id": "64210171"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 4.5 Plot Actual vs Predicted â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "pred_series = pd.Series(predictions, index=test.index)\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
        "\n",
        "axes[0].plot(train.index, train, label=\"Training\",   color=\"steelblue\",   alpha=0.6)\n",
        "axes[0].plot(test.index,  test,  label=\"Actual\",     color=\"darkorange\",  linewidth=1.5)\n",
        "axes[0].plot(test.index,  pred_series, label=\"ARIMA Forecast\",\n",
        "             color=\"green\", linewidth=1.5, linestyle=\"--\")\n",
        "axes[0].set_title(\"ARIMA(1,1,1) â€” Walk-Forward Forecast vs Actual\", fontsize=13)\n",
        "axes[0].set_ylabel(\"Price (USD)\")\n",
        "axes[0].legend()\n",
        "\n",
        "# Error plot\n",
        "errors = actuals - predictions\n",
        "axes[1].bar(test.index, errors, color=\"red\", alpha=0.5, width=1)\n",
        "axes[1].axhline(0, color=\"black\", linewidth=0.8)\n",
        "axes[1].set_title(\"Forecast Errors (Actual âˆ’ Predicted)\", fontsize=13)\n",
        "axes[1].set_ylabel(\"Error (USD)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Store for later comparison\n",
        "arima_metrics = {\"Model\": \"ARIMA(1,1,1)\", \"MAE\": mae, \"RMSE\": rmse, \"MAPE%\": mape}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22657476",
      "metadata": {
        "id": "22657476"
      },
      "source": [
        "### ðŸ’¬ Discussion Question 4A\n",
        "> 1. Look at the model summary from Section 4.2. What do the AR and MA coefficients tell you? Are they statistically significant?\n",
        "> 2. Walk-forward validation is more computationally expensive than a single train/test split. When is the extra cost justified in a production system?\n",
        "> 3. ARIMA essentially predicts \"the next value will be close to today's value plus a small correction.\" Given this, what do you think the fundamental limitation of ARIMA is for longer-horizon forecasting?\n",
        "\n",
        "**Write your answers below:**\n",
        "\n",
        "1.\n",
        "2.\n",
        "3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68162007",
      "metadata": {
        "id": "68162007"
      },
      "source": [
        "---\n",
        "## Section 5 â€” Time Series as Supervised Learning with `sklearn` ðŸ¤–\n",
        "\n",
        "Here is a paradigm shift that connects directly to last week's lecture on supervised learning. **You can reframe any time series forecasting problem as a supervised regression problem** by engineering lag features.\n",
        "\n",
        "The idea: if `price[t]` depends on `price[t-1], price[t-2], ..., price[t-n]`, then those lagged values become features `X` and `price[t]` becomes the target `y`. Now you can use any sklearn model you already know.\n",
        "\n",
        "This is called the **\"sliding window\" or \"lag feature\" approach** and is widely used in production ML systems.\n",
        "\n",
        "### 5.1 Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52133898",
      "metadata": {
        "id": "52133898"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 5.1 Engineer Lag Features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def make_lag_features(series, lags=20, target_col=\"price\"):\n",
        "    \"\"\"\n",
        "    Convert a time series into a supervised learning dataset.\n",
        "\n",
        "    Parameters:\n",
        "        series     : pd.Series of closing prices\n",
        "        lags       : number of lagged time steps to use as features\n",
        "        target_col : name for the target column\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame with lag features + target\n",
        "    \"\"\"\n",
        "    data = pd.DataFrame(series.copy())\n",
        "    data.columns = [target_col]\n",
        "\n",
        "    for lag in range(1, lags + 1):\n",
        "        data[f\"lag_{lag}\"] = data[target_col].shift(lag)\n",
        "\n",
        "    # Additional engineered features\n",
        "    data[\"rolling_mean_5\"]  = data[target_col].shift(1).rolling(5).mean()\n",
        "    data[\"rolling_mean_20\"] = data[target_col].shift(1).rolling(20).mean()\n",
        "    data[\"rolling_std_5\"]   = data[target_col].shift(1).rolling(5).std()\n",
        "    data[\"day_of_week\"]     = data.index.dayofweek\n",
        "    data[\"month\"]           = data.index.month\n",
        "\n",
        "    return data.dropna()\n",
        "\n",
        "feat_df = make_lag_features(df[\"price\"], lags=20)\n",
        "\n",
        "print(f\"Feature matrix shape: {feat_df.shape}\")\n",
        "print(f\"Features: {list(feat_df.columns)}\")\n",
        "feat_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66732723",
      "metadata": {
        "id": "66732723"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 5.2 Chronological Train/Test Split (no shuffling!) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "feature_cols = [c for c in feat_df.columns if c != \"price\"]\n",
        "\n",
        "X = feat_df[feature_cols]\n",
        "y = feat_df[\"price\"]\n",
        "\n",
        "split_idx   = feat_df.index.searchsorted(pd.Timestamp(SPLIT_DATE))\n",
        "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
        "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
        "\n",
        "print(f\"X_train: {X_train.shape}  |  X_test: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40130491",
      "metadata": {
        "id": "40130491"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 5.3 Linear Regression (Baseline) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "lr_preds = lr.predict(X_test)\n",
        "\n",
        "lr_mae  = mean_absolute_error(y_test, lr_preds)\n",
        "lr_rmse = np.sqrt(mean_squared_error(y_test, lr_preds))\n",
        "lr_mape = np.mean(np.abs((y_test.values - lr_preds) / y_test.values)) * 100\n",
        "lr_r2   = r2_score(y_test, lr_preds)\n",
        "\n",
        "print(f\"Linear Regression on Lag Features\")\n",
        "print(f\"  MAE  : ${lr_mae:.2f}\")\n",
        "print(f\"  RMSE : ${lr_rmse:.2f}\")\n",
        "print(f\"  MAPE : {lr_mape:.2f}%\")\n",
        "print(f\"  RÂ²   : {lr_r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28455893",
      "metadata": {
        "id": "28455893"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 5.4 Random Forest (Non-linear Model) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_preds = rf.predict(X_test)\n",
        "\n",
        "rf_mae  = mean_absolute_error(y_test, rf_preds)\n",
        "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_preds))\n",
        "rf_mape = np.mean(np.abs((y_test.values - rf_preds) / y_test.values)) * 100\n",
        "rf_r2   = r2_score(y_test, rf_preds)\n",
        "\n",
        "print(f\"Random Forest on Lag Features\")\n",
        "print(f\"  MAE  : ${rf_mae:.2f}\")\n",
        "print(f\"  RMSE : ${rf_rmse:.2f}\")\n",
        "print(f\"  MAPE : {rf_mape:.2f}%\")\n",
        "print(f\"  RÂ²   : {rf_r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87003952",
      "metadata": {
        "id": "87003952"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 5.5 Feature Importance â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Random Forest gives us feature importances â€” a major advantage over ARIMA.\n",
        "\n",
        "importances = pd.Series(rf.feature_importances_, index=feature_cols)\n",
        "top_features = importances.nlargest(15)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "top_features.sort_values().plot(kind=\"barh\", ax=ax, color=\"steelblue\")\n",
        "ax.set_title(\"Random Forest â€” Top 15 Feature Importances\", fontsize=13)\n",
        "ax.set_xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Top 5 most predictive features:\")\n",
        "print(top_features.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18509051",
      "metadata": {
        "id": "18509051"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 5.6 Compare LR vs RF vs ARIMA visually â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "ax.plot(y_test.index, y_test.values,  label=\"Actual\",     color=\"black\",      linewidth=1.5)\n",
        "ax.plot(y_test.index, lr_preds,       label=\"Lin. Reg.\",  color=\"steelblue\",  linewidth=1, linestyle=\"--\")\n",
        "ax.plot(y_test.index, rf_preds,       label=\"Rand. Forest\", color=\"green\",    linewidth=1, linestyle=\"-.\")\n",
        "ax.plot(test.index,   predictions,    label=\"ARIMA(1,1,1)\", color=\"darkorange\",linewidth=1, linestyle=\":\")\n",
        "ax.set_title(\"Model Comparison â€” Test Period Forecasts\", fontsize=14)\n",
        "ax.set_ylabel(\"Price (USD)\")\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Tabulate metrics\n",
        "sklearn_metrics = [\n",
        "    {\"Model\": \"Linear Regression (lag)\", \"MAE\": lr_mae,  \"RMSE\": lr_rmse,  \"MAPE%\": lr_mape},\n",
        "    {\"Model\": \"Random Forest (lag)\",     \"MAE\": rf_mae,  \"RMSE\": rf_rmse,  \"MAPE%\": rf_mape},\n",
        "]\n",
        "metrics_table = pd.DataFrame([arima_metrics] + sklearn_metrics)\n",
        "metrics_table = metrics_table.set_index(\"Model\")\n",
        "print(\"\\nModel Comparison Table:\")\n",
        "print(metrics_table.to_string())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66339851",
      "metadata": {
        "id": "66339851"
      },
      "source": [
        "### ðŸ’¬ Discussion Question 5A\n",
        "> 1. Why might Random Forest achieve a high RÂ² score on this problem even though predicting stock prices is notoriously difficult? Is high RÂ² always a sign of a good model?\n",
        "> 2. The feature importance plot likely shows that `lag_1` dominates. What does this tell you about the nature of stock price movements? How does this relate to the Efficient Market Hypothesis?\n",
        "> 3. If you were presenting these models to a portfolio manager at a hedge fund, which would you choose and why? What additional information would they likely demand before trusting the model?\n",
        "\n",
        "**Write your answers below:**\n",
        "\n",
        "1.\n",
        "2.\n",
        "3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36239286",
      "metadata": {
        "id": "36239286"
      },
      "source": [
        "---\n",
        "## Section 6 â€” Forecasting with `Prophet` ðŸ”®\n",
        "\n",
        "**Prophet** was developed by Meta's Core Data Science team and released as open source in 2017. It is designed for analysts who need reliable forecasts quickly, without deep time series expertise. It is widely used in industry for:\n",
        "\n",
        "- Business metric forecasting (DAU, revenue, inventory)\n",
        "- Capacity planning\n",
        "- Anomaly detection baselines\n",
        "\n",
        "**Key advantages:**\n",
        "- Handles missing data and outliers automatically\n",
        "- Models trend, weekly seasonality, yearly seasonality, and holidays out of the box\n",
        "- Interpretable components\n",
        "- Uncertainty intervals built in\n",
        "\n",
        "Prophet expects a DataFrame with exactly two columns: `ds` (datestamp) and `y` (value). That's it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59125980",
      "metadata": {
        "id": "59125980"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 6.1 Prepare Prophet Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "prophet_df = df[\"price\"].reset_index()\n",
        "prophet_df.columns = [\"ds\", \"y\"]\n",
        "prophet_df[\"ds\"] = pd.to_datetime(prophet_df[\"ds\"])\n",
        "\n",
        "# Remove timezone info if present\n",
        "if prophet_df[\"ds\"].dt.tz is not None:\n",
        "    prophet_df[\"ds\"] = prophet_df[\"ds\"].dt.tz_localize(None)\n",
        "\n",
        "prophet_train = prophet_df[prophet_df[\"ds\"] < SPLIT_DATE]\n",
        "prophet_test  = prophet_df[prophet_df[\"ds\"] >= SPLIT_DATE]\n",
        "\n",
        "print(f\"Prophet train shape : {prophet_train.shape}\")\n",
        "print(f\"Prophet test shape  : {prophet_test.shape}\")\n",
        "prophet_train.tail()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25801076",
      "metadata": {
        "id": "25801076"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 6.2 Fit Prophet Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "prophet_model = Prophet(\n",
        "    daily_seasonality   = False,\n",
        "    weekly_seasonality  = True,\n",
        "    yearly_seasonality  = True,\n",
        "    changepoint_prior_scale = 0.05,   # Controls flexibility of trend\n",
        "    seasonality_prior_scale = 10,     # Controls flexibility of seasonality\n",
        "    interval_width      = 0.95,       # 95% uncertainty intervals\n",
        ")\n",
        "\n",
        "# Add US stock market holidays\n",
        "prophet_model.add_country_holidays(country_name=\"US\")\n",
        "\n",
        "print(\"Fitting Prophet model...\")\n",
        "prophet_model.fit(prophet_train)\n",
        "print(\"âœ… Prophet model fitted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63374122",
      "metadata": {
        "id": "63374122"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 6.3 Generate Forecast â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Create a future dataframe for the test period + 90 additional days\n",
        "future = prophet_model.make_future_dataframe(\n",
        "    periods = len(prophet_test) + 90,\n",
        "    freq    = \"B\"   # \"B\" = business days only\n",
        ")\n",
        "\n",
        "forecast = prophet_model.predict(future)\n",
        "forecast_test = forecast[forecast[\"ds\"] >= SPLIT_DATE]\n",
        "\n",
        "print(f\"Forecast columns: {list(forecast.columns)}\")\n",
        "forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\", \"trend\", \"weekly\", \"yearly\"]].tail(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26091092",
      "metadata": {
        "id": "26091092"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 6.4 Evaluation on Test Set â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Align test actuals with prophet predictions\n",
        "prophet_test_merged = prophet_test.merge(\n",
        "    forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
        "    on=\"ds\", how=\"inner\"\n",
        ")\n",
        "\n",
        "p_mae  = mean_absolute_error(prophet_test_merged[\"y\"], prophet_test_merged[\"yhat\"])\n",
        "p_rmse = np.sqrt(mean_squared_error(prophet_test_merged[\"y\"], prophet_test_merged[\"yhat\"]))\n",
        "p_mape = np.mean(np.abs((prophet_test_merged[\"y\"] - prophet_test_merged[\"yhat\"])\n",
        "                          / prophet_test_merged[\"y\"])) * 100\n",
        "\n",
        "print(f\"Prophet Forecast Evaluation\")\n",
        "print(f\"  MAE  : ${p_mae:.2f}\")\n",
        "print(f\"  RMSE : ${p_rmse:.2f}\")\n",
        "print(f\"  MAPE : {p_mape:.2f}%\")\n",
        "\n",
        "prophet_metrics = {\"Model\": \"Prophet\", \"MAE\": p_mae, \"RMSE\": p_rmse, \"MAPE%\": p_mape}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67511093",
      "metadata": {
        "id": "67511093"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 6.5 Prophet Component Plots â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# These plots are one of Prophet's most valuable features â€” they decompose the\n",
        "# forecast into interpretable components.\n",
        "\n",
        "fig = prophet_model.plot_components(forecast)\n",
        "plt.suptitle(\"Prophet â€” Forecast Components\", fontsize=13, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71930852",
      "metadata": {
        "id": "71930852"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 6.6 Full Forecast Plot with Uncertainty Intervals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "# Historical\n",
        "ax.plot(prophet_train[\"ds\"], prophet_train[\"y\"],\n",
        "        color=\"steelblue\", label=\"Training\", alpha=0.7, linewidth=1)\n",
        "\n",
        "# Test actuals\n",
        "ax.plot(prophet_test_merged[\"ds\"], prophet_test_merged[\"y\"],\n",
        "        color=\"black\", label=\"Actual (test)\", linewidth=1.5)\n",
        "\n",
        "# Prophet forecast\n",
        "ax.plot(forecast_test[\"ds\"], forecast_test[\"yhat\"],\n",
        "        color=\"darkorange\", label=\"Prophet Forecast\", linewidth=1.5, linestyle=\"--\")\n",
        "\n",
        "# Uncertainty band\n",
        "ax.fill_between(forecast_test[\"ds\"],\n",
        "                forecast_test[\"yhat_lower\"],\n",
        "                forecast_test[\"yhat_upper\"],\n",
        "                alpha=0.2, color=\"darkorange\", label=\"95% CI\")\n",
        "\n",
        "# 90-day future\n",
        "future_only = forecast[forecast[\"ds\"] > prophet_df[\"ds\"].max()]\n",
        "ax.plot(future_only[\"ds\"], future_only[\"yhat\"],\n",
        "        color=\"green\", linewidth=1.5, linestyle=\"-.\", label=\"90-day Ahead Forecast\")\n",
        "ax.fill_between(future_only[\"ds\"],\n",
        "                future_only[\"yhat_lower\"], future_only[\"yhat_upper\"],\n",
        "                alpha=0.15, color=\"green\")\n",
        "\n",
        "ax.axvline(pd.Timestamp(SPLIT_DATE), color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Split Date\")\n",
        "ax.set_title(f\"Prophet Forecast â€” {TICKER}\", fontsize=14)\n",
        "ax.set_ylabel(\"Price (USD)\")\n",
        "ax.legend(loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55684200",
      "metadata": {
        "id": "55684200"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 6.7 Final Model Comparison Table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "all_metrics = pd.DataFrame([\n",
        "    arima_metrics,\n",
        "    {\"Model\": \"Linear Regression (lag)\", \"MAE\": lr_mae,  \"RMSE\": lr_rmse,  \"MAPE%\": lr_mape},\n",
        "    {\"Model\": \"Random Forest (lag)\",     \"MAE\": rf_mae,  \"RMSE\": rf_rmse,  \"MAPE%\": rf_mape},\n",
        "    prophet_metrics,\n",
        "]).set_index(\"Model\").round(2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(\"    COMPLETE MODEL COMPARISON TABLE\")\n",
        "print(\"=\"*55)\n",
        "print(all_metrics.to_string())\n",
        "print(\"\\nâ†’ Lower MAE / RMSE / MAPE% = better forecast accuracy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73769956",
      "metadata": {
        "id": "73769956"
      },
      "source": [
        "### ðŸ’¬ Discussion Question 6A\n",
        "> 1. Prophet's uncertainty intervals (the shaded region) widen as you forecast further into the future. Why? What are the practical implications for a business relying on 90-day forecasts?\n",
        "> 2. The weekly and yearly seasonality components in the Prophet decomposition may look surprising for a stock. What economic or behavioral factors could explain weekly patterns in stock prices?\n",
        "> 3. Looking at the final comparison table: if you had to choose ONE model to deploy in a production system that generates daily forecasts automatically, which would you choose and what criteria drove that decision beyond raw accuracy metrics?\n",
        "\n",
        "**Write your answers below:**\n",
        "\n",
        "1.\n",
        "2.\n",
        "3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67134770",
      "metadata": {
        "id": "67134770"
      },
      "source": [
        "---\n",
        "## Section 7 â€” Agentic Forecasting Pipeline with Smolagents + GROQ + HuggingFace ðŸ¤–ðŸ§ \n",
        "\n",
        "This is where everything comes together. We now build an **AI agent** that:\n",
        "\n",
        "1. Accepts a stock ticker as input\n",
        "2. Downloads and analyzes the time series\n",
        "3. Runs a stationarity check\n",
        "4. Fits an ARIMA model\n",
        "5. Generates a 30-day forecast\n",
        "6. Narrates the results in plain English using an LLM (via GROQ or HuggingFace)\n",
        "\n",
        "This mirrors a real **MLOps agentic pipeline** pattern â€” the kind you would build at a fintech firm, hedge fund, or data-driven enterprise.\n",
        "\n",
        "### Architecture Overview\n",
        "\n",
        "```\n",
        "User Query\n",
        "    â”‚\n",
        "    â–¼\n",
        "CodeAgent (Smolagents)\n",
        "    â”‚\n",
        "    â”œâ”€â”€ Tool: fetch_and_analyze_series()\n",
        "    â”œâ”€â”€ Tool: run_stationarity_check()\n",
        "    â”œâ”€â”€ Tool: fit_arima_forecast()\n",
        "    â””â”€â”€ Tool: summarize_with_llm()  â†â”€â”€ GROQ / HuggingFace\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Define Agent Tools"
      ],
      "metadata": {
        "id": "nGQjCn6doL-L"
      },
      "id": "nGQjCn6doL-L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56243542",
      "metadata": {
        "id": "56243542"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 7.1 Define Smolagents Tools â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Each @tool is a self-contained callable the agent can invoke.\n",
        "# Type hints and docstrings are essential â€” the agent reads them to understand\n",
        "# what each tool does and when to use it.\n",
        "\n",
        "@tool\n",
        "def fetch_stock_data(ticker: str, start: str, end: str) -> str:\n",
        "    \"\"\"\n",
        "    Fetches daily closing price data for a stock ticker from Yahoo Finance.\n",
        "    Returns a summary string with basic statistics about the time series.\n",
        "\n",
        "    Args:\n",
        "        ticker: Stock ticker symbol (e.g., 'AAPL', 'SPY', 'MSFT')\n",
        "        start:  Start date in YYYY-MM-DD format\n",
        "        end:    End date in YYYY-MM-DD format\n",
        "    \"\"\"\n",
        "    import yfinance as yf\n",
        "    import numpy as np\n",
        "\n",
        "    data = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n",
        "    if data.empty:\n",
        "        return f\"ERROR: No data found for {ticker}.\"\n",
        "\n",
        "    close = data[\"Close\"].squeeze()\n",
        "    returns = close.pct_change().dropna()\n",
        "\n",
        "    summary = (\n",
        "        f\"Ticker: {ticker}\\n\"\n",
        "        f\"Date Range: {close.index.min().date()} to {close.index.max().date()}\\n\"\n",
        "        f\"Observations: {len(close)}\\n\"\n",
        "        f\"Price Range: ${close.min():.2f} â€“ ${close.max():.2f}\\n\"\n",
        "        f\"Mean Price: ${close.mean():.2f}\\n\"\n",
        "        f\"Std Dev: ${close.std():.2f}\\n\"\n",
        "        f\"Mean Daily Return: {returns.mean()*100:.4f}%\\n\"\n",
        "        f\"Return Volatility (annualized): {returns.std()*np.sqrt(252)*100:.2f}%\\n\"\n",
        "        f\"Total Return over period: {((close.iloc[-1]/close.iloc[0])-1)*100:.2f}%\"\n",
        "    )\n",
        "    return summary\n",
        "\n",
        "\n",
        "@tool\n",
        "def run_stationarity_check(ticker: str, start: str, end: str) -> str:\n",
        "    \"\"\"\n",
        "    Downloads stock data and runs an Augmented Dickey-Fuller stationarity test.\n",
        "    Returns the test result and recommended differencing order.\n",
        "\n",
        "    Args:\n",
        "        ticker: Stock ticker symbol\n",
        "        start:  Start date in YYYY-MM-DD format\n",
        "        end:    End date in YYYY-MM-DD format\n",
        "    \"\"\"\n",
        "    import yfinance as yf\n",
        "    from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "    data  = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n",
        "    close = data[\"Close\"].squeeze().dropna()\n",
        "\n",
        "    result_raw  = adfuller(close)\n",
        "    result_diff = adfuller(close.diff().dropna())\n",
        "\n",
        "    d = 0 if result_raw[1] < 0.05 else 1\n",
        "\n",
        "    report = (\n",
        "        f\"ADF Test â€” {ticker} Raw Price:\\n\"\n",
        "        f\"  p-value: {result_raw[1]:.6f} â†’ {'STATIONARY' if result_raw[1]<0.05 else 'NON-STATIONARY'}\\n\\n\"\n",
        "        f\"ADF Test â€” {ticker} First Difference:\\n\"\n",
        "        f\"  p-value: {result_diff[1]:.6f} â†’ {'STATIONARY' if result_diff[1]<0.05 else 'NON-STATIONARY'}\\n\\n\"\n",
        "        f\"Recommended d (differencing order): {d}\"\n",
        "    )\n",
        "    return report\n",
        "\n",
        "\n",
        "@tool\n",
        "def fit_arima_and_forecast(ticker: str, start: str, end: str,\n",
        "                            p: int, d: int, q: int, forecast_days: int) -> str:\n",
        "    \"\"\"\n",
        "    Fits an ARIMA(p,d,q) model to stock closing prices and generates a forecast.\n",
        "    Returns a text summary of model performance and future price predictions.\n",
        "\n",
        "    Args:\n",
        "        ticker:        Stock ticker symbol\n",
        "        start:         Training data start date (YYYY-MM-DD)\n",
        "        end:           Training data end date (YYYY-MM-DD)\n",
        "        p:             AR order\n",
        "        d:             Differencing order (use result from stationarity check)\n",
        "        q:             MA order\n",
        "        forecast_days: Number of trading days to forecast ahead\n",
        "    \"\"\"\n",
        "    import yfinance as yf\n",
        "    import numpy as np\n",
        "    from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "    data  = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n",
        "    close = data[\"Close\"].squeeze().dropna()\n",
        "\n",
        "    model  = ARIMA(close, order=(p, d, q))\n",
        "    result = model.fit()\n",
        "\n",
        "    forecast    = result.forecast(steps=forecast_days)\n",
        "    conf_int    = result.get_forecast(steps=forecast_days).conf_int(alpha=0.05)\n",
        "    last_price  = close.iloc[-1]\n",
        "\n",
        "    forecast_lines = []\n",
        "    for i, (yhat, (lo, hi)) in enumerate(zip(forecast, conf_int.values), 1):\n",
        "        forecast_lines.append(\n",
        "            f\"  Day +{i:02d}: ${yhat:.2f}  (95% CI: ${lo:.2f} â€“ ${hi:.2f})\"\n",
        "        )\n",
        "\n",
        "    report = (\n",
        "        f\"ARIMA({p},{d},{q}) Model â€” {ticker}\\n\"\n",
        "        f\"{'='*50}\\n\"\n",
        "        f\"AIC: {result.aic:.2f}  |  BIC: {result.bic:.2f}\\n\"\n",
        "        f\"Last Observed Price: ${last_price:.2f}\\n\\n\"\n",
        "        f\"{forecast_days}-Day Ahead Forecast:\\n\"\n",
        "        + \"\\n\".join(forecast_lines[:10])   # Cap at 10 lines for readability\n",
        "        + f\"\\n  ...\\n  Day +{forecast_days}: ${forecast.iloc[-1]:.2f}\"\n",
        "        + f\"\\n\\nImplied {forecast_days}-day Return: \"\n",
        "        + f\"{((forecast.iloc[-1]/last_price)-1)*100:.2f}%\"\n",
        "    )\n",
        "    return report\n",
        "\n",
        "print(\"âœ… All agent tools defined.\")\n",
        "print(\"   Tools available: fetch_stock_data, run_stationarity_check, fit_arima_and_forecast\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28562345",
      "metadata": {
        "id": "28562345"
      },
      "source": [
        "### 7.2 Initialize the LLM Backbone\n",
        "\n",
        "We will demonstrate both GROQ and HuggingFace as LLM providers. This is a real industry skill â€” knowing how to swap model backends without rewriting your agentic code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59592577",
      "metadata": {
        "id": "59592577"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 7.2a GROQ Backend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# GROQ provides blazing-fast inference of open-source LLMs (Llama 3, Mixtral, etc.)\n",
        "# Free tier: ~14,400 requests/day with no credit card required\n",
        "\n",
        "from smolagents import OpenAIServerModel, InferenceClientModel, CodeAgent\n",
        "\n",
        "# GROQ uses an OpenAI-compatible REST API â€” use OpenAIServerModel, not HfApiModel\n",
        "groq_model = OpenAIServerModel(\n",
        "    model_id = \"llama-3.3-70b-versatile\",          # GROQ model name (not HF path)\n",
        "    api_base = \"https://api.groq.com/openai/v1\",\n",
        "    api_key  = GROQ_API_KEY,\n",
        ")\n",
        "\n",
        "print(\"âœ… GROQ backend initialized.\")\n",
        "print(\"   Model: llama-3.3-70b-versatile via GROQ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57062744",
      "metadata": {
        "id": "57062744"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 7.2b HuggingFace Backend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# HuggingFace Inference API provides free access to thousands of open-source models\n",
        "# No API key required for many smaller models; free tier available\n",
        "\n",
        "hf_model = InferenceClientModel(\n",
        "    model_id = \"Qwen/Qwen2.5-72B-Instruct\",   # Strong open-source model, free tier\n",
        "    token    = HF_TOKEN,\n",
        ")\n",
        "\n",
        "print(\"âœ… HuggingFace backend initialized.\")\n",
        "print(\"   Model: Qwen/Qwen2.5-72B-Instruct via HuggingFace Inference API\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35666996",
      "metadata": {
        "id": "35666996"
      },
      "source": [
        "### 7.3 Build and Run the Forecasting Agent\n",
        "\n",
        "Now we assemble the full agentic pipeline. The `CodeAgent` will reason over our tools, decide which to call, interpret the results, and produce a final narrative.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44378924",
      "metadata": {
        "id": "44378924"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 7.3 Create the Smolagents CodeAgent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# CodeAgent reasons by writing and executing Python code, then using\n",
        "# the tool results to form a final answer. This is more powerful than\n",
        "# a simple ReAct agent for data science tasks.\n",
        "\n",
        "# Re-run this before the CodeAgent cell if agent_tools is undefined\n",
        "agent_tools = [\n",
        "    fetch_stock_data,\n",
        "    run_stationarity_check,\n",
        "    fit_arima_and_forecast,\n",
        "]\n",
        "\n",
        "print([t.name for t in agent_tools])   # Should print the three tool names\n",
        "\n",
        "forecasting_agent = CodeAgent(\n",
        "    tools          = agent_tools,\n",
        "    model          = groq_model,    # â† swap to hf_model for HuggingFace\n",
        "    max_steps      = 8,\n",
        "    verbosity_level = 2,            # 0=silent, 1=minimal, 2=full reasoning steps\n",
        ")\n",
        "\n",
        "print(\"âœ… Forecasting agent ready.\")\n",
        "print(\"   Agent type  : CodeAgent\")\n",
        "print(\"   Tools bound :\", [t.name for t in agent_tools])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54378075",
      "metadata": {
        "id": "54378075"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 7.4 Run the Agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# This is the key moment â€” we give the agent a natural language task and let it\n",
        "# reason, call tools, and synthesize a response autonomously.\n",
        "\n",
        "AGENT_QUERY = f\"\"\"\n",
        "You are a senior quantitative analyst. Analyze the stock {TICKER} using the following steps:\n",
        "\n",
        "1. Fetch and summarize the stock data from 2018-01-01 to 2024-12-31.\n",
        "2. Run a stationarity test and determine the appropriate differencing order (d).\n",
        "3. Fit an ARIMA(1, d, 1) model and generate a 30-day ahead price forecast.\n",
        "4. Write a concise, professional analyst report (3â€“5 paragraphs) summarizing:\n",
        "   - Key statistics and trend observations\n",
        "   - Stationarity findings and what they mean for modeling\n",
        "   - The 30-day forecast and implied return\n",
        "   - One risk caveat a portfolio manager should be aware of\n",
        "   - A clear bottom-line recommendation\n",
        "\n",
        "Write in the style of a Bloomberg Intelligence analyst note.\n",
        "\"\"\"\n",
        "\n",
        "print(\"ðŸ¤– Agent starting analysis...\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "agent_report = forecasting_agent.run(AGENT_QUERY)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“‹ FINAL AGENT REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(agent_report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92183372",
      "metadata": {
        "id": "92183372"
      },
      "source": [
        "### 7.4 Swap Backends â€” HuggingFace Edition\n",
        "\n",
        "One of the most important architectural principles in modern ML systems is **provider abstraction** â€” your pipeline should not be locked into one vendor. Below we re-run the agent with the HuggingFace backend so you can compare outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31625682",
      "metadata": {
        "id": "31625682"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 7.5 Re-run with HuggingFace Backend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "forecasting_agent_hf = CodeAgent(\n",
        "    tools           = agent_tools,\n",
        "    model           = hf_model,\n",
        "    max_steps       = 8,\n",
        "    verbosity_level = 0,    # Set to 2 to see full reasoning steps\n",
        ")\n",
        "\n",
        "print(\"ðŸ¤– HuggingFace agent starting analysis...\\n\")\n",
        "hf_report = forecasting_agent_hf.run(AGENT_QUERY)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“‹ HUGGINGFACE AGENT REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(hf_report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51541293",
      "metadata": {
        "id": "51541293"
      },
      "source": [
        "### 7.5 Direct LLM Comparison (GROQ vs HuggingFace)\n",
        "\n",
        "Beyond using the agent, you can also call GROQ and HuggingFace directly for pure text generation tasks. This is useful when you have already computed your results and just need the LLM to narrate them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67351016",
      "metadata": {
        "id": "67351016"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 7.6 Direct GROQ API call â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# This uses the Groq Python client directly â€” no agent overhead.\n",
        "# Use this pattern when you have structured results and just need narration.\n",
        "\n",
        "from groq import Groq as GroqClient\n",
        "\n",
        "groq_client = GroqClient(api_key=GROQ_API_KEY)\n",
        "\n",
        "# Prepare a structured prompt from our computed metrics\n",
        "metrics_text = all_metrics.to_string()\n",
        "\n",
        "groq_chat = groq_client.chat.completions.create(\n",
        "    model    = \"llama-3.3-70b-versatile\",\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a senior data scientist writing for a financial audience. \"\n",
        "                \"Be concise, precise, and use correct statistical terminology.\"\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                f\"Here are the results from four forecasting models applied to {TICKER} stock:\\n\\n\"\n",
        "                f\"{metrics_text}\\n\\n\"\n",
        "                \"In 3 sentences, explain which model performed best, why that might be, \"\n",
        "                \"and what a practitioner should consider before deploying any of these models in production.\"\n",
        "            )\n",
        "        }\n",
        "    ],\n",
        "    temperature = 0.3,\n",
        "    max_tokens  = 400,\n",
        ")\n",
        "\n",
        "print(\"GROQ Direct Response:\")\n",
        "print(\"â”€\" * 50)\n",
        "print(groq_chat.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10735860",
      "metadata": {
        "id": "10735860"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ 7.7 Direct HuggingFace Inference API call â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# InferenceClient is the standard HuggingFace SDK for calling hosted models.\n",
        "\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "hf_client = InferenceClient(token=HF_TOKEN)\n",
        "\n",
        "hf_response = hf_client.chat_completion(\n",
        "    model    = \"Qwen/Qwen2.5-72B-Instruct\",\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a concise financial data science assistant.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                f\"Model comparison for {TICKER} forecasting:\\n{metrics_text}\\n\\n\"\n",
        "                \"In 3 sentences: which model wins, likely reason, and one production caveat.\"\n",
        "            )\n",
        "        }\n",
        "    ],\n",
        "    max_tokens  = 400,\n",
        "    temperature = 0.3,\n",
        ")\n",
        "\n",
        "print(\"HuggingFace Direct Response:\")\n",
        "print(\"â”€\" * 50)\n",
        "print(hf_response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80928663",
      "metadata": {
        "id": "80928663"
      },
      "source": [
        "### ðŸ’¬ Discussion Question 7A â€” The Big Picture\n",
        "\n",
        "> These are the capstone questions for this workshop. Take your time. These are the types of questions you may face in a final-round data science interview.\n",
        "\n",
        "> 1. **Agentic Design:** Our agent used three tools. If you were extending this pipeline for a production trading desk, what additional tools would you add? Think about data sources, risk checks, and compliance requirements.\n",
        "\n",
        "> 2. **LLM Provider Trade-offs:** You just ran the same task on GROQ and HuggingFace. What differences did you observe in speed, output quality, and reasoning style? In a production system, what factors would determine which provider you choose?\n",
        "\n",
        "> 3. **The Limits of Forecasting:** All four models were applied to a financial time series. Stock price prediction is widely considered one of the hardest forecasting problems. Given everything you have built today, what is your honest assessment of the practical utility of these models? Under what conditions might they genuinely add value?\n",
        "\n",
        "> 4. **Connecting to Last Week:** How does the agentic pipeline we built today relate to the federated learning paradigm discussed last week? Could you imagine a federated forecasting agent? What would that look like and what problem would it solve?\n",
        "\n",
        "> 5. **Capstone Connection:** How could you apply the techniques in this notebook to your own final project? Identify one specific component â€” a tool, a model, or an agentic pattern â€” that you could adapt for your research.\n",
        "\n",
        "**Write your answers below (double-click to edit):**\n",
        "\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "3.\n",
        "\n",
        "4.\n",
        "\n",
        "5.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80506441",
      "metadata": {
        "id": "80506441"
      },
      "source": [
        "---\n",
        "## ðŸŽ“ Workshop Summary\n",
        "\n",
        "Congratulations â€” you have built a complete, industry-grade time series forecasting and agentic pipeline. Here is what you accomplished today:\n",
        "\n",
        "| Section | What You Built | Industry Skill |\n",
        "|---------|---------------|----------------|\n",
        "| 1 | Reproducible environment setup | Dependency management |\n",
        "| 2 | Live financial data acquisition + EDA | `yfinance`, exploratory analysis |\n",
        "| 3 | Stationarity testing + decomposition | ADF test, ACF/PACF interpretation |\n",
        "| 4 | ARIMA with walk-forward validation | Classical time series modeling |\n",
        "| 5 | Lag-feature regression (LR + RF) | Supervised ML on time series |\n",
        "| 6 | Prophet with uncertainty intervals | Production-grade forecasting |\n",
        "| 7 | Agentic pipeline (GROQ + HuggingFace) | LLM orchestration, tool use, MLOps |\n",
        "\n",
        "### Key Interview Concepts to Remember\n",
        "\n",
        "- **Stationarity is a prerequisite**, not a suggestion. Always ADF-test first.\n",
        "- **Never randomly split time series** â€” always chronological splits.\n",
        "- **Walk-forward validation** is the gold standard for time series evaluation.\n",
        "- **Lag features** are the bridge between time series and supervised learning.\n",
        "- **Agentic pipelines** are the future of production ML â€” tools + LLMs + orchestration.\n",
        "- **Provider abstraction** â€” your code should not be locked to one LLM vendor.\n",
        "\n",
        "### ðŸ“š Further Reading\n",
        "- Box, Jenkins et al. â€” *Time Series Analysis: Forecasting and Control* (the ARIMA bible)\n",
        "- Taylor & Letham â€” *Forecasting at Scale* (the original Prophet paper, 2018)\n",
        "- Sutton & Barto â€” *Reinforcement Learning* (connects to last week's paradigms)\n",
        "- HuggingFace Smolagents docs â€” [huggingface.co/docs/smolagents](https://huggingface.co/docs/smolagents)\n",
        "- GROQ docs â€” [console.groq.com/docs](https://console.groq.com/docs)\n"
      ]
    }
  ]
}