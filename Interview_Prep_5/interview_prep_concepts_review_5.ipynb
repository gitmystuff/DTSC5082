{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC5082/blob/main/Interview_Prep_5/interview_prep_concepts_review_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised, Unsupervised, and Other Learning Paradigms"
      ],
      "metadata": {
        "id": "81szfn6JuGtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1\n",
        "\n",
        "## ðŸŽ¯ Objective\n",
        "The prep assignments are not about getting a high accuracy score. It is about **process** and **interpretation**. In a technical interview, you will rarely be judged solely on your model's RMSE; you will be judged on:\n",
        "1.  Can you explain *why* you chose that model?\n",
        "2.  Can you interpret the results for a non-technical stakeholder?\n",
        "3.  Do you understand the mathematical foundations (e.g., OLS assumptions, Log-Odds)?\n",
        "\n",
        "70+ points you move on to the next round. < 70 you don't get a call back.\n",
        "\n",
        "## ðŸ“š Topics Covered\n",
        "* **Linear Regression:** OLS, Assumptions, and Coefficient Interpretation.\n",
        "* **GLMs:** Moving beyond the Normal distribution.\n",
        "* **Logistic Regression:** Probabilities, Thresholds, and Log-Odds.\n",
        "* **Tree-Based Models:** Decision Trees, Random Forests, and Overfitting."
      ],
      "metadata": {
        "id": "TAo8Va8mt1Y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Configuration for prettier plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "n-w6kVpEuU9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Linear Foundation & Interpretation\n",
        "\n",
        "### ðŸ’¡ Concept Review\n",
        "We start with **Ordinary Least Squares (OLS)**. The goal is to minimize the Sum of Squared Residuals (SSR).\n",
        "$$\\min \\sum (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "### ðŸŽ¤ Interview Hot Seat\n",
        "**Interviewer:** \"I see you have an $R^2$ of 0.89. What does that actually mean? And why might we look at Adjusted $R^2$ instead?\"\n",
        "\n"
      ],
      "metadata": {
        "id": "OsF87Cjuubbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CODE CHALLENGE 1: Interpret the Coefficients ---\n",
        "# Scenario: We are predicting Sales based on TV, Radio, and Newspaper ad spend.\n",
        "# Data generation (mimicking the \"Advertising\" dataset from your notes)\n",
        "\n",
        "np.random.seed(42)\n",
        "n_samples = 200\n",
        "tv = np.random.normal(150, 50, n_samples)\n",
        "radio = np.random.normal(30, 10, n_samples)\n",
        "newspaper = np.random.normal(40, 20, n_samples)\n",
        "\n",
        "# True relationship: Sales = 2.8 + 0.046*TV + 0.19*Radio + noise\n",
        "sales = 2.8 + 0.046 * tv + 0.19 * radio + np.random.normal(0, 2, n_samples)\n",
        "\n",
        "df_ads = pd.DataFrame({'TV': tv, 'Radio': radio, 'Newspaper': newspaper, 'Sales': sales})\n",
        "\n",
        "# 1. Prepare Data for Statsmodels (Add Constant)\n",
        "X = df_ads[['TV', 'Radio', 'Newspaper']]\n",
        "y = df_ads['Sales']\n",
        "\n",
        "# TODO: Add a constant to X (Statsmodels requires this explicitly!)\n",
        "X_const = sm.add_constant(X)\n",
        "\n",
        "# 2. Fit OLS Model\n",
        "model = sm.OLS(y, X_const).fit()\n",
        "\n",
        "# 3. Print Summary\n",
        "print(model.summary())\n",
        "\n",
        "# --- DISCUSSION ---\n",
        "# Look at the coefficient for 'Radio' (approx 0.19).\n",
        "# QUESTION: If we spend $1,000 more on Radio ads, how do Sales change?\n",
        "# HINT: Check the units. If Sales is in units and spend is in dollars..."
      ],
      "metadata": {
        "id": "Ay6HmQKtutXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalized Linear Models (GLMs)\n",
        "\n",
        "### ðŸ’¡ Concept Review\n",
        "Standard Linear Regression assumes the residuals are normally distributed (Gaussian) and the relationship is linear ($\\eta = \\mu$). But real data is rarely this clean.\n",
        "* **GLMs** allow us to specify a **Random Component** (distribution like Poisson, Binomial) and a **Link Function**.\n",
        "* **Link Function:** Transforms the expected mean to fit the linear predictor.\n",
        "    * Linear Regression: Identity Link ($\\eta = \\mu$).\n",
        "    * Logistic Regression: Logit Link ($\\eta = \\log(\\frac{\\mu}{1-\\mu})$).\n",
        "    * Poisson Regression: Log Link ($\\eta = \\log(\\mu)$).\n",
        "\n",
        "### ðŸŽ¤ Interview Hot Seat\n",
        "**Interviewer:** \"Why can't I use standard OLS regression to predict the number of website visitors per hour?\"\n"
      ],
      "metadata": {
        "id": "OO_C43_iu3nq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How it Works\n",
        "\n",
        "| Step | Component | Mathematical Representation | What it Does |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **1** | **Linear Predictor** | $\\eta = \\beta_0 + \\beta_1X_1 + \\dots$ | This is your \"score\"â€”the weighted sum of your inputs. |\n",
        "| **2** | **Link Function** | $g(\\mu) = \\eta$ | The bridge that connects the score to the expected mean. |\n",
        "| **3** | **Random Component** | $Y \\sim \\text{Normal}(\\mu, \\sigma^2)$ | Adds the \"noise\" (residuals) to account for real-world variation. |\n",
        "\n",
        "### Why distinguish between $\\eta$ and $\\mu$?\n",
        "\n",
        "In standard linear regression, we often treat them as the same thing because the **Identity Link** doesn't change anything ($\\eta = \\mu$).\n",
        "\n",
        "However, the distinction becomes vital when your data isn't a straight line. For example:\n",
        "\n",
        "* **In Logistic Regression:** Your equation ($\\eta$) could result in a value of **10.5**. But a probability ($\\mu$) cannot be 10.5; it must be between 0 and 1. The **Logit Link** acts as the bridge that \"squashes\" that 10.5 into a valid probability.\n",
        "* **In Poisson Regression:** Your equation ($\\eta$) might result in **-2.0**. But you can't have a count ($\\mu$) of negative apples. The **Log Link** ensures the final prediction is always positive."
      ],
      "metadata": {
        "id": "_si3xgEfjiym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transforming the Linear Equation to a Probability (Logistic Regression)\n",
        "\n",
        "To convert the raw \"score\" from a linear equation into a probability, we use the **Inverse Link Function** (specifically the **Logistic** or **Sigmoid** function). This ensures the output is always between 0 and 1.\n",
        "\n",
        "#### The Mathematical Formula\n",
        "Given the linear predictor $\\eta = \\beta_0 + \\beta_1X_1 + \\dots$, the probability $\\mu$ is calculated as:\n",
        "\n",
        "$$\\mu = \\frac{1}{1 + e^{-\\eta}}$$\n",
        "\n",
        "---\n",
        "\n",
        "#### Step-by-Step Example: Predicting Exam Success\n",
        "* **Scenario:** Predicting if a student passes (1) or fails (0) based on study hours.\n",
        "* **Model Coefficients:** Intercept $\\beta_0 = -3$, Slope $\\beta_1 = 1$.\n",
        "* **Input:** Student studied for **5 hours**.\n",
        "\n",
        "| Step | Operation | Formula / Calculation | Result |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **1** | **Calculate Score ($\\eta$)** | $\\eta = -3 + 1(5)$ | **2** |\n",
        "| **2** | **Apply Exponential** | $e^{-\\eta} = e^{-2}$ | **0.135** |\n",
        "| **3** | **Calculate Probability ($\\mu$)** | $\\mu = \\frac{1}{1 + 0.135}$ | **0.88** |\n",
        "\n",
        "**Conclusion:** The model predicts an **88% probability** ($\\mu = 0.88$) that the student will pass the exam."
      ],
      "metadata": {
        "id": "GawY7NH-m3BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Why Linear Fits Fail on Curved Data ---\n",
        "\n",
        "# Generate non-linear data (Polynomial)\n",
        "X_poly = 6 * np.random.rand(100, 1) - 3\n",
        "y_poly = 0.5 * X_poly**2 + X_poly + 2 + np.random.normal(0, 1, (100, 1))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_poly, y_poly, color='blue', label='Data')\n",
        "\n",
        "# Fit Linear Regression (Underfitting)\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y_poly)\n",
        "y_pred_lin = lin_reg.predict(X_poly)\n",
        "\n",
        "plt.plot(X_poly, y_pred_lin, color='red', label='Linear Fit (Underfitting)')\n",
        "\n",
        "# Fit Polynomial Regression (Better Fit)\n",
        "# In an interview, you might be asked how to fix the red line.\n",
        "# Answer: \"Feature Engineering - adding polynomial terms.\"\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly_trans = poly_features.fit_transform(X_poly)\n",
        "lin_reg_poly = LinearRegression()\n",
        "lin_reg_poly.fit(X_poly_trans, y_poly)\n",
        "\n",
        "# Sort for plotting\n",
        "X_new = np.linspace(-3, 3, 100).reshape(100, 1)\n",
        "X_new_poly = poly_features.transform(X_new)\n",
        "y_new = lin_reg_poly.predict(X_new_poly)\n",
        "\n",
        "plt.plot(X_new, y_new, \"g-\", linewidth=2, label=\"Polynomial Degree 2\")\n",
        "plt.legend()\n",
        "plt.title(\"Linear vs. Polynomial Regression\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LugFULXlvUxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression (Classification)\n",
        "\n",
        "### ðŸ’¡ Concept Review\n",
        "We move from predicting a value to predicting a class (0 or 1).\n",
        "To do this, we wrap our linear equation in a **Sigmoid** function to squash the output between 0 and 1.\n",
        "\n",
        "$$P(y=1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}$$\n",
        "\n",
        "### ðŸŽ¤ Interview Hot Seat\n",
        "**Interviewer:** \"What is the relationship between Probability, Odds, and Log-Odds?\"\n",
        "\n",
        "### ðŸŽ¤ Interview Hot Seat\n",
        "**Interviewer:** \"Is the decision threshold always 0.5?\"\n",
        "\n"
      ],
      "metadata": {
        "id": "VCXrGXinvewb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Sigmoid Visualization ---\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "z = np.linspace(-10, 10, 100)\n",
        "phi_z = sigmoid(z)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(z, phi_z)\n",
        "plt.axvline(0.0, color='k')\n",
        "plt.axhspan(0.0, 1.0, facecolor='1.0', alpha=1.0, ls='dotted')\n",
        "plt.axhline(y=0.5, ls='dotted', color='k', label='Default Threshold (0.5)')\n",
        "plt.yticks([0.0, 0.5, 1.0])\n",
        "plt.ylim(-0.1, 1.1)\n",
        "plt.title('The Sigmoid Function: Squashing outputs to Probabilities')\n",
        "plt.xlabel('z (Linear Combination)')\n",
        "plt.ylabel('$\\phi (z)$ (Probability)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Quick Logistic Regression on our Ad Data (converting Sales to High/Low)\n",
        "# Class 1 = High Sales (> 20), Class 0 = Low Sales\n",
        "df_ads['High_Sales'] = (df_ads['Sales'] > 20).astype(int)\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(df_ads[['TV']], df_ads['High_Sales'])\n",
        "\n",
        "# Check coefficient\n",
        "print(f\"Coefficient for TV: {log_reg.coef_[0][0]}\")\n",
        "# Interpretation: Positive coef means higher TV spend increases log-odds of having 'High Sales'.\n"
      ],
      "metadata": {
        "id": "vJFZlxIEvveo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Trees & Random Forests\n",
        "\n",
        "### ðŸ’¡ Concept Review\n",
        "**Decision Trees** split data based on rules (e.g., if TV Spend > 100). They are prone to **overfitting** (memorizing the noise).\n",
        "**Random Forests** solve this via **Bagging** (Bootstrap Aggregating):\n",
        "1.  Train many trees on random subsets of data.\n",
        "2.  Each tree sees only a subset of features.\n",
        "3.  Average the results (Regression) or Vote (Classification).\n",
        "\n",
        "### ðŸŽ¤ Interview Hot Seat\n",
        "**Interviewer:** \"What are hyperparameters, and how do we tune them?\"\n"
      ],
      "metadata": {
        "id": "ENtgt46Iv386"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search in Action ---\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load Data\n",
        "data = load_iris()\n",
        "X_iris = data.data\n",
        "y_iris = data.target\n",
        "\n",
        "# Split\n",
        "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# [cite_start]Define Grid (The \"Hyperparameter Space\") [cite: 5776]\n",
        "# We are testing different depths and number of estimators\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# TODO: Initialize Grid Search with 5-fold cross-validation\n",
        "# HINT: Look at the variable names above\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5)\n",
        "\n",
        "# Fit\n",
        "print(\"Training Grid Search... (this might take a second)\")\n",
        "grid_search.fit(X_train_i, y_train_i)\n",
        "\n",
        "# Results\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-Val Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Final Test\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred_i = best_rf.predict(X_test_i)\n",
        "print(f\"Test Set Accuracy: {accuracy_score(y_test_i, y_pred_i):.4f}\")"
      ],
      "metadata": {
        "id": "VdoEVyBmwD8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section Wrap-Up\n",
        "\n",
        "In this section, we covered the core \"Whiteboard\" topics for supervised learning:\n",
        "1.  **Linear Regression:** Interpreting coefficients as unit changes.\n",
        "2.  **GLMs:** Matching the distribution to the data type.\n",
        "3.  **Logistic Regression:** Understanding the Sigmoid and thresholds.\n",
        "4.  **Random Forests:** Using Ensembles to fight overfitting.\n",
        "\n",
        "**Next Steps:** Review the supporting PDFs for deeper derivations of the math (specifically the \"Linear Regression Metrics\" and \"GLMs\" PDFs).\n"
      ],
      "metadata": {
        "id": "sc6GsH_dtJIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2\n",
        "\n",
        "For Part 2, we shift gears from \"Predicting the Answer\" to \"Finding the Question.\" In interviews, Unsupervised Learning often trips candidates up because there is no clear \"accuracy\" metric to hide behind. They have to defend *why* a cluster is meaningful."
      ],
      "metadata": {
        "id": "HPJn1u5M15c8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised Learning & Pattern Discovery\n",
        "\n",
        "## ðŸŽ¯ Objective\n",
        "In Supervised Learning, we had a target variable ($y$). In Unsupervised Learning, we are flying blind. We are looking for **structure** in the data.\n",
        "**The Interview Trap:** Candidates often run K-Means, get 3 clusters, and stop. A good Data Scientist asks:\n",
        "1.  \"Are these clusters real or just forced?\"\n",
        "2.  \"How do we visualize 50 dimensions?\"\n",
        "3.  \"What does 'Cluster 1' actually represent in the real world?\"\n",
        "\n",
        "## ðŸ“š Topics Covered\n",
        "* **The Curse of Dimensionality:** Why distance breaks down in high dimensions.\n",
        "* **PCA (Principal Component Analysis):** Squashing dimensions while keeping the \"story\" (variance).\n",
        "* **K-Means Clustering:** Partitioning data and finding the \"Elbow.\"\n",
        "* **Evaluation:** Silhouette Scores (because we don't have Accuracy).\n",
        "* **DBSCAN:** When clusters aren't perfect circles.\n",
        "`\n"
      ],
      "metadata": {
        "id": "1brlVk202UvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import load_wine, make_blobs, make_moons\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "\n",
        "# Configuration\n",
        "sns.set_style(\"whitegrid\")\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "id": "khIbnfnj2hdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 1: The Curse of Dimensionality & PCA\n",
        "\n",
        "### ðŸ’¡ Concept Review\n",
        "As you add more features (dimensions), the \"volume\" of the space increases so fast that the available data becomes sparse. In high dimensions, **Euclidean distance** (the basis of most clustering) loses meaning because every point is roughly equidistant from every other point.\n",
        "\n",
        "**PCA** rotates the data to align with the directions of maximum variance (information) and discards the directions of noise.\n",
        "\n",
        "### ðŸŽ¤ Interview Hot Seat\n",
        "**Interviewer:** \"I have a dataset with 100 features. Should I just feed all of them into K-Means?\"\n"
      ],
      "metadata": {
        "id": "1Ey7KOxx2gYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the \"Squash\" ---\n",
        "# Load the Wine Dataset (13 features)\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target # We won't use y for training, only for coloring the plot to check our work.\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# 1. Standardize the Data (CRITICAL for PCA and K-Means)\n",
        "# Why? Because 'Proline' is ~1000 and 'Hue' is ~1. PCA follows the largest scale if not normalized.\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 2. Apply PCA\n",
        "pca = PCA(n_components=2) # Squash 13 dimensions down to 2\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# 3. Visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "# We color by 'y' just to see if PCA kept the classes separated naturally\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=60)\n",
        "plt.title(f'PCA: Reduced 13 dims to 2. Explained Variance: {sum(pca.explained_variance_ratio_):.2f}')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.colorbar(scatter, label='True Wine Class')\n",
        "plt.show()\n",
        "\n",
        "# Print the \"Variance Ratio\" - How much info did we keep?\n",
        "print(f\"Variance explained by each component: {pca.explained_variance_ratio_}\")\n"
      ],
      "metadata": {
        "id": "6MC8PaBS209O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 2: K-Means Clustering\n",
        "\n",
        "### ðŸ’¡ Concept Review\n",
        "K-Means tries to separate data into $K$ groups of equal variance, minimizing the **Inertia** (sum of squared distances to the nearest cluster center).\n",
        "\n",
        "**The Algorithm:**\n",
        "1.  Pick $K$ random centroids.\n",
        "2.  Assign every point to the nearest centroid.\n",
        "3.  Move the centroid to the average of its assigned points.\n",
        "4.  Repeat until they stop moving.\n",
        "\n",
        "### ðŸŽ¤ Interview Hot Seat\n",
        "**Interviewer:** \"How do you handle categorical data in K-Means?\"\n",
        "* **Better Approach:** Use **K-Modes** (for categorical) or **K-Prototypes** (mixed), or embed the categories into a vector space first.\n"
      ],
      "metadata": {
        "id": "mq8ptes93AlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Elbow Method ---\n",
        "\n",
        "# Let's pretend we don't know there are 3 wine classes. Let's find K.\n",
        "inertia = []\n",
        "range_k = range(1, 10)\n",
        "\n",
        "for k in range_k:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range_k, inertia, 'bx-')\n",
        "plt.xlabel('k (Number of clusters)')\n",
        "plt.ylabel('Inertia (Sum of Squared Distances)')\n",
        "plt.title('The Elbow Method: Showing the optimal k')\n",
        "plt.show()\n",
        "\n",
        "# DISCUSSION:\n",
        "# Where is the \"Elbow\"? (Likely around 3).\n",
        "# Inertia ALWAYS goes down as K increases. We want the point of diminishing returns.\n"
      ],
      "metadata": {
        "id": "imS-IY053I19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 3: Evaluation (Silhouette Score)\n",
        "\n",
        "### ðŸ’¡ Concept Review\n",
        "Since we don't have labels, we can't calculate Accuracy. We use **Silhouette Score**, which measures:\n",
        "* **Cohesion:** How close is the point to its own cluster?\n",
        "* **Separation:** How far is the point from the *next nearest* cluster?\n",
        "\n",
        "Range: -1 (Wrong cluster) to +1 (Perfect cluster). 0 means overlapping.\n",
        "\n",
        "### ðŸŽ¤ Interview Hot Seat\n",
        "**Interviewer:** \"Your Elbow plot is ambiguous. It looks like a smooth curve. How do you decide on K?\"\n",
        "\n"
      ],
      "metadata": {
        "id": "2L5tB46h3Rqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Silhouette Analysis ---\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "for k in [2, 3, 4, 5]:\n",
        "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = km.fit_predict(X_scaled)\n",
        "    score = silhouette_score(X_scaled, labels)\n",
        "    print(f\"For n_clusters = {k}, the Silhouette Score is: {score:.3f}\")\n",
        "\n",
        "# Look for the highest score.\n",
        "# Note: Sometimes K=3 is best, sometimes K=2. In Unsupervised learning, \"Truth\" is relative."
      ],
      "metadata": {
        "id": "BnnITHwE3X2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 4: When K-Means Fails (DBSCAN)\n",
        "\n",
        "### ðŸ’¡ Concept Review\n",
        "K-Means assumes clusters are **globular** (spherical blobs). It fails on:\n",
        "* Crescents or moons.\n",
        "* Concentric circles (donuts).\n",
        "* Data with lots of noise/outliers.\n",
        "\n",
        "**DBSCAN** (Density-Based Spatial Clustering) groups points that are close together and marks points in low-density regions as **Noise (-1)**.\n",
        "* **Epsilon:** The radius to search.\n",
        "* **Min_Samples:** Minimum points to form a dense region.\n"
      ],
      "metadata": {
        "id": "Ri6tlyjL3itO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- VISUALIZATION: K-Means vs DBSCAN ---\n",
        "# Generate \"Moon\" data (Non-spherical)\n",
        "X_moons, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
        "\n",
        "# Fit K-Means\n",
        "kmeans_moon = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "labels_km = kmeans_moon.fit_predict(X_moons)\n",
        "\n",
        "# Fit DBSCAN\n",
        "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
        "labels_db = dbscan.fit_predict(X_moons)\n",
        "\n",
        "# Plot Comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# K-Means Plot\n",
        "axes[0].scatter(X_moons[:, 0], X_moons[:, 1], c=labels_km, cmap='plasma')\n",
        "axes[0].set_title('K-Means (Fails on shapes)')\n",
        "\n",
        "# DBSCAN Plot\n",
        "# Note: Points labeled -1 are noise\n",
        "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=labels_db, cmap='plasma')\n",
        "axes[1].set_title('DBSCAN (Captures shapes)')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "E0QHByjb3t7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section Wrap-Up\n",
        "\n",
        "In this section, we explored the \"Dark Arts\" of Unsupervised Learning:\n",
        "\n",
        "1.  **PCA:** Pre-processing to remove noise and curse of dimensionality.\n",
        "\n",
        "2.  **K-Means:** The workhorse for spherical clusters.\n",
        "\n",
        "3.  **Silhouette Score:** How we grade ourselves without an answer key.\n",
        "\n",
        "4.  **DBSCAN:** The alternative for complex shapes and outlier detection."
      ],
      "metadata": {
        "id": "lZ1H0Gez10eT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3"
      ],
      "metadata": {
        "id": "OVn5IgKSEmUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Learning Paradigms\n",
        "\n",
        "## ðŸŽ¯ Objective\n",
        "We have covered Supervised (Labeled) and Unsupervised (Unlabeled) learning. Now we look at **how** we learn when the data is scarce, decentralized, or requires interaction.\n",
        "* **Reinforcement Learning:** Learning by trial and error (Interaction).\n",
        "* **Transfer Learning:** Learning from someone else's experience (Reuse).\n",
        "* **Federated Learning:** Learning without seeing the data (Privacy).\n",
        "* **Semi-Supervised:** Learning with a tiny bit of help (Efficiency).\n",
        "\n",
        "## ðŸ“š Topics Covered\n",
        "* **Markov Decision Processes (MDPs):** States, Actions, Rewards.\n",
        "* **Q-Learning:** The Bellman Equation in code.\n",
        "* **Pre-training & Fine-tuning:** The \"Warm Start.\"\n",
        "* **FedAvg:** Averaging weights from decentralized devices."
      ],
      "metadata": {
        "id": "LAJGTPwAE5dI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import standard libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Configuration\n",
        "sns.set_style(\"whitegrid\")\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "id": "QWl6qi1fFCqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 1: Reinforcement Learning (RL)\n",
        "\n",
        "## ðŸ’¡ Concept Review\n",
        "RL is about an **Agent** interacting with an **Environment** to maximize a **Reward**.\n",
        "\n",
        "**The MDP Tuple: $(S, A, P, R, \\gamma)$**\n",
        "\n",
        "In Reinforcement Learning, the **Markov Decision Process (MDP)** provides the mathematical framework for modeling decision-making where outcomes are partly random and partly under the control of a decision-maker (the agent).\n",
        "\n",
        "| Symbol | Name | Mathematical Form | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **$S$** | **States** | $s \\in S$ | The set of all possible \"situations\" or configurations of the environment. |\n",
        "| **$A$** | **Actions** | $a \\in A$ | The set of all possible moves or decisions the agent can make. |\n",
        "| **$P$** | **Transition Probability** | $P(s' \\mid s, a)$ | The \"physics\" of the world: the probability of landing in state $s'$ after taking action $a$ in state $s$. |\n",
        "| **$R$** | **Reward Function** | $R(s, a)$ | The immediate scalar feedback (score) received after taking an action. |\n",
        "| **$\\gamma$** | **Discount Factor** | $\\gamma \\in [0, 1]$ | Determines the present value of future rewards (how \"farsighted\" the agent is). |\n",
        "\n",
        "---\n",
        "\n",
        "### Deep Dive on $P$ (Transition Dynamics)\n",
        "The **$P$** component is critical because it defines whether an environment is **Deterministic** or **Stochastic**:\n",
        "\n",
        "* **Deterministic:** If you take action $a$ in state $s$, you *always* end up in $s'$. ($P = 1$)\n",
        "* **Stochastic (Probabilistic):** Even if you take action $a$, there is a distribution of possible next states. For example, a robot trying to move forward on ice might have a $0.8$ probability of moving forward and $0.1$ probability of slipping left or right.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### The Goal of the Agent\n",
        "The agent's objective is to find a **Policy** ($\\pi$), which is a mapping from states to actions ($\\pi: S \\rightarrow A$), that maximizes the **Expected Return**:\n",
        "\n",
        "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots$$\n",
        "\n",
        "*Note: This return formula uses an **Exponential Decay** via the Discount Factor ($\\gamma$) to ensure that rewards in the far future are worth less than immediate rewards.*\n",
        "\n",
        "### The Q-Learning Update Rule (The \"Interviewer's Favorite Equation\")\n",
        "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha [R + \\gamma \\max_{a'} Q(s', a') - Q(s,a)]$$\n",
        "We update our current estimate of \"Quality\" ($Q$) based on the reward we just got plus the best possible future reward.\n",
        "\n",
        "### ðŸŽ¤ Interview Hot Seat\n",
        "**Interviewer:** \"What is the difference between Exploration and Exploitation?\"\n"
      ],
      "metadata": {
        "id": "5xE9YdmKFKkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CODE SIMULATION: The \"Cliff Walking\" GridWorld ---\n",
        "# We will build a simple environment and solve it with Q-Learning.\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.height = 3\n",
        "        self.width = 4\n",
        "        self.goal = (0, 3) # Top right\n",
        "        self.trap = (1, 3) # Right below goal\n",
        "        self.state = (2, 0) # Start at bottom left\n",
        "\n",
        "    def step(self, action):\n",
        "        # Actions: 0=Up, 1=Down, 2=Left, 3=Right\n",
        "        y, x = self.state\n",
        "        if action == 0: y = max(0, y - 1)\n",
        "        elif action == 1: y = min(self.height - 1, y + 1)\n",
        "        elif action == 2: x = max(0, x - 1)\n",
        "        elif action == 3: x = min(self.width - 1, x + 1)\n",
        "\n",
        "        new_state = (y, x)\n",
        "        self.state = new_state\n",
        "\n",
        "        # Rewards\n",
        "        if new_state == self.goal: return new_state, 10, True  # Win!\n",
        "        if new_state == self.trap: return new_state, -10, True # Dead!\n",
        "        return new_state, -1, False # Living cost (encourages speed)\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = (2, 0)\n",
        "        return self.state\n",
        "\n",
        "# Q-Learning\n",
        "env = GridWorld()\n",
        "q_table = np.zeros((env.height, env.width, 4)) # 3x4 grid, 4 actions\n",
        "alpha = 0.1  # Learning Rate\n",
        "gamma = 0.9  # Discount Factor\n",
        "epsilon = 0.1 # Exploration Rate\n",
        "\n",
        "# Training Loop\n",
        "for episode in range(500):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Epsilon-Greedy Strategy\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            action = np.random.randint(0, 4) # Explore\n",
        "        else:\n",
        "            action = np.argmax(q_table[state[0], state[1]]) # Exploit\n",
        "\n",
        "        next_state, reward, done = env.step(action)\n",
        "\n",
        "        # Update Q-Value (Bellman Equation)\n",
        "        old_value = q_table[state[0], state[1], action]\n",
        "        next_max = np.max(q_table[next_state[0], next_state[1]])\n",
        "        new_value = old_value + alpha * (reward + gamma * next_max - old_value)\n",
        "\n",
        "        q_table[state[0], state[1], action] = new_value\n",
        "        state = next_state\n",
        "\n",
        "# Visualization of the Learned Policy\n",
        "print(\"Final Q-Table (Best Action per Cell):\")\n",
        "actions = [\"â†‘\", \"â†“\", \"â†\", \"â†’\"]\n",
        "grid = [['' for _ in range(4)] for _ in range(3)]\n",
        "for y in range(3):\n",
        "    for x in range(4):\n",
        "        best_action = np.argmax(q_table[y, x])\n",
        "        grid[y][x] = actions[best_action]\n",
        "\n",
        "grid[0][3] = \"GOAL\"\n",
        "grid[1][3] = \"TRAP\"\n",
        "for row in grid:\n",
        "    print(row)\n"
      ],
      "metadata": {
        "id": "FaXqihqKFWvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 2: Transfer Learning\n",
        "\n",
        "### ðŸ’¡ Concept Review\n",
        "Training Deep Learning models from scratch is expensive (weeks of GPU time).\n",
        "**Transfer Learning** allows us to take a model trained on a large dataset (e.g., ImageNet) and \"fine-tune\" it on a smaller, specific dataset.\n",
        "\n",
        "**Strategies:**\n",
        "1.  **Feature Extraction:** Freeze the backbone (the complex part), only retrain the last layer (the classifier).\n",
        "2.  **Fine-Tuning:** Unfreeze everything but use a very small learning rate to adjust weights slightly.\n",
        "3. https://www.researchgate.net/figure/The-architecture-of-our-transfer-learning-model_fig4_342400905\n",
        "\n",
        "### ðŸŽ¤ Interview Hot Seat\n",
        "**Interviewer:** \"I have a small dataset of medical images (1000 X-rays). Should I train a ResNet-50 from scratch?\"\n"
      ],
      "metadata": {
        "id": "9zR_5JmxFiSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFER LEARNING DEMO WITH EXPLANATORY PRINTS\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# STEP 1: Generate Dataset\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\nSTEP 1: Generating synthetic dataset...\\n\")\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_classes=4,\n",
        "    n_informative=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Dataset shape:\", X.shape)\n",
        "print(\"Unique classes:\", set(y))\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# STEP 2: Split into Source and Target Tasks\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\nSTEP 2: Splitting into Source Task (0,1) and Target Task (2,3)\\n\")\n",
        "\n",
        "mask_source = (y < 2)\n",
        "mask_target = (y >= 2)\n",
        "\n",
        "X_source, y_source = X[mask_source], y[mask_source]\n",
        "X_target, y_target = X[mask_target], y[mask_target]\n",
        "\n",
        "print(\"Source samples:\", len(y_source))\n",
        "print(\"Target samples:\", len(y_target))\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# STEP 3: Create Model\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\nSTEP 3: Creating Neural Network\\n\")\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(10,), random_state=42)\n",
        "\n",
        "print(\"Model created.\")\n",
        "print(\"Hidden layer size:\", mlp.hidden_layer_sizes)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# STEP 4: Initialize Model with ALL classes\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\nSTEP 4: Initializing model with ALL classes\\n\")\n",
        "\n",
        "all_classes = [0,1,2,3]\n",
        "\n",
        "mlp.partial_fit(X_source, y_source, classes=all_classes)\n",
        "\n",
        "print(\"Classes known to model:\", mlp.classes_)\n",
        "print(\"Output layer size:\", len(mlp.classes_))\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# STEP 5: Train on Source Task\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\nSTEP 5: Training on Source Task\\n\")\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "for i in range(epochs):\n",
        "\n",
        "    mlp.partial_fit(X_source, y_source)\n",
        "\n",
        "    if i in [0, 1, 5, 10, 25, 49]:\n",
        "\n",
        "        acc = mlp.score(X_source, y_source)\n",
        "\n",
        "        print(f\"Epoch {i+1:2d}  Source Accuracy: {acc:.3f}\")\n",
        "\n",
        "\n",
        "print(\"\\nFinal Source Accuracy:\", mlp.score(X_source, y_source))\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# STEP 6: TRANSFER LEARNING BEGINS HERE\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\nSTEP 6: Transfer Learning Begins (Fine-Tuning on Target Task)\\n\")\n",
        "\n",
        "print(\"IMPORTANT: We are NOT creating a new model.\")\n",
        "print(\"We are continuing training with EXISTING learned weights.\\n\")\n",
        "\n",
        "\n",
        "epochs_target = 50\n",
        "\n",
        "for i in range(epochs_target):\n",
        "\n",
        "    mlp.partial_fit(X_target, y_target)\n",
        "\n",
        "    if i in [0, 1, 2, 5, 10, 25, 49]:\n",
        "\n",
        "        acc = mlp.score(X_target, y_target)\n",
        "\n",
        "        print(f\"Epoch {i+1:2d}  Target Accuracy: {acc:.3f}\")\n",
        "\n",
        "\n",
        "print(\"\\nFinal Target Accuracy (Transfer Learning):\",\n",
        "      mlp.score(X_target, y_target))\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# STEP 7: Train From Scratch (Control Group)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\nSTEP 7: Training Separate Model From Scratch\\n\")\n",
        "\n",
        "mlp_scratch = MLPClassifier(\n",
        "    hidden_layer_sizes=(10,),\n",
        "    max_iter=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "mlp_scratch.fit(X_target, y_target)\n",
        "\n",
        "print(\"\\nScratch Model Target Accuracy:\",\n",
        "      mlp_scratch.score(X_target, y_target))\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# STEP 8: FINAL COMPARISON\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\nSTEP 8: FINAL COMPARISON\\n\")\n",
        "\n",
        "print(\"Transfer Learning Accuracy:\",\n",
        "      mlp.score(X_target, y_target))\n",
        "\n",
        "print(\"Scratch Accuracy:\",\n",
        "      mlp_scratch.score(X_target, y_target))\n"
      ],
      "metadata": {
        "id": "D-Vv7lyMFsOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 3: Federated Learning\n",
        "\n",
        "### ðŸ’¡ Concept Review\n",
        "Standard ML requires collecting all data into one central server. This raises **Privacy** and **Bandwidth** concerns (e.g., typing prediction on phones).\n",
        "**Federated Learning (FedAvg)** reverses this:\n",
        "1.  Send the *Model* to the device (Client).\n",
        "2.  Train locally on private data.\n",
        "3.  Send only the *Weights* (updates) back to the server.\n",
        "4.  Average the weights to create a new Global Model.\n",
        "\n",
        "### ðŸŽ¤ Interview Hot Seat\n",
        "**Interviewer:** \"If I use Federated Learning, can I guarantee user privacy?\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7ASwRIpBF9ON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CODE SIMULATION: Federated Averaging (FedAvg) ---\n",
        "\n",
        "# Simulate Global Data\n",
        "X_fed, y_fed = make_classification(n_samples=1000, n_features=5, random_state=42)\n",
        "X_test_fed = X_fed[-200:]\n",
        "y_test_fed = y_fed[-200:]\n",
        "\n",
        "# Simulate 3 Clients (Data Silos)\n",
        "client_1_X, client_1_y = X_fed[0:200], y_fed[0:200]\n",
        "client_2_X, client_2_y = X_fed[200:400], y_fed[200:400]\n",
        "client_3_X, client_3_y = X_fed[400:600], y_fed[400:600]\n",
        "\n",
        "# Initialize 3 Local Models (Identical Architecture)\n",
        "model_1 = SGDClassifier(loss='log_loss', random_state=42)\n",
        "model_2 = SGDClassifier(loss='log_loss', random_state=42)\n",
        "model_3 = SGDClassifier(loss='log_loss', random_state=42)\n",
        "\n",
        "# Train Locally (Server sees NONE of this data)\n",
        "model_1.partial_fit(client_1_X, client_1_y, classes=[0,1])\n",
        "model_2.partial_fit(client_2_X, client_2_y, classes=[0,1])\n",
        "model_3.partial_fit(client_3_X, client_3_y, classes=[0,1])\n",
        "\n",
        "# The Server Aggregation Step (FedAvg)\n",
        "# We average the coefficients (weights) and intercepts\n",
        "avg_coef = (model_1.coef_ + model_2.coef_ + model_3.coef_) / 3\n",
        "avg_intercept = (model_1.intercept_ + model_2.intercept_ + model_3.intercept_) / 3\n",
        "\n",
        "# Create Global Model with Averaged Weights\n",
        "global_model = SGDClassifier(loss='log_loss', random_state=42)\n",
        "global_model.partial_fit(X_test_fed[:5], y_test_fed[:5], classes=[0,1]) # Initialize structure\n",
        "global_model.coef_ = avg_coef\n",
        "global_model.intercept_ = avg_intercept\n",
        "\n",
        "# Evaluate\n",
        "print(f\"Client 1 Accuracy: {model_1.score(X_test_fed, y_test_fed):.2f}\")\n",
        "print(f\"Global Model (Federated) Accuracy: {global_model.score(X_test_fed, y_test_fed):.2f}\")\n",
        "\n",
        "# DISCUSSION:\n",
        "# The Global Model performs well without ever seeing the full training set in one place!\n"
      ],
      "metadata": {
        "id": "ogNvAGw7GEG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 4: Semi-Supervised Learning (Label Propagation)\n",
        "\n",
        "### ðŸ’¡ Concept Review\n",
        "What if you have 1,000,000 images but only 100 are labeled?\n",
        "**Label Propagation** assumes that points close to each other in high-dimensional space likely share the same label. It \"spreads\" the known labels to nearby unknown neighbors.\n",
        "\n",
        "**Code Tip:** In `sklearn`, we use `-1` to denote unlabeled data.\n",
        "\n"
      ],
      "metadata": {
        "id": "AHScXE6wGQKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CODE DEMO: Label Propagation ---\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "\n",
        "# Generate Data\n",
        "X_semi, y_semi = make_classification(n_samples=200, n_features=2, n_redundant=0, random_state=42)\n",
        "\n",
        "# Create \"Unlabeled\" Dataset\n",
        "y_train_mixed = np.copy(y_semi)\n",
        "# Mask 95% of labels (Set to -1)\n",
        "rng = np.random.RandomState(42)\n",
        "random_unlabeled_points = rng.rand(len(y_semi)) < 0.95\n",
        "y_train_mixed[random_unlabeled_points] = -1\n",
        "\n",
        "print(f\"Total Points: {len(y_semi)}\")\n",
        "print(f\"Labeled Points: {len(y_semi) - np.sum(random_unlabeled_points)}\")\n",
        "\n",
        "# Train Label Spreading Model\n",
        "lp_model = LabelSpreading(kernel='knn', alpha=0.8)\n",
        "lp_model.fit(X_semi, y_train_mixed)\n",
        "\n",
        "# Evaluate on the ground truth of the masked points\n",
        "predicted_labels = lp_model.transduction_[random_unlabeled_points]\n",
        "true_labels = y_semi[random_unlabeled_points]\n",
        "\n",
        "print(f\"Accuracy on Unlabeled Data: {accuracy_score(true_labels, predicted_labels):.2f}\")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Plot unlabeled (now predicted)\n",
        "plt.scatter(X_semi[random_unlabeled_points, 0], X_semi[random_unlabeled_points, 1],\n",
        "            c=predicted_labels, cmap='viridis', marker='.', label='Propagated Labels')\n",
        "# Plot original labeled\n",
        "plt.scatter(X_semi[~random_unlabeled_points, 0], X_semi[~random_unlabeled_points, 1],\n",
        "            c=y_semi[~random_unlabeled_points], cmap='viridis', marker='s', s=100, edgecolor='k', label='Original Labeled')\n",
        "plt.legend()\n",
        "plt.title(\"Semi-Supervised Learning: Spreading 5% labels to 95% data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "G4PKRQ23GZbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section Wrap-Up\n",
        "\n",
        "In this section, we stepped outside the traditional \"Fit-Predict\" loop.\n",
        "\n",
        "1.  **RL:** We built an agent that learns from *consequences*.\n",
        "\n",
        "2.  **Transfer:** We learned how to stand on the shoulders of giants (pre-trained models).\n",
        "\n",
        "3.  **Federated:** We learned how to train without centralizing data (privacy).\n",
        "\n",
        "4.  **Semi-Supervised:** We learned how to leverage unlabeled data.\n",
        "\n"
      ],
      "metadata": {
        "id": "yN7iHAhYEZTK"
      }
    }
  ]
}