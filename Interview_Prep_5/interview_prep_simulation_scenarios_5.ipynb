{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyM8HndkkhdNpDyHnPfmodEa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Interview Prep Simulation Scenarios 5\n","\n","Your Name"],"metadata":{"id":"xq1M7NzMxZNz"}},{"cell_type":"markdown","source":["## Getting Started\n","\n","* Colab - get notebook from the gitmystuff DTSC5082 repository\n","* Save a Copy in Drive\n","* Rename the file using your name, just your name (Your Name.ipynb)\n","* Edit your name in the cell above - your name should be the same as the filename\n","* Clean up Colab Notebooks folder\n","* Submit shared link"],"metadata":{"id":"tv86CANY3_Xy"}},{"cell_type":"markdown","source":["**Grading:** Pass/Fail (Complete all sections with justification)\n","\n","# üìù Part 1: Scenario\n","You have passed the initial phone screen for a Data Scientist role at **\"OmniCorp,\"** a mid-sized tech company. They have sent you this Jupyter Notebook as a take-home assessment.\n","\n","**Your Instructions from the Hiring Manager:**\n","> \"We've attached some synthetic data representing two of our core business problems: estimating server costs (Regression) and predicting customer churn (Classification).\n",">\n","> We don't care about hitting 99% accuracy. We care about **how you think**.\n","> 1.  Check your assumptions.\n","> 2.  Tell us what the coefficients *mean*.\n","> 3.  Make a business recommendation based on your model.\"\n","\n","## üõ†Ô∏è Instructions\n","1.  Run the \"Data Generation\" cells to create your dataset.\n","2.  Complete the **TODO** sections.\n","3.  Answer the **Interview Questions** in the Markdown cells provided.\n"],"metadata":{"id":"YpqUCaSSxgZM"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression, LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import mean_squared_error, classification_report, confusion_matrix, precision_recall_curve\n","\n","# Setup\n","np.random.seed(101)\n","sns.set_style(\"whitegrid\")\n","%matplotlib inline\n"],"metadata":{"id":"v9nPUwlQy3cQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üìä Task 1: Regression (Server Cost Estimation)\n","\n","**Context:** OmniCorp wants to predict the hourly cost of running their cloud servers based on three features:\n","1.  `CPU_Usage` (Percentage 0-100)\n","2.  `Memory_Usage` (GB)\n","3.  `Server_Age` (Years)\n","\n","**The Catch:** The relationship isn't perfectly linear. Old servers become exponentially more expensive to maintain.\n"],"metadata":{"id":"y9Nl9ezUy9lw"}},{"cell_type":"code","source":["\n","# --- DATA GENERATION (DO NOT MODIFY) ---\n","n = 1000\n","cpu = np.random.uniform(10, 90, n)\n","memory = np.random.uniform(4, 64, n)\n","age = np.random.uniform(0, 10, n)\n","\n","# True relationship: Cost = Base + 2*CPU + 5*Memory + Exp(0.5 * Age)\n","# This exponential term on 'age' is the trap for a standard linear model.\n","cost = 50 + 2 * cpu + 5 * memory + np.exp(0.5 * age) + np.random.normal(0, 10, n)\n","\n","df_servers = pd.DataFrame({'CPU': cpu, 'Memory': memory, 'Age': age, 'Cost': cost})\n","\n","# Split Data\n","X_reg = df_servers[['CPU', 'Memory', 'Age']]\n","y_reg = df_servers['Cost']\n","X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reg, y_reg, test_size=0.2, random_state=101)\n","\n","df_servers.head()"],"metadata":{"id":"O07bUiYAzH1F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# üíª TODO: Fit a Standard Linear Regression Model (OLS)\n","# 1. Instantiate and fit LinearRegression to X_train_r, y_train_r\n","# 2. Predict on X_test_r\n","# 3. Calculate and print the RMSE (Root Mean Squared Error)\n","\n","# [YOUR CODE HERE]\n","\n","# üíª TODO: Residual Plot\n","# Plot the Predicted Values (x-axis) vs. Residuals (Actual - Predicted) (y-axis).\n","# Look for a pattern. If it's random cloud, good. If it's a funnel or curve, bad.\n","\n","# [YOUR CODE HERE]\n"],"metadata":{"id":"03Lnv1xWzPTp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üé§ Interview Question 1\n","**The Hiring Manager asks:** \"We noticed your Residual Plot has a distinct 'funnel' or 'U' shape. What does that imply about the assumptions of Linear Regression? How would you fix this model to better capture the server age dynamics?\"\n","\n","**Your Answer:**\n","* **Observation:** [Write here]\n","* **Violation:** [Which OLS assumption is violated? Homoscedasticity? Linearity?]\n","* **Fix:** [Would you use a Log Transform? A Polynomial feature? A Gamma GLM? Pick one and explain why.]\n"],"metadata":{"id":"xtUNyAeYzXae"}},{"cell_type":"markdown","source":["## üìâ Task 2: Classification (Churn Prediction)\n","\n","**Context:** The marketing team wants to predict which customers will cancel their subscription (`Churn = 1`) next month.\n","**The Catch:** The data is **imbalanced**. Most customers are happy (0); only a few churn (1).\n"],"metadata":{"id":"ji7zHX1Bziox"}},{"cell_type":"code","source":["# --- DATA GENERATION (DO NOT MODIFY) ---\n","from sklearn.datasets import make_classification\n","\n","# Generate Imbalanced Data (90% Class 0, 10% Class 1)\n","X_cls, y_cls = make_classification(n_samples=1000, n_features=5, n_informative=3,\n","                                   n_redundant=1, n_classes=2, weights=[0.9, 0.1],\n","                                   flip_y=0.01, random_state=42)\n","\n","# Convert to DataFrame for readability\n","cols = ['Avg_Login_Time', 'Support_Tickets', 'Bill_Amount', 'Feature_4', 'Feature_5']\n","df_churn = pd.DataFrame(X_cls, columns=cols)\n","df_churn['Churn'] = y_cls\n","\n","X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(df_churn[cols], df_churn['Churn'],\n","                                                            test_size=0.3, stratify=df_churn['Churn'],\n","                                                            random_state=42)"],"metadata":{"id":"95MKKeIXzq1q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# üíª TODO: Fit a Logistic Regression Model\n","# 1. Fit LogisticRegression to the training data.\n","# 2. Print the Accuracy Score.\n","\n","# [YOUR CODE HERE]\n","\n","# üíª TODO: Print the Confusion Matrix\n","# Use sklearn.metrics.confusion_matrix\n","\n","# [YOUR CODE HERE]\n"],"metadata":{"id":"0t-HlK8DzwH6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üé§ Interview Question 2\n","**The Hiring Manager asks:** \"You reported an Accuracy of 90%+. That sounds amazing! But looking at the Confusion Matrix, how many *actual* churners did we catch? Why is 'Accuracy' a dangerous metric for this specific dataset?\"\n","\n","**Your Answer:**\n","* [Discuss the False Negatives vs False Positives]\n","* [Explain why Accuracy is misleading in imbalanced datasets]\n"],"metadata":{"id":"n2Aio0noz2oP"}},{"cell_type":"markdown","source":["## ‚öñÔ∏è Task 3: The Business Trade-off (Thresholding)\n","\n","**Context:**\n","\n","* A marketing campaign to retain a user costs **$10**.\n","\n","* If we fail to identify a churner, we lose their **$100** lifetime value.\n","\n","* **False Positive Cost:** $10 (Wasted marketing on a happy user).\n","\n","* **False Negative Cost:** $100 (Lost customer).\n","\n","\n","We cannot treat these errors equally. We need to adjust our **Decision Threshold**.\n"],"metadata":{"id":"pYgusaaXz9Th"}},{"cell_type":"code","source":["# üíª TODO: Threshold Moving\n","# 1. Get the predicted probabilities using .predict_proba() (look at column 1 for positive class)\n","# 2. Instead of the default 0.5 threshold, try a threshold of 0.2 (be more aggressive in predicting churn).\n","# 3. Calculate the \"Business Cost\" for both thresholds.\n","\n","# HINT:\n","# Cost = (False Positives * 10) + (False Negatives * 100)\n","\n","model_log = LogisticRegression()\n","model_log.fit(X_train_c, y_train_c)\n","probs = model_log.predict_proba(X_test_c)[:, 1]\n","\n","# Default Threshold (0.5)\n","preds_default = (probs > 0.5).astype(int)\n","tn, fp, fn, tp = confusion_matrix(y_test_c, preds_default).ravel()\n","cost_default = (fp * 10) + (fn * 100)\n","\n","# Aggressive Threshold (0.2)\n","preds_aggressive = (probs > 0.2).astype(int)\n","tn_a, fp_a, fn_a, tp_a = confusion_matrix(y_test_c, preds_aggressive).ravel()\n","cost_aggressive = (fp_a * 10) + (fn_a * 100)\n","\n","print(f\"Cost at Threshold 0.5: ${cost_default}\")\n","print(f\"Cost at Threshold 0.2: ${cost_aggressive}\")\n"],"metadata":{"id":"s4VxQXIp0I72"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üé§ Interview Question 3\n","**The Hiring Manager asks:** \"Based on the costs calculated above, which model deployment strategy do you recommend? Should we spam more people (Low Threshold) or be conservative (High Threshold)? Why?\"\n","\n","**Your Answer:**\n","* [Recommendation]\n","* [Justification based on ROI]\n","\n"],"metadata":{"id":"bbM9iteJ0PU1"}},{"cell_type":"markdown","source":["## üå≥ Task 4: Random Forest & Feature Importance\n","\n","**Context:** The engineering team wants to know *which* features actually drive churn so they can fix the product. Logistic Regression coefficients can be hard to interpret if data isn't scaled. Let's use a Random Forest.\n","\n"],"metadata":{"id":"HS8-RjLb0WDd"}},{"cell_type":"code","source":["# üíª TODO: Fit a Random Forest & Plot Feature Importance\n","# 1. Fit a RandomForestClassifier to the churn data.\n","# 2. Extract `feature_importances_`\n","# 3. Create a bar plot showing which feature is most important.\n","\n","# [YOUR CODE HERE]\n"],"metadata":{"id":"OZ9T2RXx0cVF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üé§ Interview Question 4\n","**The Hiring Manager asks:** \"Your Random Forest performs slightly better than the Logistic Regression, but it's a 'Black Box.' If the CEO asks *how* 'Support_Tickets' impacts churn (e.g., does more tickets = more churn?), how would you derive that directionality from a Random Forest?\"\n","\n","**Your Answer:**\n","* [HINT: Feature Importance tells you *magnitude* but not *direction*. How do we find direction? Partial Dependence Plots? SHAP values? Correlation matrix check?]"],"metadata":{"id":"Z4A5guwGw7b-"}},{"cell_type":"markdown","source":["# üìù Part 2: Scenario\n","\n","**OmniCorp Marketing Dept.** has a problem: They are treating every customer the same. They send the same \"10% Off\" email to everyone, from the college student buying socks to the enterprise client buying servers.\n","\n","**Your Instructions from the Head of Marketing:**\n","> \"We have a dataset of user behavior on our platform. We don't know who these people are (no labels), but we suspect there are at least 3 distinct groups.\n",">\n","> 1.  Find the groups (Clusters).\n","> 2.  Tell us what defines them (e.g., 'Big Spenders').\n","> 3.  We also suspect there are some 'Bots' messing up our analytics. Can you identify them?\"\n","\n","## üõ†Ô∏è Instructions\n","1.  Run the \"Data Generation\" cells.\n","2.  Complete the **TODO** sections.\n","3.  Answer the **Interview Questions**.\n"],"metadata":{"id":"bm5InVHQ5TG6"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans, DBSCAN\n","from sklearn.metrics import silhouette_score\n","\n","# Setup\n","np.random.seed(42)\n","sns.set_style(\"whitegrid\")\n","%matplotlib inline\n"],"metadata":{"id":"CymBlizh5xfD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üìä Data Generation (Do Not Modify)\n","\n","We are generating data with **5 Features**:\n","1.  `Avg_Session_Time`: How long they stay on the site (minutes).\n","2.  `Items_Viewed`: Number of distinct products looked at.\n","3.  `Cart_Value`: Total value of items in cart ($).\n","4.  `Return_Rate`: Percentage of items returned (0.0 to 1.0).\n","5.  `Support_Tickets`: Number of complaints filed.\n"],"metadata":{"id":"XROrzI2k5-xN"}},{"cell_type":"code","source":["from sklearn.datasets import make_blobs\n","\n","# We create 3 \"Natural\" Customer Segments + 1 \"Bot\" Cluster (Dense & Weird)\n","# Segment 1: Window Shoppers (High views, low cart, low returns)\n","# Segment 2: Big Spenders (High cart, high session, low tickets)\n","# Segment 3: Complainers (Low cart, high tickets, high returns)\n","# Segment 4: Bots (Super fast, high views, 0 cart, 0 tickets)\n","\n","data_1 = np.random.normal(loc=[15, 20, 50, 0.1, 1], scale=[5, 5, 20, 0.05, 1], size=(300, 5)) # Window Shoppers\n","data_2 = np.random.normal(loc=[45, 10, 500, 0.05, 0], scale=[10, 2, 50, 0.02, 0.5], size=(200, 5)) # Whales\n","data_3 = np.random.normal(loc=[10, 5, 30, 0.8, 5], scale=[5, 2, 10, 0.1, 2], size=(150, 5)) # Problems\n","data_bots = np.random.normal(loc=[0.5, 100, 0, 0, 0], scale=[0.1, 10, 0, 0, 0], size=(50, 5)) # Bots\n","\n","# Combine\n","data = np.vstack([data_1, data_2, data_3, data_bots])\n","df_users = pd.DataFrame(data, columns=['Avg_Session_Time', 'Items_Viewed', 'Cart_Value', 'Return_Rate', 'Support_Tickets'])\n","\n","# Shuffle\n","df_users = df_users.sample(frac=1).reset_index(drop=True)\n","\n","print(f\"Dataset Shape: {df_users.shape}\")\n","df_users.head()\n"],"metadata":{"id":"mDmS7OeG6RZz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üßπ Task 1: Preprocessing & PCA\n","\n","**Context:** The features are on totally different scales. `Cart_Value` is in the hundreds, `Return_Rate` is 0-1. If you run K-Means now, `Cart_Value` will dominate the distance metric.\n","\n","**Interview Logic:** You can't visualize 5 dimensions. We need to squash it down to 2 dimensions to \"see\" the clusters.\n"],"metadata":{"id":"mArPJS5C6d8-"}},{"cell_type":"code","source":["# üíª TODO: Scaling & PCA\n","# 1. Initialize a StandardScaler and transform df_users. Save as X_scaled.\n","# 2. Initialize PCA with n_components=2. Fit and transform X_scaled. Save as X_pca.\n","# 3. Create a Scatter Plot of PC1 vs PC2.\n","\n","# [YOUR CODE HERE]\n"],"metadata":{"id":"Rfsb6IsK6lOz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üé§ Interview Question 1\n","**The Hiring Manager asks:** \"I see you used PCA to visualize the data. Look at the variance explained ratio (print it if you haven't). Did we lose a lot of information by compressing 5 dimensions into 2? If we lost 40% of the variance, is this plot still trustworthy?\"\n","\n","**Your Answer:**\n","* [Discuss Explained Variance Ratio]\n","* [Is the tradeoff worth it for visualization?]\n","\n"],"metadata":{"id":"qOusl7Wu6pxX"}},{"cell_type":"markdown","source":["## üéØ Task 2: K-Means Clustering\n","\n","**Context:** We need to group these users. We don't know $K$.\n","\n"],"metadata":{"id":"tCBUcTjS61Wb"}},{"cell_type":"code","source":["# üíª TODO: Find Optimal K\n","# 1. Run K-Means for k=2 to k=10.\n","# 2. Calculate the Inertia (WCSS) for each k.\n","# 3. Plot the Elbow Curve.\n","# 4. (Optional but Recommended) Calculate Silhouette Score for k=3, 4, 5.\n","\n","# [YOUR CODE HERE]\n","\n","# üíª TODO: Apply Final K-Means\n","# 1. Choose your \"Best K\" (Hint: Look for the elbow or highest silhouette).\n","# 2. Fit K-Means to X_scaled.\n","# 3. Add the 'Cluster_Labels' to your original DataFrame (df_users).\n","\n","# [YOUR CODE HERE]\n"],"metadata":{"id":"-9x7sUv8BlPh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üß† Task 3: Cluster Interpretation (The \"Business\" Part)\n","\n","**Context:** Getting \"Cluster 0, 1, 2\" means nothing to the Marketing Director. You need to name them.\n"],"metadata":{"id":"9S-Hj3wzBtUc"}},{"cell_type":"code","source":["# üíª TODO: Profiling\n","# Group by 'Cluster_Labels' and calculate the MEAN of every feature.\n","# Look at the differences.\n","\n","# print(df_users.groupby('Cluster_Labels').mean())\n"],"metadata":{"id":"7b_8sqL-BzZC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üé§ Interview Question 2\n","**The Hiring Manager asks:** \"Okay, you found 4 clusters. Based on the means you calculated above, give each cluster a **Marketing Persona Name** (e.g., 'Loyalists', 'Window Shoppers') and explain why.\"\n","\n","**Your Answer:**\n","* **Cluster 0:** Name = [Name] | Reason: [High cart value? Low time?]\n","* **Cluster 1:** Name = [Name] | Reason: ...\n","* **Cluster 2:** Name = [Name] | Reason: ...\n","* **Cluster 3:** Name = [Name] | Reason: ..."],"metadata":{"id":"FREaBklCB4SH"}},{"cell_type":"markdown","source":["## üïµÔ∏è Task 4: Anomaly Detection (DBSCAN)\n","\n","**Context:** Your K-Means model likely forced the \"Bots\" into a cluster, or maybe merged them with another group.\n","Bots are weird. They have extreme values (e.g., 100 items viewed in 0.5 minutes). They are \"density\" outliers compared to normal human behavior.\n","\n","Let's see if DBSCAN can isolate them better as \"Noise\" or a distinct, tight cluster."],"metadata":{"id":"V4MsqF45CAKI"}},{"cell_type":"code","source":["# üíª TODO: DBSCAN\n","# 1. Fit DBSCAN to X_scaled.\n","#    Note: You will need to tune 'eps' and 'min_samples'.\n","#    Try eps=0.5, min_samples=5 as a starting point.\n","# 2. Plot PC1 vs PC2 again, but color by DBSCAN labels.\n","#    (Remember: DBSCAN labels noise as -1).\n","\n","# [YOUR CODE HERE]\n"],"metadata":{"id":"RcWDk5mxCGJA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üé§ Interview Question 3\n","**The Hiring Manager asks:** \"In K-Means, every point *must* belong to a cluster. In DBSCAN, some points are labeled `-1`.\n","1.  What does `-1` represent in this specific dataset?\n","2.  If you were building a real-time fraud detection system, would you use K-Means or DBSCAN? Why?\"\n","\n","**Your Answer:**\n","* [Explain Noise/Outliers]\n","* [Compare K-Means (Centroid based) vs DBSCAN (Density based) for fraud]\n"],"metadata":{"id":"I406x3FK4e8g"}},{"cell_type":"markdown","source":["# Part 3: Scenario\n","\n","**\"GreenGrid Solutions\"** has hired you to optimize a large-scale battery storage system.\n","* **The Goal:** Buy electricity when it's cheap, store it, and sell it back to the grid when it's expensive.\n","* **The Constraint:** The battery has limited capacity, and charging/discharging degrades it slightly (cost).\n","* **The Problem:** You don't have a labeled dataset of \"Correct Actions.\" You only have the market environment.\n","\n","**Your Instructions from the CTO:**\n","> \"We need an RL Agent that learns to trade energy.\n","> 1.  Build a custom OpenAI Gym-style environment.\n","> 2.  Train a Q-Learning agent to maximize profit.\n","> 3.  Explain how you would deploy this if customer data privacy was a concern (Federated Learning).\"\n","\n","## üõ†Ô∏è Instructions\n","1.  Run the Environment Setup.\n","2.  Implement the **Q-Learning Algorithm**.\n","3.  Answer the **System Design Questions**."],"metadata":{"id":"3kd5ldflHdGi"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Setup\n","np.random.seed(42)\n","sns.set_style(\"whitegrid\")\n","%matplotlib inline\n"],"metadata":{"id":"ZYRulpdrHhPS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üîã Task 1: The Environment (GridWorld for Batteries)\n","\n","We will define a simple class `BatteryEnv`.\n","* **State:** Current Battery Level (0% to 100%, discretized into 10 steps).\n","* **Action:** 0=Hold, 1=Charge (Buy), 2=Discharge (Sell).\n","* **Reward:** (Price_Sold - Price_Bought) - Cost_Degradation.\n"],"metadata":{"id":"MOVDVh04HrV3"}},{"cell_type":"code","source":["# --- SIMULATED MARKET DATA (DO NOT MODIFY) ---\n","# 24-hour cycle of electricity prices (Low at night, High at peak)\n","prices = np.array([10, 10, 10, 12, 15, 20, 30, 45, 60, 50, 40, 35,\n","                   30, 25, 20, 18, 25, 40, 80, 100, 90, 60, 30, 15])\n","\n","# Plot Prices\n","plt.figure(figsize=(10, 4))\n","plt.plot(prices, marker='o', linestyle='--')\n","plt.title(\"24-Hour Electricity Price Cycle\")\n","plt.xlabel(\"Hour\")\n","plt.ylabel(\"Price ($/MWh)\")\n","plt.show()\n","\n","# --- ENVIRONMENT CLASS ---\n","class BatteryEnv:\n","    def __init__(self, prices):\n","        self.prices = prices\n","        self.max_capacity = 10\n","        self.current_charge = 0\n","        self.time_step = 0\n","        self.end_step = len(prices) - 1\n","\n","    def reset(self):\n","        self.current_charge = 0\n","        self.time_step = 0\n","        return (self.current_charge, self.time_step)\n","\n","    def step(self, action):\n","        # Current Price\n","        current_price = self.prices[self.time_step]\n","        reward = 0\n","\n","        # Execute Action\n","        if action == 1: # CHARGE (Buy)\n","            if self.current_charge < self.max_capacity:\n","                self.current_charge += 1\n","                reward = -current_price # Cost money to buy\n","            else:\n","                reward = -50 # Penalty for trying to overcharge\n","\n","        elif action == 2: # DISCHARGE (Sell)\n","            if self.current_charge > 0:\n","                self.current_charge -= 1\n","                reward = current_price # Gain money\n","            else:\n","                reward = -50 # Penalty for trying to sell empty\n","\n","        # Action 0 (Hold) has reward 0\n","\n","        # Move Time Forward\n","        self.time_step += 1\n","        done = (self.time_step >= self.end_step)\n","        next_state = (self.current_charge, self.time_step)\n","\n","        return next_state, reward, done"],"metadata":{"id":"JnvzBi-kHow3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üß† Task 2: Build the Agent (Q-Learning)\n","\n","**Context:** You need to fill in the Q-Learning update loop.\n","Recall the Bellman Equation:\n","$$Q(s,a) = Q(s,a) + \\alpha [R + \\gamma \\max Q(s', a') - Q(s,a)]$$\n"],"metadata":{"id":"HuoOMF-tH8Bn"}},{"cell_type":"code","source":["# üíª TODO: Implement Q-Learning Training Loop\n","\n","# Hyperparameters\n","alpha = 0.1   # Learning Rate\n","gamma = 0.95  # Discount Factor (Care about future profit)\n","epsilon = 0.1 # Exploration Rate\n","episodes = 1000\n","\n","# Initialize Q-Table: [Charge_Level(11), Time_Step(24), Actions(3)]\n","q_table = np.zeros((11, 24, 3))\n","\n","env = BatteryEnv(prices)\n","\n","rewards_history = []\n","\n","for ep in range(episodes):\n","    state = env.reset() # Returns (charge, time)\n","    done = False\n","    total_reward = 0\n","\n","    while not done:\n","        charge, time = state\n","\n","        # 1. Epsilon-Greedy Action Selection\n","        if np.random.uniform(0, 1) < epsilon:\n","            action = np.random.choice([0, 1, 2]) # Explore\n","        else:\n","            action = np.argmax(q_table[charge, time]) # Exploit\n","\n","        # 2. Take Action\n","        next_state, reward, done = env.step(action)\n","        next_charge, next_time = next_state\n","\n","        # 3. Update Q-Table (THE TODO PART)\n","        # [YOUR CODE HERE]\n","        # Implement the line: old_value + alpha * (reward + gamma * max_future - old_value)\n","\n","        # q_table[charge, time, action] = ...\n","\n","        state = next_state\n","        total_reward += reward\n","\n","    rewards_history.append(total_reward)\n","\n","# Plot Learning Curve\n","plt.plot(rewards_history)\n","plt.title(\"Agent Profit over Episodes\")\n","plt.xlabel(\"Episode\")\n","plt.ylabel(\"Total Profit\")\n","plt.show()\n"],"metadata":{"id":"2rHrWpDuII3P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üé§ Interview Question 1\n","**The CTO asks:** \"Look at your training curve. It likely goes up and then flattens out.\n","1.  Why does it fluctuate so much even after 'learning'? (Hint: Look at `epsilon`).\n","2.  If we deployed this to a real grid where prices spike unpredictably (unlike this fixed 24h cycle), how would you modify the state definition to handle that?\"\n","\n","**Your Answer:**\n","* [Discuss Exploration vs Exploitation noise]\n","* [Discuss State Space augmentation (e.g., adding 'Current_Price' or 'Forecast_Price' to the state tuple)]\n"],"metadata":{"id":"dIcPdMvyIRcn"}},{"cell_type":"markdown","source":["## ü§ù Task 3: Transfer & Federated Learning (Design)\n","\n","**Context:**\n","GreenGrid wants to install these batteries in 10,000 residential homes.\n","1.  **Issue A:** Training a model from scratch for *every single house* takes too long (2 weeks per house).\n","2.  **Issue B:** Homeowners are worried that GreenGrid will spy on their energy usage patterns (privacy).\n"],"metadata":{"id":"lSFQWbz6IZHE"}},{"cell_type":"markdown","source":["### üé§ Interview Question 2 (Transfer Learning)\n","**The CTO asks:** \"We have a perfectly trained model for House A (Solar Panels, Texas weather). House B (Wind Turbine, Chicago weather) comes online tomorrow.\n","Instead of random initialization, how can we use **Transfer Learning** to speed up House B's training? Which parts of the Q-Network (if we used Deep RL) would you freeze vs. retrain?\"\n","\n","**Your Answer:**\n","* [Concept: Warm Start]\n","* [Strategy: Transfer general grid dynamics, fine-tune for local weather patterns]\n","\n"],"metadata":{"id":"JEe1hkEKIibQ"}},{"cell_type":"markdown","source":["### üé§ Interview Question 3 (Federated Learning)\n","**The CTO asks:** \"Explain to the Legal Team how **Federated Learning** solves the privacy issue. Specifically, describe what information is sent to the cloud and what stays on the device.\"\n","\n","**Your Answer:**\n","* [Data stays local]\n","* [Only Weight Updates (Gradients) are shared]\n","* [Aggregation (FedAvg) happens in the cloud]\n","\n","**Remember:** In the interview, your code must run, but your **explanations** get you the job."],"metadata":{"id":"vp6xMJTkHI-x"}}]}