{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC5082/blob/main/Interview_Prep_2/interview_prep_concepts_review_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zykWDTbAtpfM"
      },
      "source": [
        "# Data Science Fundamentals - Study Guide\n",
        "\n",
        "**Purpose**: This notebook provides a comprehensive review of essential data science concepts, including descriptive statistics, data types, data preparation techniques, Exploratory Data Analysis, and data pipelines.\n",
        "\n",
        "**Learning Objectives**:\n",
        "- Understand fundamental statistical concepts and terminology\n",
        "- Work with different data types and structures\n",
        "- Apply descriptive statistics to real datasets\n",
        "- Identify and handle data quality issues\n",
        "- Conduct thorough exploratory data analysis (univariate, bivariate, multivariate)\n",
        "- Generate and test hypotheses from data exploration\n",
        "- Build automated preprocessing pipelines using sklearn\n",
        "- Create custom transformers for domain-specific operations\n",
        "- Implement pipeline persistence and versioning\n",
        "- Integrate pipelines with model training workflows"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Basics"
      ],
      "metadata": {
        "id": "ddF7537_v4el"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Z4q2iRtpfQ"
      },
      "source": [
        "## Part 1: Key Terminology\n",
        "\n",
        "### Statistical Terms\n",
        "- **Parameter**: A characteristic of a population (e.g., population mean μ)\n",
        "- **Statistic**: A characteristic of a sample (e.g., sample mean x̄)\n",
        "- **Variable**: Any characteristic or attribute that can be measured or counted\n",
        "- **Inference**: Drawing conclusions about a population based on sample data\n",
        "\n",
        "### Measures of Central Tendency\n",
        "- **Mean**: The average value (sum of all values / number of values)\n",
        "- **Median**: The middle value when data is sorted\n",
        "- **Mode**: The most frequently occurring value\n",
        "\n",
        "### Measures of Spread\n",
        "- **Variance**: Average squared deviation from the mean\n",
        "- **Standard Deviation**: Square root of variance; measures spread in original units\n",
        "- **Range**: Difference between max and min values\n",
        "- **IQR (Interquartile Range)**: Q3 - Q1; contains middle 50% of data\n",
        "\n",
        "### Measures of Shape\n",
        "- **Skewness**: Measure of asymmetry in a distribution\n",
        "  - Positive skew: Tail on the right (Mean > Median > Mode)\n",
        "  - Negative skew: Tail on the left (Mean < Median < Mode)\n",
        "- **Kurtosis**: Measure of \"tailedness\" of a distribution\n",
        "  - Mesokurtic: Normal distribution\n",
        "  - Leptokurtic: Heavy tails, more extreme values\n",
        "  - Platykurtic: Light tails, fewer extreme values\n",
        "\n",
        "### Distribution Terms\n",
        "- **Quartiles**: Values dividing data into four equal parts (Q1, Q2/median, Q3)\n",
        "- **Quantiles**: Generalization of quartiles (any number of equal parts)\n",
        "- **Outliers**: Data points significantly different from other observations\n",
        "- **Box Plot**: Graphical representation showing median, quartiles, and outliers\n",
        "- **Fence**: Cutoff value for identifying outliers (1.5 × IQR from quartiles)\n",
        "\n",
        "### Data Types\n",
        "- **Numerical**:\n",
        "  - Discrete: Countable values (number of children)\n",
        "  - Continuous: Measurable values (height, temperature)\n",
        "- **Categorical**:\n",
        "  - Nominal: No inherent order (colors, names)\n",
        "  - Ordinal: Meaningful order (rankings, ratings)\n",
        "- **Interval**: Ordered with equal distances (temperature in Celsius)\n",
        "- **Ratio**: Has true zero point (height, weight)\n",
        "\n",
        "### Data Quality Terms\n",
        "- **Cardinality**: Number of unique values in a dataset or column\n",
        "- **MCAR (Missing Completely at Random)**: Missingness unrelated to any variables\n",
        "- **MAR (Missing at Random)**: Missingness depends on observed variables\n",
        "- **MNAR (Missing Not at Random)**: Missingness depends on missing values themselves\n",
        "- **Reliability**: Consistency/reproducibility of measurements\n",
        "- **Validity**: Accuracy of measurements in representing true values\n",
        "- **Precision**: Level of detail/exactness in measurement\n",
        "- **Accuracy**: How close measurements are to true values\n",
        "\n",
        "### Constants\n",
        "\n",
        "Features with constant values should be deleted because they provide **zero information or predictive power** to a machine learning model.\n",
        "\n",
        "* **The Problem:** Machine learning models, especially those based on statistical principles (like regression) or information theory (like decision trees), rely on **variance** or **differences** in the data to learn relationships.\n",
        "* **The Effect:** A feature where *every* row has the exact same value (e.g., a column called `Country` where every value is 'USA') cannot help distinguish one data point (row) from another. It offers no insight into why the target variable (what you are trying to predict) changes.\n",
        "    * **Analogy:** Imagine trying to predict a student's grade based on whether they attended a school. If *every* student in your dataset attended a school, that \"attended school\" feature is useless for predicting grades.\n",
        "* **Mathematical Issue:** Many machine learning algorithms involve calculations based on the standard deviation or variance of features.\n",
        "    * For example, in standardizing data (a common preprocessing step), you divide by the standard deviation: $z = \\frac{x - \\mu}{\\sigma}$.\n",
        "    * If a feature is constant, its standard deviation ($\\sigma$) is **zero**. **Dividing by zero** is mathematically undefined and will cause algorithms to fail, raise errors, or produce unstable results.\n",
        "\n",
        "* **The Benefit:** While a single constant column doesn't add much overhead, in datasets with hundreds or thousands of features, removing all zero-variance columns is a form of **dimensionality reduction**.\n",
        "* **The Result:** Removing these irrelevant features speeds up model training and prediction times, and reduces memory usage, all without any loss in predictive performance.\n",
        "\n",
        "### Quasi-Constant\n",
        "\n",
        "A **Quasi-Constant** feature is a feature (column) where **a single value is present for a vast majority of the observations** (rows), but not *all* of them.\n",
        "\n",
        "* **Constant Feature:** 100% of values are the same. (e.g., all 1s)\n",
        "* **Quasi-Constant Feature:** 99.5% of values are the same, and the remaining 0.5% are different. (e.g., 995 rows are 'A', 5 rows are 'B')\n",
        "\n",
        "We delete quasi constant values because **they provide extremely little predictive value** but increase model complexity and computational cost.\n",
        "\n",
        "1.  **Low Information:** The small variations have minimal, if any, predictive power because they affect only a tiny fraction of the dataset.\n",
        "2.  **Overfitting Risk:** Including features that are mostly constant might occasionally cause a complex model to *overfit* to the few rare non-constant values, learning noise instead of the general pattern.\n",
        "3.  **Efficiency:** Removing them is a straightforward way to reduce the dimensionality of your dataset without meaningfully impacting the model's performance.\n",
        "\n",
        "Because \"quasi-constant\" is a subjective term, you must define a **threshold** to decide when a feature has too little variance to be useful. This threshold is based on the **percentage of the most frequent value (mode)**.\n",
        "\n",
        "There is **no single, universally mandatory percentage**. The threshold is a **hyperparameter** that you, the data scientist, must tune based on your project, dataset size, and modeling goals.\n",
        "\n",
        "Commonly cited thresholds in the data science community are usually very high, indicating a strong consensus that the value should be *almost* constant:\n",
        "\n",
        "| Common Threshold Range | Meaning |\n",
        "| :--- | :--- |\n",
        "| **95%** | If one value makes up 95% or more of the data, drop the feature. |\n",
        "| **98%** | If one value makes up 98% or more of the data, drop the feature. |\n",
        "| **99% - 99.9%** | **Most frequently recommended starting point,** especially for large datasets. |\n",
        "\n",
        "### Imputation\n",
        "\n",
        "In the world of data cleaning, **imputation** is the process of replacing missing data with substituted values. Think of it as \"filling in the blanks\" so that your dataset remains complete and usable for analysis or machine learning models, most of which cannot handle empty cells (null values).\n",
        "\n",
        "Instead of simply deleting rows with missing info—which can lead to losing valuable insights—imputation uses logic, statistics, or machine learning to guess the most likely value for those gaps.\n",
        "\n",
        "**Common Imputation Techniques**\n",
        "\n",
        "How you fill the gap depends entirely on the nature of your data. Here is a breakdown of the standard \"repair\" strategies:\n",
        "\n",
        "| Method | How it Works | Best Used For... |\n",
        "| --- | --- | --- |\n",
        "| **Mean Imputation** | Fills gaps with the average of the column. | Numerical data with no major outliers. |\n",
        "| **Median Imputation** | Fills gaps with the middle value. | Numerical data with outliers (more robust). |\n",
        "| **Mode Imputation** | Fills gaps with the most frequent value. | Categorical data (e.g., \"City\" or \"Color\"). |\n",
        "| **K-Nearest Neighbors (KNN)** | Finds similar rows and uses their values to predict the missing one. | Complex datasets where relationships exist between variables. |\n",
        "| **Last Observation Carried Forward (LOCF)** | Uses the previous known value to fill the next gap. | Time-series data (e.g., stock prices or daily weather). |\n",
        "\n",
        "\n",
        "**Why Impute? (The Pros and Cons)**\n",
        "\n",
        "While it sounds like \"making up data,\" imputation is a calculated necessity.\n",
        "\n",
        "* **The Good:** It preserves the \"sample size.\" If you have 1,000 rows but 200 have one missing column, deleting them loses 20% of your data. Imputation keeps those rows in play.\n",
        "* **The Bad:** If done poorly, it can introduce **bias**. For example, if you use Mean Imputation on a column with heavy outliers, you might artificially shrink the variance of your data, making your model overconfident.\n",
        "\n",
        "### Outliers\n",
        "\n",
        "In data cleaning, **outliers** are data points that differ significantly from other observations in your dataset. They are the \"loners\" or \"rebels\" that don't follow the general trend.\n",
        "\n",
        "Detecting and handling them is crucial because outliers can skew statistical measures (like the mean) and lead to misleading conclusions or poor machine learning performance.\n",
        "\n",
        "**How to Identify Outliers**\n",
        "\n",
        "Before you can clean them, you have to find them. Common mathematical approaches include:\n",
        "\n",
        "* **The Z-Score:** Measures how many standard deviations a point is from the mean. Typically, a  or  is considered an outlier.\n",
        "* **Interquartile Range (IQR):** This focuses on the \"middle 50%\" of your data. Anything that falls significantly above the 75th percentile or below the 25th percentile is flagged.\n",
        "* *Lower Bound* =\n",
        "* *Upper Bound* =\n",
        "\n",
        "**Handling Strategies**\n",
        "\n",
        "Once identified, you have four main ways to deal with them:\n",
        "\n",
        "1. **Trimming (Deletion):** Removing the outlier rows entirely. This is best if the outlier is a clear data entry error (e.g., a student’s age listed as 200).\n",
        "2. **Capping (Winsorization):** Instead of deleting the data, you \"cap\" it at a certain limit. For example, any age over 100 is simply recorded as 100.\n",
        "3. **Transformation:** Applying a mathematical function (like a  transformation) to the entire column to \"pull\" the outliers closer to the center.\n",
        "4. **Imputation:** Treating the outlier as a missing value and replacing it using the techniques we discussed earlier (like the Median).\n",
        "\n",
        "### Discretization\n",
        "\n",
        "In the context of data cleaning and preprocessing, **discretization** (also known as **binning**) is the process of converting continuous, numerical data into discrete \"bins\" or intervals.\n",
        "\n",
        "Essentially, you are taking a infinite range of numbers and grouping them into a finite number of categories. This is often done to simplify data, reduce the impact of small observation errors, or satisfy the requirements of certain algorithms (like Decision Trees or Naive Bayes) that prefer categorical input.\n",
        "\n",
        "**How It Works: Common Strategies**\n",
        "\n",
        "There are two primary ways to decide where the boundaries of your \"bins\" should be:\n",
        "\n",
        "| Method | Description | Example |\n",
        "| --- | --- | --- |\n",
        "| **Equal Width Binning** | Divides the range into  intervals of equal size. | Ages 0-20, 21-40, 41-60. |\n",
        "| **Equal Frequency Binning** | Divides the data so that each bin has roughly the same number of rows. | 10 students in bin A, 10 in bin B, 10 in bin C. |\n",
        "| **Custom/Domain Binning** | Creating bins based on specific business or logic rules. | \"Minor\" (<18) vs. \"Adult\" (>=18). |\n",
        "\n",
        "\n",
        "**Why Use Discretization?**\n",
        "\n",
        "* **Handling Noise:** Small fluctuations in data (like a sensor reading 20.1°C vs 20.2°C) often don't matter. Binning them into \"Room Temp\" ignores that minor noise.\n",
        "* **Improving Interpretability:** It is much easier for a human (or a stakeholder) to understand \"High Income\" vs. \"Low Income\" than a list of 5,000 unique salary figures.\n",
        "* **Managing Outliers:** Since outliers are pushed into the \"highest\" or \"lowest\" bin, they no longer have a disproportionate pull on the statistical mean of your model.\n",
        "\n",
        "### Scaling\n",
        "\n",
        "In data cleaning and preprocessing, **Scaling** is the process of transforming your numerical data so that all features occupy a similar range.\n",
        "\n",
        "If you have one column for \"Age\" (0–100) and another for \"Annual Salary\" (0–200,000), a machine learning model might mistakenly think Salary is 2,000 times more important just because the numbers are bigger. Scaling levels the playing field.\n",
        "\n",
        "### The Big Three Scaling Methods\n",
        "\n",
        "**1. Min-Max Scaling (Normalization)**\n",
        "\n",
        "This technique squishes all your data into a fixed range, usually **0 to 1**. It is the go-to for image processing or algorithms that don't assume a specific distribution (like KNN or Neural Networks).\n",
        "\n",
        "* **Formula:**\n",
        "* **Pros:** Preserves the relative distances between values.\n",
        "* **Cons:** Very sensitive to **outliers**. If one person has a salary of $10 million, everyone else will be squashed to 0.0001.\n",
        "\n",
        "**2. Standardization (Z-Score Scaling)**\n",
        "\n",
        "Standardization centers the data around a mean of **0** with a standard deviation of **1**. This is the standard for algorithms like Principal Component Analysis (PCA) or Linear Regression.\n",
        "\n",
        "* **Formula:**  (where  is the mean and  is the standard dev)\n",
        "* **Pros:** Much more robust to outliers than Min-Max.\n",
        "* **Cons:** The resulting data doesn't have a fixed \"bounding box\" (values can go to +4 or -5).\n",
        "\n",
        "**3. Robust Scaling**\n",
        "\n",
        "If your data is \"dirty\" with many outliers that you can't delete, Robust Scaling is your best friend. It uses the **Median** and the **Interquartile Range (IQR)** instead of the Mean and Standard Deviation.\n",
        "\n",
        "* **Formula:**\n",
        "* **Pros:** Outliers don't influence the scaling of the \"normal\" data points.\n",
        "\n",
        "\n",
        "**Quick Comparison Table**\n",
        "\n",
        "| Method | Range | Best For... | Handling Outliers |\n",
        "| --- | --- | --- | --- |\n",
        "| **Min-Max** | [0, 1] | Neural Nets, Images | Poor (Sensitive) |\n",
        "| **Standardization** | No fixed range | Regression, PCA | Decent |\n",
        "| **Robust Scaling** | No fixed range | Datasets with many \"extreme\" errors | Excellent |\n",
        "\n",
        "### Category Encoding\n",
        "\n",
        "In data cleaning and machine learning, Categorical Encoding is the process of converting text-based labels or categories into numbers. Since most mathematical models and algorithms (like Linear Regression or Neural Networks) cannot \"calculate\" using words like \"Red\" or \"London,\" we must translate them into a numerical format.\n",
        "\n",
        "**Label / Ordinal Encoding**\n",
        "\n",
        "This assigns a unique integer to each category (e.g., 1, 2, 3).\n",
        "\n",
        "Best for: Ordinal data, where the categories have a natural rank or order (e.g., \"Small,\" \"Medium,\" \"Large\").\n",
        "\n",
        "The Risk: If used on non-ordered data (like \"Colors\"), the model might think \"Yellow\" (3) is mathematically greater than \"Red\" (1), which can lead to poor predictions.\n",
        "\n",
        "**One-Hot Encoding**\n",
        "\n",
        "This creates a new \"dummy\" column for every unique category in the original column. Each column contains a 1 if the row belongs to that category and a 0 if it doesn't.\n",
        "\n",
        "Best for: Nominal data, where there is no inherent order (e.g., \"City,\" \"Gender,\" \"Department\").\n",
        "\n",
        "The Risk: The \"Dummy Variable Trap.\" If you have 100 unique cities, you’ll end up with 100 new columns, which can make your dataset massive and slow (this is called the \"Curse of Dimensionality\").\n",
        "\n",
        "**Frequency Encoding**\n",
        "\n",
        "Frequency Encoding is a technique where you replace each categorical value with the number of times it appears in the dataset. Alternatively, you can use the percentage (normalized frequency) of its occurrence. It is a powerful alternative to One-Hot Encoding when you have a column with a high number of unique categories (high cardinality)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQxsFkTktpfR"
      },
      "source": [
        "## Part 2: Setup and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhluXlWXtpfR"
      },
      "outputs": [],
      "source": [
        "# Import essential libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.precision', 2)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzggXnbgtpfS"
      },
      "outputs": [],
      "source": [
        "# Load a sample dataset from seaborn\n",
        "df = sns.load_dataset('tips')\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69fErWh8tpfT"
      },
      "source": [
        "## Part 3: Descriptive Statistics in Practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QEHcdfotpfT"
      },
      "outputs": [],
      "source": [
        "# Basic dataset information\n",
        "print(\"Dataset Information:\")\n",
        "print(\"=\"*50)\n",
        "df.info()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Statistical Summary:\")\n",
        "print(\"=\"*50)\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIhFNpeBtpfT"
      },
      "source": [
        "### 3.1 Measures of Central Tendency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqAG9pGitpfU"
      },
      "outputs": [],
      "source": [
        "# Calculate mean, median, and mode for total_bill\n",
        "column = 'total_bill'\n",
        "\n",
        "mean_val = df[column].mean()\n",
        "median_val = df[column].median()\n",
        "mode_val = df[column].mode()[0] if len(df[column].mode()) > 0 else np.nan\n",
        "\n",
        "print(f\"Analysis of '{column}':\")\n",
        "print(f\"Mean:   ${mean_val:.2f}\")\n",
        "print(f\"Median: ${median_val:.2f}\")\n",
        "print(f\"Mode:   ${mode_val:.2f}\")\n",
        "\n",
        "# Visualize with histogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df[column], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "plt.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: ${mean_val:.2f}')\n",
        "plt.axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: ${median_val:.2f}')\n",
        "plt.axvline(mode_val, color='blue', linestyle='--', linewidth=2, label=f'Mode: ${mode_val:.2f}')\n",
        "plt.xlabel('Total Bill ($)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Total Bill with Central Tendency Measures')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n703sts5tpfU"
      },
      "source": [
        "### 3.2 Measures of Spread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY4sbazetpfU"
      },
      "outputs": [],
      "source": [
        "# Calculate measures of spread\n",
        "variance = df[column].var()\n",
        "std_dev = df[column].std()\n",
        "data_range = df[column].max() - df[column].min()\n",
        "iqr = df[column].quantile(0.75) - df[column].quantile(0.25)\n",
        "\n",
        "print(f\"Measures of Spread for '{column}':\")\n",
        "print(f\"Variance:           ${variance:.2f}²\")\n",
        "print(f\"Standard Deviation: ${std_dev:.2f}\")\n",
        "print(f\"Range:              ${data_range:.2f}\")\n",
        "print(f\"IQR:                ${iqr:.2f}\")\n",
        "\n",
        "# Calculate quartiles\n",
        "Q1 = df[column].quantile(0.25)\n",
        "Q2 = df[column].quantile(0.50)  # median\n",
        "Q3 = df[column].quantile(0.75)\n",
        "\n",
        "print(f\"\\nQuartiles:\")\n",
        "print(f\"Q1 (25th percentile): ${Q1:.2f}\")\n",
        "print(f\"Q2 (50th percentile): ${Q2:.2f}\")\n",
        "print(f\"Q3 (75th percentile): ${Q3:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo_nqYDktpfV"
      },
      "source": [
        "### 3.3 Measures of Shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzL-TlOLtpfV"
      },
      "outputs": [],
      "source": [
        "# Calculate skewness and kurtosis\n",
        "skewness = df[column].skew()\n",
        "kurtosis = df[column].kurtosis()\n",
        "\n",
        "print(f\"Measures of Shape for '{column}':\")\n",
        "print(f\"Skewness: {skewness:.3f}\")\n",
        "if skewness > 0.5:\n",
        "    print(\"  → Positively skewed (right-tailed)\")\n",
        "elif skewness < -0.5:\n",
        "    print(\"  → Negatively skewed (left-tailed)\")\n",
        "else:\n",
        "    print(\"  → Approximately symmetric\")\n",
        "\n",
        "print(f\"\\nKurtosis: {kurtosis:.3f}\")\n",
        "if kurtosis > 0:\n",
        "    print(\"  → Leptokurtic (heavy tails, more outliers)\")\n",
        "elif kurtosis < 0:\n",
        "    print(\"  → Platykurtic (light tails, fewer outliers)\")\n",
        "else:\n",
        "    print(\"  → Mesokurtic (normal-like tails)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNI53IM_tpfV"
      },
      "source": [
        "## Part 4: Visualizing Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3yl78RBtpfV"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Histogram with KDE\n",
        "axes[0, 0].hist(df[column], bins=30, alpha=0.6, color='skyblue', edgecolor='black', density=True)\n",
        "df[column].plot(kind='kde', ax=axes[0, 0], color='red', linewidth=2)\n",
        "axes[0, 0].set_xlabel('Total Bill ($)')\n",
        "axes[0, 0].set_ylabel('Density')\n",
        "axes[0, 0].set_title('Histogram with KDE')\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# Box Plot\n",
        "bp = axes[0, 1].boxplot(df[column], vert=True, patch_artist=True)\n",
        "bp['boxes'][0].set_facecolor('lightblue')\n",
        "axes[0, 1].set_ylabel('Total Bill ($)')\n",
        "axes[0, 1].set_title('Box Plot')\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "# Violin Plot\n",
        "parts = axes[1, 0].violinplot([df[column].dropna()], positions=[1], showmeans=True, showmedians=True)\n",
        "axes[1, 0].set_ylabel('Total Bill ($)')\n",
        "axes[1, 0].set_title('Violin Plot')\n",
        "axes[1, 0].set_xticks([1])\n",
        "axes[1, 0].set_xticklabels(['Total Bill'])\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# Q-Q Plot for normality\n",
        "stats.probplot(df[column], dist=\"norm\", plot=axes[1, 1])\n",
        "axes[1, 1].set_title('Q-Q Plot (Normality Check)')\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaQeVMVItpfV"
      },
      "source": [
        "## Part 5: Outlier Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U32sgi35tpfW"
      },
      "outputs": [],
      "source": [
        "# Calculate fences and identify outliers\n",
        "Q1 = df[column].quantile(0.25)\n",
        "Q3 = df[column].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define fences\n",
        "lower_fence = Q1 - 1.5 * IQR\n",
        "upper_fence = Q3 + 1.5 * IQR\n",
        "\n",
        "# Identify outliers\n",
        "outliers = df[(df[column] < lower_fence) | (df[column] > upper_fence)]\n",
        "\n",
        "print(\"Outlier Analysis:\")\n",
        "print(f\"Lower Fence: ${lower_fence:.2f}\")\n",
        "print(f\"Upper Fence: ${upper_fence:.2f}\")\n",
        "print(f\"\\nNumber of outliers: {len(outliers)}\")\n",
        "print(f\"Percentage of outliers: {len(outliers)/len(df)*100:.2f}%\")\n",
        "\n",
        "if len(outliers) > 0:\n",
        "    print(f\"\\nOutlier values:\")\n",
        "    print(outliers[column].sort_values(ascending=False).head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSMvYofdtpfW"
      },
      "source": [
        "## Part 6: Data Types and Structures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH1ZC78htpfW"
      },
      "outputs": [],
      "source": [
        "# Identify different data types in the dataset\n",
        "print(\"Data Types in Dataset:\")\n",
        "print(\"=\"*50)\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Numerical Columns:\")\n",
        "numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "print(numerical_cols)\n",
        "\n",
        "print(\"\\nCategorical Columns:\")\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "print(categorical_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI8FGXZwtpfW"
      },
      "outputs": [],
      "source": [
        "# Analyze categorical variables\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Cardinality: {df[col].nunique()}\")\n",
        "    print(f\"  Unique values: {df[col].unique()}\")\n",
        "    print(f\"  Value counts:\")\n",
        "    print(df[col].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWlh_QTitpfX"
      },
      "source": [
        "## Part 7: Handling Missing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl6XcX1FtpfX"
      },
      "outputs": [],
      "source": [
        "# Create a copy with some missing values for demonstration\n",
        "df_missing = df.copy()\n",
        "\n",
        "# Introduce missing values randomly\n",
        "np.random.seed(42)\n",
        "mask = np.random.random(df_missing.shape) < 0.1\n",
        "df_missing = df_missing.mask(mask)\n",
        "\n",
        "# Analyze missing data\n",
        "print(\"Missing Data Analysis:\")\n",
        "print(\"=\"*50)\n",
        "missing_counts = df_missing.isnull().sum()\n",
        "missing_pct = (missing_counts / len(df_missing)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_counts.index,\n",
        "    'Missing_Count': missing_counts.values,\n",
        "    'Missing_Percentage': missing_pct.values\n",
        "})\n",
        "\n",
        "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
        "print(missing_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgY-lis7tpfX"
      },
      "outputs": [],
      "source": [
        "# Demonstrate different imputation strategies\n",
        "df_imputed = df_missing.copy()\n",
        "\n",
        "# For numerical columns: use median (robust to outliers)\n",
        "for col in numerical_cols:\n",
        "    if df_imputed[col].isnull().any():\n",
        "        median_val = df_imputed[col].median()\n",
        "        df_imputed[col].fillna(median_val, inplace=True)\n",
        "        print(f\"Imputed {col} with median: {median_val:.2f}\")\n",
        "\n",
        "# For categorical columns: use mode\n",
        "for col in categorical_cols:\n",
        "    if df_imputed[col].isnull().any():\n",
        "        mode_val = df_imputed[col].mode()[0]\n",
        "        df_imputed[col].fillna(mode_val, inplace=True)\n",
        "        print(f\"Imputed {col} with mode: {mode_val}\")\n",
        "\n",
        "print(\"\\nMissing values after imputation:\")\n",
        "print(df_imputed.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTIldXebtpfX"
      },
      "source": [
        "## Part 8: Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qf_QlW_9tpfX"
      },
      "outputs": [],
      "source": [
        "# Calculate correlation matrix\n",
        "correlation_matrix = df[numerical_cols].corr()\n",
        "\n",
        "print(\"Correlation Matrix:\")\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Visualize correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Heatmap', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GBv9vdFtpfY"
      },
      "outputs": [],
      "source": [
        "# Analyze strongest correlations\n",
        "# Get upper triangle of correlation matrix\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "tri_df = correlation_matrix.mask(mask)\n",
        "\n",
        "# Find correlations above threshold\n",
        "threshold = 0.5\n",
        "strong_corr = tri_df.unstack().sort_values(ascending=False)\n",
        "strong_corr = strong_corr[abs(strong_corr) > threshold]\n",
        "\n",
        "print(f\"\\nStrong Correlations (|r| > {threshold}):\")\n",
        "print(\"=\"*50)\n",
        "for (var1, var2), corr_value in strong_corr.items():\n",
        "    print(f\"{var1} <-> {var2}: {corr_value:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0n2PGnjtpfY"
      },
      "source": [
        "## Summary: Key Takeaways\n",
        "\n",
        "### Statistical Concepts\n",
        "1. **Central Tendency**: Mean, median, and mode describe the \"center\" of data\n",
        "   - Use median when data is skewed or has outliers\n",
        "   - Use mean for symmetric distributions\n",
        "\n",
        "2. **Spread**: Variance, standard deviation, and IQR describe data variability\n",
        "   - Higher values indicate more spread/variability\n",
        "   - IQR is robust to outliers\n",
        "\n",
        "3. **Shape**: Skewness and kurtosis describe distribution characteristics\n",
        "   - Skewness indicates asymmetry\n",
        "   - Kurtosis indicates tail heaviness\n",
        "\n",
        "4. **Outliers**: Can be identified using box plots and IQR method\n",
        "   - Consider domain knowledge before removing\n",
        "   - May represent important extreme cases\n",
        "\n",
        "### Data Types\n",
        "- **Numerical**: Quantitative measurements (discrete or continuous)\n",
        "- **Categorical**: Qualitative categories (nominal or ordinal)\n",
        "- Different types require different analytical approaches\n",
        "\n",
        "### Data Quality\n",
        "- **Missing Data**: Understand mechanism (MCAR, MAR, MNAR)\n",
        "- **Imputation**: Choose strategy based on data type and distribution\n",
        "- **Validation**: Check for reliability, validity, accuracy\n",
        "\n",
        "### Best Practices\n",
        "1. Always explore data visually before analysis\n",
        "2. Check for missing values and outliers\n",
        "3. Understand data types and distributions\n",
        "4. Document assumptions and decisions\n",
        "5. Consider the context and domain knowledge"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "hwVmlgXbwTCu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS9Of3XEuDuH"
      },
      "source": [
        "## Comprehensive EDA Framework\n",
        "\n",
        "### What is EDA?\n",
        "\n",
        "**Exploratory Data Analysis (EDA)** is an approach to analyzing datasets to:\n",
        "- Summarize main characteristics\n",
        "- Uncover patterns and relationships\n",
        "- Detect anomalies and outliers\n",
        "- Test assumptions\n",
        "- Generate hypotheses for further investigation\n",
        "\n",
        "### EDA Process:\n",
        "1. **Univariate Analysis**: Examine each variable individually\n",
        "2. **Bivariate Analysis**: Explore relationships between pairs of variables\n",
        "3. **Multivariate Analysis**: Understand complex interactions between multiple variables\n",
        "4. **Hypothesis Generation**: Form testable questions based on observations\n",
        "5. **Hypothesis Testing**: Validate findings statistically\n",
        "\n",
        "### Key Difference from Descriptive Statistics:\n",
        "- **Descriptive Statistics**: Calculates summary measures (mean, median, std, etc.)\n",
        "- **EDA**: Uses visualizations and statistical exploration to discover patterns, relationships, and generate insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJZ8Rh9RuDuH"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For automated pipelines\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adHc5FFjuDuJ"
      },
      "outputs": [],
      "source": [
        "# Load dataset - we'll use Titanic for comprehensive EDA\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumn names:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riBfXuW9uDuJ"
      },
      "source": [
        "## Part 1: Univariate EDA\n",
        "\n",
        "Analyzing each variable independently to understand its distribution, central tendency, spread, and anomalies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1Mw0eiuuDuJ"
      },
      "source": [
        "### 1.1 Automated Univariate Analysis Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IP4s-YSuDuK"
      },
      "outputs": [],
      "source": [
        "def univariate_analysis(df, column, target=None):\n",
        "    \"\"\"\n",
        "    Comprehensive univariate analysis for a single column.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "    column : str - column name to analyze\n",
        "    target : str - optional target variable for stratified analysis\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(f\"UNIVARIATE ANALYSIS: {column}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Basic information\n",
        "    print(f\"\\n1. BASIC INFORMATION:\")\n",
        "    print(f\"   Data Type: {df[column].dtype}\")\n",
        "    print(f\"   Non-null Count: {df[column].notna().sum()} / {len(df)} ({df[column].notna().sum()/len(df)*100:.1f}%)\")\n",
        "    print(f\"   Missing Count: {df[column].isna().sum()} ({df[column].isna().sum()/len(df)*100:.1f}%)\")\n",
        "    print(f\"   Unique Values: {df[column].nunique()}\")\n",
        "\n",
        "    # Numerical analysis\n",
        "    if pd.api.types.is_numeric_dtype(df[column]):\n",
        "        print(f\"\\n2. DESCRIPTIVE STATISTICS:\")\n",
        "        print(df[column].describe())\n",
        "\n",
        "        print(f\"\\n3. DISTRIBUTION CHARACTERISTICS:\")\n",
        "        print(f\"   Skewness: {df[column].skew():.3f}\")\n",
        "        print(f\"   Kurtosis: {df[column].kurtosis():.3f}\")\n",
        "\n",
        "        # Outlier detection\n",
        "        Q1 = df[column].quantile(0.25)\n",
        "        Q3 = df[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_fence = Q1 - 1.5 * IQR\n",
        "        upper_fence = Q3 + 1.5 * IQR\n",
        "        outliers = df[(df[column] < lower_fence) | (df[column] > upper_fence)]\n",
        "\n",
        "        print(f\"\\n4. OUTLIER DETECTION (IQR Method):\")\n",
        "        print(f\"   Lower Fence: {lower_fence:.2f}\")\n",
        "        print(f\"   Upper Fence: {upper_fence:.2f}\")\n",
        "        print(f\"   Outliers: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")\n",
        "\n",
        "        # Visualization\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "        # Histogram with KDE\n",
        "        axes[0, 0].hist(df[column].dropna(), bins=30, alpha=0.6, color='skyblue', edgecolor='black', density=True)\n",
        "        df[column].dropna().plot(kind='kde', ax=axes[0, 0], color='red', linewidth=2)\n",
        "        axes[0, 0].axvline(df[column].mean(), color='green', linestyle='--', linewidth=2, label='Mean')\n",
        "        axes[0, 0].axvline(df[column].median(), color='orange', linestyle='--', linewidth=2, label='Median')\n",
        "        axes[0, 0].set_xlabel(column)\n",
        "        axes[0, 0].set_ylabel('Density')\n",
        "        axes[0, 0].set_title(f'Distribution of {column}')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "        # Box plot\n",
        "        bp = axes[0, 1].boxplot(df[column].dropna(), vert=True, patch_artist=True)\n",
        "        bp['boxes'][0].set_facecolor('lightblue')\n",
        "        axes[0, 1].set_ylabel(column)\n",
        "        axes[0, 1].set_title(f'Box Plot of {column}')\n",
        "        axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "        # Q-Q plot\n",
        "        stats.probplot(df[column].dropna(), dist=\"norm\", plot=axes[1, 0])\n",
        "        axes[1, 0].set_title(f'Q-Q Plot of {column}')\n",
        "        axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "        # Violin plot by target (if provided)\n",
        "        if target and target in df.columns:\n",
        "            df_plot = df[[column, target]].dropna()\n",
        "            sns.violinplot(data=df_plot, x=target, y=column, ax=axes[1, 1])\n",
        "            axes[1, 1].set_title(f'{column} by {target}')\n",
        "        else:\n",
        "            axes[1, 1].text(0.5, 0.5, 'No target variable provided',\n",
        "                          ha='center', va='center', transform=axes[1, 1].transAxes)\n",
        "            axes[1, 1].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Categorical analysis\n",
        "    else:\n",
        "        print(f\"\\n2. VALUE COUNTS:\")\n",
        "        counts = df[column].value_counts()\n",
        "        print(counts)\n",
        "\n",
        "        print(f\"\\n3. PROPORTIONS:\")\n",
        "        print(df[column].value_counts(normalize=True))\n",
        "\n",
        "        # Visualization\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "        # Bar plot\n",
        "        counts.plot(kind='bar', ax=axes[0], color='skyblue', edgecolor='black')\n",
        "        axes[0].set_xlabel(column)\n",
        "        axes[0].set_ylabel('Count')\n",
        "        axes[0].set_title(f'Distribution of {column}')\n",
        "        axes[0].grid(alpha=0.3, axis='y')\n",
        "\n",
        "        # Pie chart\n",
        "        if len(counts) <= 10:  # Only for reasonable number of categories\n",
        "            axes[1].pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=90)\n",
        "            axes[1].set_title(f'Proportion of {column}')\n",
        "        else:\n",
        "            axes[1].text(0.5, 0.5, f'Too many categories ({len(counts)}) for pie chart',\n",
        "                        ha='center', va='center')\n",
        "            axes[1].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Relationship with target if provided\n",
        "        if target and target in df.columns:\n",
        "            print(f\"\\n4. RELATIONSHIP WITH {target}:\")\n",
        "            crosstab = pd.crosstab(df[column], df[target], normalize='index')\n",
        "            print(crosstab)\n",
        "\n",
        "            # Visualization\n",
        "            crosstab.plot(kind='bar', stacked=False, figsize=(10, 5))\n",
        "            plt.xlabel(column)\n",
        "            plt.ylabel(f'Proportion of {target}')\n",
        "            plt.title(f'{column} vs {target}')\n",
        "            plt.legend(title=target)\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.grid(alpha=0.3, axis='y')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "print(\"Univariate analysis function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "911ngo0muDuK"
      },
      "outputs": [],
      "source": [
        "# Example: Analyze age\n",
        "univariate_analysis(df, 'age', target='survived')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPKbASleuDuL"
      },
      "outputs": [],
      "source": [
        "# Example: Analyze sex\n",
        "univariate_analysis(df, 'sex', target='survived')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLVIdZS3uDuL"
      },
      "source": [
        "## Part 2: Bivariate EDA\n",
        "\n",
        "Exploring relationships between two variables to understand associations, correlations, and dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjkGn-4wuDuL"
      },
      "source": [
        "### 2.1 Numerical vs Numerical Relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6oXfkbTuDuM"
      },
      "outputs": [],
      "source": [
        "def analyze_numerical_relationship(df, var1, var2, target=None):\n",
        "    \"\"\"\n",
        "    Analyze relationship between two numerical variables.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(f\"BIVARIATE ANALYSIS: {var1} vs {var2}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Calculate correlation\n",
        "    correlation = df[[var1, var2]].corr().iloc[0, 1]\n",
        "    print(f\"\\nPearson Correlation: {correlation:.3f}\")\n",
        "\n",
        "    # Interpretation\n",
        "    if abs(correlation) > 0.7:\n",
        "        strength = \"Strong\"\n",
        "    elif abs(correlation) > 0.4:\n",
        "        strength = \"Moderate\"\n",
        "    else:\n",
        "        strength = \"Weak\"\n",
        "\n",
        "    direction = \"positive\" if correlation > 0 else \"negative\"\n",
        "    print(f\"Interpretation: {strength} {direction} correlation\")\n",
        "\n",
        "    # Visualizations\n",
        "    if target:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "        # Scatter plot with regression line\n",
        "        axes[0].scatter(df[var1], df[var2], alpha=0.5)\n",
        "        z = np.polyfit(df[var1].dropna(), df[var2].dropna(), 1)\n",
        "        p = np.poly1d(z)\n",
        "        axes[0].plot(df[var1], p(df[var1]), \"r--\", alpha=0.8, linewidth=2)\n",
        "        axes[0].set_xlabel(var1)\n",
        "        axes[0].set_ylabel(var2)\n",
        "        axes[0].set_title(f'{var1} vs {var2}\\nCorrelation: {correlation:.3f}')\n",
        "        axes[0].grid(alpha=0.3)\n",
        "\n",
        "        # Scatter plot colored by target\n",
        "        for target_val in df[target].unique():\n",
        "            mask = df[target] == target_val\n",
        "            axes[1].scatter(df.loc[mask, var1], df.loc[mask, var2],\n",
        "                          label=f'{target}={target_val}', alpha=0.6)\n",
        "        axes[1].set_xlabel(var1)\n",
        "        axes[1].set_ylabel(var2)\n",
        "        axes[1].set_title(f'{var1} vs {var2} (colored by {target})')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(alpha=0.3)\n",
        "    else:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        ax.scatter(df[var1], df[var2], alpha=0.5)\n",
        "        z = np.polyfit(df[var1].dropna(), df[var2].dropna(), 1)\n",
        "        p = np.poly1d(z)\n",
        "        ax.plot(df[var1], p(df[var1]), \"r--\", alpha=0.8, linewidth=2)\n",
        "        ax.set_xlabel(var1)\n",
        "        ax.set_ylabel(var2)\n",
        "        ax.set_title(f'{var1} vs {var2}\\nCorrelation: {correlation:.3f}')\n",
        "        ax.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example\n",
        "analyze_numerical_relationship(df, 'age', 'fare', target='survived')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpsJVOPeuDuM"
      },
      "source": [
        "### 2.2 Categorical vs Numerical Relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5DzVXuOuDuM"
      },
      "outputs": [],
      "source": [
        "def analyze_cat_num_relationship(df, categorical, numerical):\n",
        "    \"\"\"\n",
        "    Analyze relationship between categorical and numerical variable.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(f\"BIVARIATE ANALYSIS: {categorical} (categorical) vs {numerical} (numerical)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Summary statistics by category\n",
        "    print(\"\\nSummary Statistics by Category:\")\n",
        "    summary = df.groupby(categorical)[numerical].describe()\n",
        "    print(summary)\n",
        "\n",
        "    # Statistical test (ANOVA if >2 groups, t-test if 2 groups)\n",
        "    groups = [group[numerical].dropna() for name, group in df.groupby(categorical)]\n",
        "\n",
        "    if len(groups) == 2:\n",
        "        stat, p_value = stats.ttest_ind(groups[0], groups[1])\n",
        "        test_name = \"Independent t-test\"\n",
        "    else:\n",
        "        stat, p_value = stats.f_oneway(*groups)\n",
        "        test_name = \"ANOVA\"\n",
        "\n",
        "    print(f\"\\n{test_name}:\")\n",
        "    print(f\"  Statistic: {stat:.3f}\")\n",
        "    print(f\"  P-value: {p_value:.4f}\")\n",
        "    print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'} (α=0.05)\")\n",
        "\n",
        "    # Visualizations\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "    # Box plot\n",
        "    df.boxplot(column=numerical, by=categorical, ax=axes[0])\n",
        "    axes[0].set_title(f'{numerical} by {categorical}')\n",
        "    axes[0].set_xlabel(categorical)\n",
        "    axes[0].set_ylabel(numerical)\n",
        "    plt.sca(axes[0])\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Violin plot\n",
        "    sns.violinplot(data=df, x=categorical, y=numerical, ax=axes[1])\n",
        "    axes[1].set_title(f'{numerical} Distribution by {categorical}')\n",
        "    axes[1].set_xlabel(categorical)\n",
        "    axes[1].set_ylabel(numerical)\n",
        "    plt.sca(axes[1])\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Strip plot with means\n",
        "    sns.stripplot(data=df, x=categorical, y=numerical, alpha=0.3, ax=axes[2])\n",
        "    means = df.groupby(categorical)[numerical].mean()\n",
        "    axes[2].plot(range(len(means)), means.values, 'r-o', linewidth=2, markersize=10, label='Mean')\n",
        "    axes[2].set_title(f'{numerical} by {categorical} (with means)')\n",
        "    axes[2].set_xlabel(categorical)\n",
        "    axes[2].set_ylabel(numerical)\n",
        "    axes[2].legend()\n",
        "    plt.sca(axes[2])\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example\n",
        "analyze_cat_num_relationship(df, 'class', 'fare')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp8lRF08uDuN"
      },
      "source": [
        "### 2.3 Categorical vs Categorical Relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdjYYtwLuDuN"
      },
      "outputs": [],
      "source": [
        "def analyze_cat_cat_relationship(df, cat1, cat2):\n",
        "    \"\"\"\n",
        "    Analyze relationship between two categorical variables.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(f\"BIVARIATE ANALYSIS: {cat1} vs {cat2} (both categorical)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Contingency table\n",
        "    print(\"\\nContingency Table (Counts):\")\n",
        "    ct = pd.crosstab(df[cat1], df[cat2], margins=True)\n",
        "    print(ct)\n",
        "\n",
        "    print(\"\\nContingency Table (Proportions):\")\n",
        "    ct_prop = pd.crosstab(df[cat1], df[cat2], normalize='index')\n",
        "    print(ct_prop)\n",
        "\n",
        "    # Chi-square test\n",
        "    chi2, p_value, dof, expected = stats.chi2_contingency(pd.crosstab(df[cat1], df[cat2]))\n",
        "\n",
        "    print(f\"\\nChi-Square Test of Independence:\")\n",
        "    print(f\"  Chi-square statistic: {chi2:.3f}\")\n",
        "    print(f\"  P-value: {p_value:.4f}\")\n",
        "    print(f\"  Degrees of freedom: {dof}\")\n",
        "    print(f\"  Significant association: {'Yes' if p_value < 0.05 else 'No'} (α=0.05)\")\n",
        "\n",
        "    # Cramér's V (effect size)\n",
        "    n = len(df)\n",
        "    min_dim = min(len(df[cat1].unique()), len(df[cat2].unique())) - 1\n",
        "    cramers_v = np.sqrt(chi2 / (n * min_dim))\n",
        "    print(f\"  Cramér's V (effect size): {cramers_v:.3f}\")\n",
        "\n",
        "    # Visualizations\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Stacked bar chart\n",
        "    ct_for_plot = pd.crosstab(df[cat1], df[cat2])\n",
        "    ct_for_plot.plot(kind='bar', stacked=True, ax=axes[0])\n",
        "    axes[0].set_title(f'{cat1} vs {cat2} (Stacked Bar)')\n",
        "    axes[0].set_xlabel(cat1)\n",
        "    axes[0].set_ylabel('Count')\n",
        "    axes[0].legend(title=cat2)\n",
        "    axes[0].grid(alpha=0.3, axis='y')\n",
        "    plt.sca(axes[0])\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Grouped bar chart (proportions)\n",
        "    ct_prop.plot(kind='bar', ax=axes[1])\n",
        "    axes[1].set_title(f'{cat1} vs {cat2} (Proportions)')\n",
        "    axes[1].set_xlabel(cat1)\n",
        "    axes[1].set_ylabel('Proportion')\n",
        "    axes[1].legend(title=cat2)\n",
        "    axes[1].grid(alpha=0.3, axis='y')\n",
        "    plt.sca(axes[1])\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Heatmap\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(ct_prop, annot=True, fmt='.2f', cmap='YlOrRd', cbar_kws={'label': 'Proportion'})\n",
        "    plt.title(f'Heatmap: {cat1} vs {cat2}')\n",
        "    plt.xlabel(cat2)\n",
        "    plt.ylabel(cat1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example\n",
        "analyze_cat_cat_relationship(df, 'sex', 'survived')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27RGURRXuDuN"
      },
      "source": [
        "## Part 3: Multivariate EDA\n",
        "\n",
        "Understanding complex interactions between multiple variables simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AC8FGHYuDuN"
      },
      "outputs": [],
      "source": [
        "# Correlation matrix for all numerical variables\n",
        "numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_matrix = df[numerical_cols].corr()\n",
        "\n",
        "# Visualize correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f',\n",
        "            cmap='coolwarm', center=0, square=True,\n",
        "            linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Matrix - Numerical Variables', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find strong correlations\n",
        "print(\"Strong Correlations (|r| > 0.5):\")\n",
        "print(\"=\"*50)\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i+1, len(corr_matrix.columns)):\n",
        "        if abs(corr_matrix.iloc[i, j]) > 0.5:\n",
        "            print(f\"{corr_matrix.columns[i]} <-> {corr_matrix.columns[j]}: {corr_matrix.iloc[i, j]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNaHfhuXuDuN"
      },
      "outputs": [],
      "source": [
        "# Pair plot for key variables\n",
        "key_vars = ['age', 'fare', 'pclass', 'survived']\n",
        "sns.pairplot(df[key_vars].dropna(), hue='survived', diag_kind='kde',\n",
        "             palette={0: 'red', 1: 'green'}, plot_kws={'alpha': 0.6})\n",
        "plt.suptitle('Pair Plot: Key Variables Colored by Survival', y=1.02, fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B26fxZHQuDuN"
      },
      "outputs": [],
      "source": [
        "# Three-way relationship: Age vs Fare colored by Survival, faceted by Class\n",
        "g = sns.FacetGrid(df, col='pclass', hue='survived', height=4, aspect=1.2,\n",
        "                  palette={0: 'red', 1: 'green'})\n",
        "g.map(plt.scatter, 'age', 'fare', alpha=0.6)\n",
        "g.add_legend(title='Survived')\n",
        "g.set_axis_labels('Age', 'Fare')\n",
        "g.fig.suptitle('Age vs Fare by Class and Survival', y=1.02, fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf0ESs3vuDuO"
      },
      "source": [
        "## Part 4: Hypothesis Generation from EDA\n",
        "\n",
        "Based on our exploratory analysis, we can generate testable hypotheses:\n",
        "\n",
        "1. **H1**: Women have significantly higher survival rates than men\n",
        "2. **H2**: Passengers in higher classes (1st class) have higher survival rates\n",
        "3. **H3**: Age is negatively associated with survival (younger passengers survived more)\n",
        "4. **H4**: Fare is positively associated with survival (higher fare = higher survival)\n",
        "5. **H5**: Family size affects survival (traveling with family vs alone)\n",
        "\n",
        "These hypotheses can be formally tested using statistical tests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ_KeYqj0CMb"
      },
      "source": [
        "## Summary: Best Practices\n",
        "\n",
        "### EDA Best Practices:\n",
        "1. **Start with univariate analysis** - understand each variable individually\n",
        "2. **Look for patterns in bivariate relationships** - understand associations\n",
        "3. **Use multivariate techniques** to uncover complex interactions\n",
        "4. **Generate hypotheses** from visual and statistical exploration\n",
        "5. **Document findings** and insights throughout the process\n",
        "6. **Iterate** - EDA is not linear; revisit earlier steps as you learn more"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9k_J5dTuDuO"
      },
      "source": [
        "# Automated Data Processing Pipelines\n",
        "\n",
        "Building robust, reusable, production-ready data processing pipelines using sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWx8YrJMuDuO"
      },
      "source": [
        "## Part 1: Why Use Pipelines?\n",
        "\n",
        "**Benefits**:\n",
        "1. **Reproducibility**: Same transformations applied consistently\n",
        "2. **Prevent Data Leakage**: Fit only on training data, transform on test data\n",
        "3. **Code Organization**: Clean, modular code\n",
        "4. **Production Deployment**: Easy to save and load entire workflow\n",
        "5. **Hyperparameter Tuning**: Can tune preprocessing parameters with GridSearch\n",
        "\n",
        "**Pipeline Components**:\n",
        "- **Transformers**: Classes that implement `fit()` and `transform()` methods\n",
        "- **Estimators**: Classes that implement `fit()` and `predict()` methods\n",
        "- **ColumnTransformer**: Apply different transformations to different columns\n",
        "- **FeatureUnion**: Combine multiple transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LywR0AL4uDuO"
      },
      "source": [
        "## Part 2: Custom Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U2LkYoDuDuO"
      },
      "outputs": [],
      "source": [
        "# Custom transformer for feature engineering\n",
        "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Create engineered features from existing columns.\n",
        "    \"\"\"\n",
        "    def __init__(self, create_family_size=True, create_is_alone=True,\n",
        "                 create_title=True, create_age_groups=True):\n",
        "        self.create_family_size = create_family_size\n",
        "        self.create_is_alone = create_is_alone\n",
        "        self.create_title = create_title\n",
        "        self.create_age_groups = create_age_groups\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Family size\n",
        "        if self.create_family_size and 'sibsp' in X.columns and 'parch' in X.columns:\n",
        "            X['family_size'] = X['sibsp'] + X['parch'] + 1\n",
        "\n",
        "        # Is alone\n",
        "        if self.create_is_alone and 'sibsp' in X.columns and 'parch' in X.columns:\n",
        "            X['is_alone'] = ((X['sibsp'] == 0) & (X['parch'] == 0)).astype(int)\n",
        "\n",
        "        # Extract title from name\n",
        "        if self.create_title and 'name' in X.columns:\n",
        "            X['title'] = X['name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "            # Simplify titles\n",
        "            X['title'] = X['title'].replace(['Lady', 'Countess','Capt', 'Col',\n",
        "                                             'Don', 'Dr', 'Major', 'Rev', 'Sir',\n",
        "                                             'Jonkheer', 'Dona'], 'Rare')\n",
        "            X['title'] = X['title'].replace('Mlle', 'Miss')\n",
        "            X['title'] = X['title'].replace('Ms', 'Miss')\n",
        "            X['title'] = X['title'].replace('Mme', 'Mrs')\n",
        "\n",
        "        # Age groups\n",
        "        if self.create_age_groups and 'age' in X.columns:\n",
        "            X['age_group'] = pd.cut(X['age'], bins=[0, 12, 18, 35, 60, 100],\n",
        "                                   labels=['Child', 'Teen', 'Young_Adult', 'Adult', 'Senior'])\n",
        "\n",
        "        return X\n",
        "\n",
        "print(\"FeatureEngineer transformer created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x0L8qxfuDuO"
      },
      "outputs": [],
      "source": [
        "# Custom transformer for outlier handling\n",
        "class OutlierHandler(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Handle outliers using IQR method or capping.\n",
        "    \"\"\"\n",
        "    def __init__(self, method='cap', threshold=1.5):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        -----------\n",
        "        method : str\n",
        "            'cap' - cap outliers at fence values\n",
        "            'remove' - remove outliers (use with caution)\n",
        "        threshold : float\n",
        "            IQR multiplier (typically 1.5 or 3.0)\n",
        "        \"\"\"\n",
        "        self.method = method\n",
        "        self.threshold = threshold\n",
        "        self.lower_bounds = {}\n",
        "        self.upper_bounds = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Calculate bounds for each numerical column\n",
        "        numerical_cols = X.select_dtypes(include=['number']).columns\n",
        "\n",
        "        for col in numerical_cols:\n",
        "            Q1 = X[col].quantile(0.25)\n",
        "            Q3 = X[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "\n",
        "            self.lower_bounds[col] = Q1 - self.threshold * IQR\n",
        "            self.upper_bounds[col] = Q3 + self.threshold * IQR\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        if self.method == 'cap':\n",
        "            for col in self.lower_bounds:\n",
        "                if col in X.columns:\n",
        "                    X[col] = X[col].clip(lower=self.lower_bounds[col],\n",
        "                                        upper=self.upper_bounds[col])\n",
        "        elif self.method == 'remove':\n",
        "            for col in self.lower_bounds:\n",
        "                if col in X.columns:\n",
        "                    X = X[(X[col] >= self.lower_bounds[col]) &\n",
        "                         (X[col] <= self.upper_bounds[col])]\n",
        "\n",
        "        return X\n",
        "\n",
        "print(\"OutlierHandler transformer created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOaZmfnGuDuO"
      },
      "outputs": [],
      "source": [
        "# Custom transformer for missing value summary\n",
        "class MissingValueHandler(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Handle missing values with different strategies for different column types.\n",
        "    \"\"\"\n",
        "    def __init__(self, numerical_strategy='median', categorical_strategy='most_frequent'):\n",
        "        self.numerical_strategy = numerical_strategy\n",
        "        self.categorical_strategy = categorical_strategy\n",
        "        self.numerical_imputer = None\n",
        "        self.categorical_imputer = None\n",
        "        self.numerical_cols = []\n",
        "        self.categorical_cols = []\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.numerical_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
        "        self.categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "        # Fit imputers\n",
        "        if self.numerical_cols:\n",
        "            self.numerical_imputer = SimpleImputer(strategy=self.numerical_strategy)\n",
        "            self.numerical_imputer.fit(X[self.numerical_cols])\n",
        "\n",
        "        if self.categorical_cols:\n",
        "            self.categorical_imputer = SimpleImputer(strategy=self.categorical_strategy)\n",
        "            self.categorical_imputer.fit(X[self.categorical_cols])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Transform numerical columns\n",
        "        if self.numerical_cols and self.numerical_imputer:\n",
        "            X[self.numerical_cols] = self.numerical_imputer.transform(X[self.numerical_cols])\n",
        "\n",
        "        # Transform categorical columns\n",
        "        if self.categorical_cols and self.categorical_imputer:\n",
        "            X[self.categorical_cols] = self.categorical_imputer.transform(X[self.categorical_cols])\n",
        "\n",
        "        return X\n",
        "\n",
        "print(\"MissingValueHandler transformer created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duMgK_IouDuP"
      },
      "source": [
        "## Part 3: Building the Complete Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGGJqfTmuDuP"
      },
      "outputs": [],
      "source": [
        "# Split data first (to prevent data leakage)\n",
        "# Select relevant columns\n",
        "feature_cols = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'name']\n",
        "target_col = 'survived'\n",
        "\n",
        "X = df[feature_cols].copy()\n",
        "y = df[target_col].copy()\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8z_8ZzQuDuP"
      },
      "outputs": [],
      "source": [
        "# Define column groups\n",
        "numerical_features = ['age', 'fare', 'sibsp', 'parch']\n",
        "categorical_features = ['pclass', 'sex', 'embarked']\n",
        "\n",
        "# Numerical pipeline\n",
        "numerical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('outlier_handler', OutlierHandler(method='cap', threshold=1.5)),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical pipeline\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine pipelines using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ],\n",
        "    remainder='drop'  # Drop columns not specified\n",
        ")\n",
        "\n",
        "# Complete pipeline with feature engineering\n",
        "complete_pipeline = Pipeline([\n",
        "    ('feature_engineer', FeatureEngineer(\n",
        "        create_family_size=True,\n",
        "        create_is_alone=True,\n",
        "        create_title=True,\n",
        "        create_age_groups=False  # Would need special handling in ColumnTransformer\n",
        "    )),\n",
        "    ('preprocessor', preprocessor)\n",
        "])\n",
        "\n",
        "print(\"Pipeline created successfully!\")\n",
        "print(\"\\nPipeline structure:\")\n",
        "print(complete_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vl9yonlYuDuP"
      },
      "outputs": [],
      "source": [
        "# Fit the pipeline on training data\n",
        "complete_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Transform both training and test data\n",
        "X_train_transformed = complete_pipeline.transform(X_train)\n",
        "X_test_transformed = complete_pipeline.transform(X_test)\n",
        "\n",
        "print(f\"Original training shape: {X_train.shape}\")\n",
        "print(f\"Transformed training shape: {X_train_transformed.shape}\")\n",
        "print(f\"\\nOriginal test shape: {X_test.shape}\")\n",
        "print(f\"Transformed test shape: {X_test_transformed.shape}\")\n",
        "\n",
        "# Get feature names after transformation\n",
        "feature_names = (numerical_features +\n",
        "                complete_pipeline.named_steps['preprocessor']\n",
        "                .named_transformers_['cat']\n",
        "                .named_steps['encoder']\n",
        "                .get_feature_names_out(categorical_features).tolist())\n",
        "\n",
        "print(f\"\\nFeature names after transformation:\")\n",
        "print(feature_names)\n",
        "\n",
        "# Create DataFrame for easier inspection\n",
        "X_train_df = pd.DataFrame(X_train_transformed, columns=feature_names)\n",
        "X_test_df = pd.DataFrame(X_test_transformed, columns=feature_names)\n",
        "\n",
        "print(\"\\nTransformed training data (first 5 rows):\")\n",
        "X_train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQXfq5UjuDuP"
      },
      "source": [
        "## Part 4: Pipeline Persistence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trBu-kzjuDuP"
      },
      "outputs": [],
      "source": [
        "# Save the pipeline\n",
        "pipeline_filename = 'titanic_preprocessing_pipeline.pkl'\n",
        "joblib.dump(complete_pipeline, pipeline_filename)\n",
        "print(f\"Pipeline saved to {pipeline_filename}\")\n",
        "\n",
        "# Load the pipeline\n",
        "loaded_pipeline = joblib.load(pipeline_filename)\n",
        "print(f\"Pipeline loaded from {pipeline_filename}\")\n",
        "\n",
        "# Verify it works\n",
        "X_test_from_loaded = loaded_pipeline.transform(X_test)\n",
        "print(f\"\\nTransformation successful: {X_test_from_loaded.shape}\")\n",
        "print(f\"Results match: {np.allclose(X_test_transformed, X_test_from_loaded)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnsPKNaruDuP"
      },
      "source": [
        "## Part 5: Pipeline with Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwC9En1auDuQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Create end-to-end pipeline including model\n",
        "full_pipeline = Pipeline([\n",
        "    ('feature_engineer', FeatureEngineer(\n",
        "        create_family_size=True,\n",
        "        create_is_alone=True,\n",
        "        create_title=True,\n",
        "        create_age_groups=False\n",
        "    )),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "# Fit the entire pipeline\n",
        "full_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = full_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Model Performance:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Died', 'Survived']))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqWdLj7wuDuQ"
      },
      "source": [
        "## Part 6: Pipeline with GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSpJoyGSuDuQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "# Note: parameter names must include the step name\n",
        "param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
        "    'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()],\n",
        "    'classifier__C': [0.1, 1.0, 10.0],\n",
        "    'classifier__penalty': ['l2']\n",
        "}\n",
        "\n",
        "# Create GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    full_pipeline,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "print(\"Starting grid search...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest parameters:\")\n",
        "print(grid_search.best_params_)\n",
        "print(f\"\\nBest cross-validation score: {grid_search.best_score_:.3f}\")\n",
        "print(f\"Test set score: {grid_search.score(X_test, y_test):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeynVp-YuDuQ"
      },
      "source": [
        "---\n",
        "## Summary: Best Practices\n",
        "\n",
        "### Pipeline Best Practices:\n",
        "1. **Always split data first** - prevent data leakage\n",
        "2. **Fit only on training data** - then transform both train and test\n",
        "3. **Use custom transformers** for domain-specific operations\n",
        "4. **ColumnTransformer** for different preprocessing per column type\n",
        "5. **Pipeline** for sequential operations\n",
        "6. **Save pipelines** for reproducibility and production deployment\n",
        "7. **Include preprocessing in cross-validation** - use full pipeline in GridSearchCV\n",
        "8. **Version control** - save different pipeline versions\n",
        "9. **Test thoroughly** - ensure pipeline works on new data\n",
        "10. **Document** - comment your pipeline steps and parameters\n",
        "\n",
        "### Production Considerations:\n",
        "1. **Error handling** - add try-except blocks in custom transformers\n",
        "2. **Input validation** - check data types and values\n",
        "3. **Logging** - track pipeline execution and issues\n",
        "4. **Testing** - unit tests for custom transformers\n",
        "5. **Monitoring** - track data drift in production\n",
        "6. **Versioning** - track pipeline versions with model versions\n",
        "\n",
        "---\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "**EDA Resources:**\n",
        "- \"Exploratory Data Analysis\" by John Tukey (classic reference)\n",
        "- \"Python for Data Analysis\" by Wes McKinney\n",
        "- Kaggle notebooks on EDA techniques\n",
        "\n",
        "**Pipeline Resources:**\n",
        "- Scikit-learn Pipeline documentation\n",
        "- \"Hands-On Machine Learning\" by Aurélien Géron (Chapter 2)\n",
        "- Scikit-learn custom transformer examples\n",
        "\n",
        "**Practice:**\n",
        "- Kaggle competitions for real-world EDA practice\n",
        "- UCI Machine Learning Repository datasets\n",
        "- Build pipelines for different problem types (regression, classification, clustering)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}