{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC5082/blob/main/Interview_Prep_4/interview_prep_concepts_review_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SA-fmFAdDiH"
      },
      "source": [
        "# Feature Engineering, Selection & Dimensionality Reduction\n",
        "\n",
        "## üéØ Interview Prep: Concepts Review\n",
        "**Class Duration:** ~1 hour\n",
        "\n",
        "---\n",
        "\n",
        "### üìö Topics Covered\n",
        "1. **Feature Engineering:** Creating and transforming variables to improve model performance.\n",
        "2. **Feature Selection:** Identifying the most relevant subset of predictors.\n",
        "3. **Feature Extraction:** Creating new features from raw data (including deep learning approaches).\n",
        "4. **Dimensionality & The Curse of Dimensionality:** Understanding how high-dimensional data affects model behavior.\n",
        "5. **Dimensionality Reduction Techniques:** Methods like PCA, t-SNE, and LDA.\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Learning Objectives\n",
        "By the end of this session, you should be able to:\n",
        "\n",
        "* **Implement** core feature engineering techniques used in industry (scaling, encoding, etc.).\n",
        "* **Strategize** when and how to apply specific feature selection methods (Filter, Wrapper, and Embedded).\n",
        "* **Explain** the mechanics of automatic feature extraction within deep learning architectures.\n",
        "* **Identify** and mitigate the \"Curse of Dimensionality\" in large datasets.\n",
        "* **Apply** the appropriate dimensionality reduction techniques based on the data structure and goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q95-mpHSdDiH"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hc5EmmaZdDiH"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Module 4: Environment Setup\n",
        "# ==========================================\n",
        "\n",
        "# Standard Data Science Stack\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Preprocessing & Model Selection\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler,\n",
        "    MinMaxScaler,\n",
        "    LabelEncoder\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Feature Selection Methods\n",
        "from sklearn.feature_selection import mutual_info_classif, chi2, RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression, Lasso\n",
        "\n",
        "# Dimensionality Reduction\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Datasets for Practice\n",
        "from sklearn.datasets import load_iris, fetch_california_housing\n",
        "\n",
        "# Global Configuration\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úì Libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXHMnhk_dDiI"
      },
      "source": [
        "# 1. Feature Engineering\n",
        "\n",
        "> **Interview Question:** *\"Why is feature engineering important?\"*\n",
        "> **Answer:** Feature engineering can make or break a model. Better features often matter more than better algorithms; they allow simpler models to perform better and make the underlying patterns in the data more accessible to the learner.\n",
        "\n",
        "### üõ† Key Techniques\n",
        "\n",
        "* **Derived Features:** Creating new variables from existing ones (e.g., calculating \"Age\" from \"Date of Birth\").\n",
        "* **Transformations:** Applying mathematical functions like Log, Square Root, or Box-Cox to handle skewed data.\n",
        "* **Scaling:**\n",
        "  * **Standardization:** Rescaling data to have a mean of 0 and a standard deviation of 1.\n",
        "  * **Normalization:** Rescaling data to a fixed range, usually .\n",
        "\n",
        "\n",
        "* **Encoding:** Converting categorical variables into numerical formats (e.g., One-Hot Encoding or Label Encoding).\n",
        "* **Missing Data:** Using imputation strategies (Mean, Median, Mode, or KNN) to handle null values.\n",
        "* **Outlier Treatment:** Detection via Z-score or IQR and handling via clipping, capping, or removal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlJtogQ7dDiI"
      },
      "source": [
        "## 1.1 Scaling: Why It Matters\n",
        "\n",
        "## üéØ Interview Prep: Concepts Review\n",
        "**Context:** Essential for model convergence and accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### üìö Core Concepts\n",
        "* **Definition:** Scaling transforms features to a similar range so that no single variable dominates the model due to its magnitude.\n",
        "* **Standardization ($Z$-score):** Centers data around a mean of 0 with a standard deviation of 1.\n",
        "* **Normalization (Min-Max):** Rescales data to a fixed range, typically [0, 1].\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Key Takeaways: When to Scale\n",
        "\n",
        "* **Distance-Based Algorithms:** **KNN**, **SVM**, and **K-Means** require scaling because they rely on Euclidean distance; without it, features with larger units (like Salary) will drown out smaller ones (like Age).\n",
        "* **Gradient Descent Optimization:** Models like **Neural Networks** and **Linear/Logistic Regression** converge significantly faster when features are on the same scale.\n",
        "* **Dimensionality Reduction:** **PCA** is variance-driven. If one feature has a larger scale, PCA will incorrectly identify it as the most \"important\" component.\n",
        "* **Unit Consistency:** Always scale when your dataset mixes different units (e.g., meters, kilograms, and currency)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjE_UZUIdDiI"
      },
      "outputs": [],
      "source": [
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y = housing.target\n",
        "\n",
        "# Displaying min/max to identify scale discrepancies\n",
        "print(\"Original data ranges:\")\n",
        "print(X.describe().loc[['min', 'max']])\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è Notice: Features have vastly different scales!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlLBhfHNdDiI"
      },
      "outputs": [],
      "source": [
        "# Compare scaling methods\n",
        "scaler_std = StandardScaler()\n",
        "scaler_minmax = MinMaxScaler()\n",
        "\n",
        "X_std = scaler_std.fit_transform(X)\n",
        "X_minmax = scaler_minmax.fit_transform(X)\n",
        "\n",
        "# Visualize one feature: 'MedInc'\n",
        "feature = 'MedInc'\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Original Data\n",
        "axes[0].hist(X[feature], bins=30, edgecolor='black')\n",
        "axes[0].set_title(f'Original: {feature}')\n",
        "axes[0].set_xlabel('Value')\n",
        "\n",
        "# StandardScaler\n",
        "axes[1].hist(X_std[:, 0], bins=30, edgecolor='black', color='orange')\n",
        "axes[1].set_title('StandardScaler (mean=0, std=1)')\n",
        "axes[1].set_xlabel('Scaled Value')\n",
        "\n",
        "# MinMaxScaler\n",
        "axes[2].hist(X_minmax[:, 0], bins=30, edgecolor='black', color='green')\n",
        "axes[2].set_title('MinMaxScaler (range 0-1)')\n",
        "axes[2].set_xlabel('Scaled Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úì Same distribution shape, different scales!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLKO2qJAdDiJ"
      },
      "source": [
        "## 1.2 Comparison: Choosing the Right Scaler\n",
        "\n",
        "### üìö Key Distinctions\n",
        "Use the following table to determine which scaling method fits your specific data profile.\n",
        "\n",
        "| Method | Formula | When to Use |\n",
        "| :--- | :--- | :--- |\n",
        "| **StandardScaler** | $$z = \\frac{x - \\mu}{\\sigma}$$ | **The Default Choice.** Best for algorithms assuming Gaussian distributions (Linear/Logistic Regression) and PCA. |\n",
        "| **MinMaxScaler** | $$x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$ | **Bounded Range.** Best for Image processing, Neural Networks, or when you need exactly $[0, 1]$. |\n",
        "| **RobustScaler** | $$x_{scaled} = \\frac{x - Q_2}{Q_3 - Q_1}$$ | **Outlier-Resistant.** Uses median and IQR; essential when data contains significant noise or extreme outliers. |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5jiIYTpdDiJ"
      },
      "source": [
        "## 1.3 Categorical Encoding\n",
        "\n",
        "## üéØ Interview Prep: Concepts Review\n",
        "**Context:** Converting non-numeric data into a format usable by machine learning models while maintaining statistical integrity.\n",
        "\n",
        "\n",
        "### üìö Core Concept: The Dummy Variable Trap\n",
        "> **Interview Question:** *\"What's the dummy variable trap?\"*\n",
        ">\n",
        "> **Answer:** The dummy variable trap is a scenario where independent variables are highly correlated (multicollinearity). If you have 3 categories and create 3 dummy variables, the 3rd variable is perfectly predictable from the first two. This \"perfect multicollinearity\" can make it impossible to calculate unique coefficients in models like Linear Regression.\n",
        ">\n",
        "> **Solution:** Always drop one category (use `drop_first=True` in `get_dummies` or `OneHotEncoder`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vQIzJSEdDiJ"
      },
      "outputs": [],
      "source": [
        "# Demonstration\n",
        "pets = {'Pet': ['dog', 'cat', 'cat', 'dog', 'bird', 'cat', 'dog', 'bird']}\n",
        "df = pd.DataFrame(pets)\n",
        "\n",
        "print(\"Original:\")\n",
        "print(df['Pet'].value_counts())\n",
        "\n",
        "# WRONG WAY (dummy trap)\n",
        "df_wrong = pd.get_dummies(df, columns=['Pet'], prefix='Pet')\n",
        "print(\"\\n‚ùå WRONG (creates 3 columns for 3 categories):\")\n",
        "print(df_wrong.head())\n",
        "print(f\"Columns: {df_wrong.columns.tolist()}\")\n",
        "\n",
        "# CORRECT WAY\n",
        "df_correct = pd.get_dummies(df, columns=['Pet'], prefix='Pet', drop_first=True)\n",
        "print(\"\\n‚úì CORRECT (drop_first=True, only 2 columns):\")\n",
        "print(df_correct.head())\n",
        "print(f\"Columns: {df_correct.columns.tolist()}\")\n",
        "\n",
        "print(\"\\n'cat' is the reference category (all 0s = cat)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Dummy Trap\n",
        "\n",
        "(dummy variable trap)** refers to the problem of **perfect multicollinearity** that occurs when you one-hot encode a categorical variable and keep **all** dummy columns **along with an intercept** in a linear model.\n",
        "\n",
        "**What happens**\n",
        "\n",
        "Suppose a categorical variable has *k* categories.\n",
        "One-hot encoding creates *k* binary columns.\n",
        "\n",
        "If you also include an intercept term, then:\n",
        "\n",
        "* The sum of the *k* dummy columns = 1 for every row\n",
        "* One column can be written as a linear combination of the others\n",
        "* The design matrix becomes **rank-deficient**\n",
        "\n",
        "This breaks assumptions of linear regression and similar models, making coefficients **non-identifiable** (infinite or unstable solutions).\n",
        "\n",
        "### Simple example\n",
        "\n",
        "Categories: {Red, Blue, Green}\n",
        "\n",
        "One-hot encoded:\n",
        "\n",
        "* Red\n",
        "* Blue\n",
        "* Green\n",
        "\n",
        "For every row:\n",
        "\n",
        "```\n",
        "Red + Blue + Green = 1\n",
        "```\n",
        "\n",
        "With an intercept, this creates perfect multicollinearity.\n",
        "\n",
        "### Why it matters\n",
        "\n",
        "* Linear and generalized linear models cannot uniquely estimate coefficients\n",
        "* Inversion of the matrix fails or becomes numerically unstable\n",
        "* Coefficient interpretations become meaningless\n",
        "\n",
        "### How to avoid it\n",
        "\n",
        "* **Drop one dummy column** (use *k‚àí1* encoding)\n",
        "* The dropped category becomes the **reference (baseline)**\n",
        "* Most libraries do this automatically (e.g., `drop_first=True` in pandas)\n",
        "\n",
        "### Key takeaway\n",
        "\n",
        "The dummy trap is not about encoding itself‚Äîit‚Äôs about **keeping redundant information**.\n",
        "Always use **k‚àí1 dummy variables** when your model includes an intercept.\n"
      ],
      "metadata": {
        "id": "8ihaIHQCfmpU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCwNfaBfdDiJ"
      },
      "source": [
        "## 1.5 Missing Data Strategy\n",
        "\n",
        "## üéØ Interview Prep: Concepts Review\n",
        "**Context:** Strategizing data imputation based on the underlying cause of missingness.\n",
        "\n",
        "---\n",
        "\n",
        "### üìö Core Concepts\n",
        "> **Interview Question:** *\"How do you handle missing data?\"*\n",
        ">\n",
        "> **Answer depends on the type:**\n",
        ">\n",
        "> 1. **MCAR (Missing Completely at Random):** No relationship between missing data and any values. **Implication:** Mean/Median imputation is generally safe.\n",
        "> 2. **MAR (Missing at Random):** Missingness is related to other observed features (e.g., men are less likely to report weight). **Implication:** Use other features to predict or impute.\n",
        "> 3. **MNAR (Missing Not at Random):** The missingness itself is related to the unobserved value. **Implication:** The fact it is missing is informative; flag it! **Example:** High earners refuse to report income (MNAR) ‚Üí Use an extreme value or a binary indicator to flag the missingness."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic data\n",
        "data = pd.DataFrame({\n",
        "    'Age': [25, 30, np.nan, 45, 50, np.nan, 35],\n",
        "    'Income': [50000, np.nan, 60000, 80000, np.nan, 70000, 65000]\n",
        "})\n",
        "\n",
        "print(\"Original data:\")\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "XDVLLAB77iqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7dGBdabdDiJ"
      },
      "outputs": [],
      "source": [
        "# Strategy: Imputation (Pandas 3.0 Compatible)\n",
        "data_imputed = data.copy()\n",
        "\n",
        "# Direct assignment\n",
        "data_imputed['Age'] = data_imputed['Age'].fillna(data['Age'].median())\n",
        "data_imputed['Income'] = data_imputed['Income'].fillna(data['Income'].median())\n",
        "\n",
        "print(\"\\nAfter median imputation:\")\n",
        "print(data_imputed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeW6W9lddDiJ"
      },
      "source": [
        "# 2. Feature Selection\n",
        "\n",
        "## üéØ Interview Prep: Concepts Review\n",
        "**Context:** Identifying the most relevant features to improve model efficiency and reduce overfitting.\n",
        "\n",
        "\n",
        "\n",
        "### üìö Core Concepts\n",
        "> **Interview Question:** *\"What are the three types of feature selection methods?\"*\n",
        ">\n",
        "> **Answer:**\n",
        "| Method | How It Works | Speed | Accuracy |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Filter** | Statistical tests (correlation, chi¬≤, MI) | Fast | Good |\n",
        "| **Wrapper** | Train model iteratively (RFE) | Slow | Best |\n",
        "| **Embedded** | Built into training (Lasso, tree importance) | Medium | High |\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07E-JD5idDiJ"
      },
      "source": [
        "## 2.1 Filter Method: Correlation\n",
        "\n",
        "## üéØ Interview Prep: Concepts Review\n",
        "**Context:** Using statistical measures to identify and remove redundant information before model training.\n",
        "\n",
        "\n",
        "### üìö Core Concepts\n",
        "* **Use Case:** Quick first pass to remove redundant features.\n",
        "* **Rule of Thumb:** If |correlation| > 0.8‚Äì0.9, consider dropping one feature to reduce multicollinearity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnR3FFVQdDiJ"
      },
      "outputs": [],
      "source": [
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y_iris = iris.target\n",
        "\n",
        "# Correlation matrix\n",
        "corr_matrix = X_iris.corr()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Feature Correlation Matrix', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nHighly correlated pairs:\")\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i+1, len(corr_matrix.columns)):\n",
        "        if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
        "            print(f\"  {corr_matrix.columns[i]} & {corr_matrix.columns[j]}: {corr_matrix.iloc[i, j]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation matrix\n",
        "corr_matrix = X_iris.corr()\n",
        "\n",
        "# Create a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    corr_matrix,\n",
        "    mask=mask,\n",
        "    annot=True,\n",
        "    cmap='coolwarm',\n",
        "    center=0,\n",
        "    square=True,\n",
        "    linewidths=1,\n",
        "    cbar_kws={\"shrink\": 0.8}\n",
        ")\n",
        "\n",
        "plt.title('Feature Correlation Matrix (Lower Triangle)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Jdv5GqBdg2-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XCOjlErdDiJ"
      },
      "source": [
        "## 2.2 Filter Method: Mutual Information\n",
        "\n",
        "## üéØ Interview Prep: Concepts Review\n",
        "**Context:** Evaluating feature relevance through information gain and statistical dependence.\n",
        "\n",
        "\n",
        "### üìö Core Concepts\n",
        "* **Advantage over correlation:** Unlike Pearson correlation, which only measures linear associations, **Mutual Information (MI)** captures any kind of statistical dependency, including non-linear relationships!\n",
        "* **Scale:**\n",
        "    * **MI = 0:** The variables are completely independent.\n",
        "    * **Higher MI:** Indicates a stronger relationship between the feature and the target.\n",
        "\n",
        "\n",
        "\n",
        "> **Interview Tip:** Always mention that Mutual Information is non-parametric and doesn't assume a normal distribution, making it more robust than simple correlation for complex datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBmULAbWdDiK"
      },
      "outputs": [],
      "source": [
        "# Mutual Information\n",
        "mi_scores = mutual_info_classif(X_iris, y_iris, random_state=42)\n",
        "mi_scores = pd.Series(mi_scores, index=X_iris.columns).sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "mi_scores.plot(kind='barh', color='coral')\n",
        "plt.xlabel('Mutual Information Score')\n",
        "plt.title('Feature Importance via Mutual Information')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Mutual Information Scores:\")\n",
        "print(mi_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNz4lA5tdDiK"
      },
      "source": [
        "## 2.3 Wrapper Method: Recursive Feature Elimination (RFE)\n",
        "\n",
        "## üéØ Interview Prep: Concepts Review\n",
        "**Context:** Iteratively selecting features by training a model and removing the least important predictors.\n",
        "\n",
        "### üìö Core Concepts\n",
        "> **Interview Question:** *\"What's the advantage of wrapper methods?\"*\n",
        ">\n",
        "> **Answer:** Unlike filter methods, wrappers consider **feature interactions**. They evaluate how features perform together within a specific model architecture, making them highly optimized for that learner.\n",
        ">\n",
        "> **Downside:** They are **computationally expensive** because they require retraining the model multiple times (e.g., $N-1$ times for RFE).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifK62BHfdDiK"
      },
      "outputs": [],
      "source": [
        "# RFE with Logistic Regression\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "rfe = RFE(estimator=model, n_features_to_select=2)\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "rfe_results = pd.DataFrame({\n",
        "    'Feature': X_iris.columns,\n",
        "    'Selected': rfe.support_,\n",
        "    'Ranking': rfe.ranking_\n",
        "}).sort_values('Ranking')\n",
        "\n",
        "print(\"RFE Results:\")\n",
        "print(rfe_results)\n",
        "print(f\"\\nSelected features: {X_iris.columns[rfe.support_].tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NU-GgLPdDiK"
      },
      "source": [
        "## 2.4 Embedded Method: Tree-Based Feature Importance\n",
        "\n",
        "## üéØ Interview Prep: Concepts Review\n",
        "**Context:** Leveraging models that perform feature selection automatically as part of the training process.\n",
        "\n",
        "### üìö Core Concepts\n",
        "* **Definition:** Embedded methods integrate feature selection directly into the model construction.\n",
        "* **Tree-Based Importance:** Decision trees and ensembles (like Random Forest or XGBoost) calculate importance based on how much each feature reduces impurity (Gini or Entropy) across all nodes.\n",
        "\n",
        "\n",
        "> **Interview Tip:** Random Forests and Gradient Boosting Machines provide feature importance \"for free\" during the training phase, making them highly efficient for initial data exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZwNIqcfdDiK"
      },
      "outputs": [],
      "source": [
        "# Random Forest feature importance\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get importances\n",
        "importances = pd.Series(rf.feature_importances_, index=X_iris.columns).sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "importances.plot(kind='barh', color='forestgreen')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Random Forest Feature Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "print(importances)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importance measures how much a feature reduces impurity (e.g., Gini or entropy)"
      ],
      "metadata": {
        "id": "nM3Zt-TGsP7M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IOVgTLVdDiK"
      },
      "source": [
        "# 3. Feature Extraction (Deep Learning)\n",
        "\n",
        "## üéØ Interview Prep: Concepts Review\n",
        "**Context:** Understanding how deep neural networks automatically transform raw data into informative representations.\n",
        "\n",
        "### üìö Core Concepts\n",
        "> **Interview Question:** *\"How does feature extraction differ from feature selection?\"*\n",
        ">\n",
        "> **Answer:** > * **Feature Selection:** Selecting a subset of the **existing** features.\n",
        "> * **Feature Extraction:** Creating **new** features by transforming or projecting existing data into a new space (e.g., PCA or Neural Network embeddings).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 CNNs (Computer Vision)\n",
        "**How CNNs extract features:**\n",
        "CNNs learn a hierarchy of features through successive layers of convolution and pooling:\n",
        "\n",
        "1. **Early Layers:** Detect low-level features like **edges, corners, and colors**.\n",
        "2. **Middle Layers:** Combine low-level features into **textures and patterns**.\n",
        "3. **Deep Layers:** Assemble patterns into complex, high-level **objects or faces**.\n",
        "\n",
        "CNNs learn visual meaning the way we learn language: from letters, to words, to sentences, and finally to concepts.\n",
        "\n",
        "**Transfer Learning:**\n",
        "You can use a pre-trained CNN (such as **VGG16**, **ResNet**, or **Inception**) as a fixed feature extractor by removing the final classification head and using the output of the convolutional base as input for a new model."
      ],
      "metadata": {
        "id": "bjlHiVq32wF1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMnjAtkGdDiK"
      },
      "source": [
        "## 3.2 LLMs (Natural Language Processing)\n",
        "\n",
        "## üéØ Interview Prep: Concepts Review\n",
        "**Context:** Understanding how Transformer architectures transform raw text into high-dimensional contextual embeddings.\n",
        "\n",
        "\n",
        "### üìö Core Concepts\n",
        "**How Transformers extract features:**\n",
        "\n",
        "1.  **Tokenization:** Breaking raw text into smaller sub-word units or tokens.\n",
        "2.  **Embeddings:** Mapping these tokens into a dense vector space where semantic meaning is represented numerically.\n",
        "3.  **Attention:** The core mechanism that creates **contextualized representations** by weighing the importance of surrounding words.\n",
        "\n",
        "\n",
        "\n",
        "**Example: The Polysemy Problem**\n",
        "The word \"**bank**\" extracts entirely different feature vectors depending on its neighbors:\n",
        "* *\"river **bank**\"*: Contextual features relate to **geography/nature**.\n",
        "* *\"**bank** account\"*: Contextual features relate to **finance/business**.\n",
        "\n",
        "The **Attention Mechanism** dynamically extracts the relevant context, ensuring the model \"understands\" which version of the word is being used based on the surrounding tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWrGpah1dDiK"
      },
      "source": [
        "# 4. Dimensionality & The Curse\n",
        "\n",
        "## üéØ Interview Prep: Concepts Review\n",
        "**Context:** Understanding the theoretical and practical challenges of working with high-dimensional feature spaces.\n",
        "\n",
        "\n",
        "### üìö Core Concepts\n",
        "> **Interview Question:** *\"What is the curse of dimensionality?\"*\n",
        ">\n",
        "> **Answer:** The \"Curse\" refers to a set of phenomena that arise when analyzing data in high-dimensional spaces that do not occur in low-dimensional settings. As dimensions increase:\n",
        ">\n",
        "> 1. **Data Sparsity:** The volume of the space increases so fast that the available data becomes sparse. You require exponentially more data to maintain the same level of density.\n",
        "> 2. **Distance Meaninglessness:** In high dimensions, the difference between the minimum and maximum distance between points decreases. To a model, everything begins to look equally \"far apart,\" breaking distance-based algorithms like KNN.\n",
        "> 3. **Computational Explosion:** Processing time and memory requirements grow significantly.\n",
        "> 4. **Overfitting:** With too many features relative to observations, models easily \"memorize\" noise rather than learning the underlying signal.\n",
        "\n",
        "\n",
        "**Example:** To maintain the same data density when moving from **2D** to **10D**, you would theoretically need **$10^8$** times more data! (**$10^8$** = **$10^10$** - **$10^2$**)\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDOh746KdDiK"
      },
      "outputs": [],
      "source": [
        "# Demonstrate curse of dimensionality\n",
        "from scipy.spatial import distance\n",
        "\n",
        "dimensions = [2, 10, 50, 100]\n",
        "n_points = 100\n",
        "results = []\n",
        "\n",
        "for d in dimensions:\n",
        "    # Generate random points\n",
        "    points = np.random.rand(n_points, d)\n",
        "\n",
        "    # Calculate all pairwise distances\n",
        "    distances = []\n",
        "    for i in range(n_points):\n",
        "        for j in range(i+1, n_points):\n",
        "            distances.append(distance.euclidean(points[i], points[j]))\n",
        "\n",
        "    distances = np.array(distances)\n",
        "    results.append({\n",
        "        'Dimensions': d,\n",
        "        'Mean Distance': distances.mean(),\n",
        "        'Std Distance': distances.std(),\n",
        "        'Coef. of Variation': distances.std() / distances.mean()\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"Effect of Dimensionality on Distances:\")\n",
        "print(results_df)\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è As dimensions increase, coefficient of variation DECREASES\")\n",
        "print(\"   This means all points become roughly equidistant!\")\n",
        "print(\"   ‚Üí Clustering and nearest-neighbor methods fail!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24153my1dDiK"
      },
      "source": [
        "# 5. Dimensionality Reduction\n",
        "\n",
        "## üéØ Interview Prep: Concepts Review\n",
        "**Context:** Compressing feature spaces while retaining essential information or maximizing class discriminability.\n",
        "\n",
        "---\n",
        "\n",
        "### üìö Core Concepts\n",
        "> **Interview Question:** *\"When would you use PCA vs LDA vs t-SNE?\"*\n",
        ">\n",
        "> **Answer:**\n",
        "| Method | Type | Supervised? | Best For |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **PCA** (Principal Component Analysis) | Linear | No | **Variance Preservation.** Best for general preprocessing and noise reduction. |\n",
        "| **LDA** (Linear Discriminant Analysis) | Linear | Yes | **Class Separation.** Best for supervised dimensionality reduction before classification. |\n",
        "| **t-SNE** (t-Distributed Stochastic Neighbor Embedding) | Non-linear | No | **Visualization.** Best for exploring local clusters in high-dimensional data; not for preprocessing. |\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Key Takeaways\n",
        "* **PCA** finds the axes with the maximum variance regardless of labels.\n",
        "* **LDA** finds the axes that maximize the distance between different classes.\n",
        "* **t-SNE** is computationally intensive and preserves local neighborhoods, but the distances between clusters in a t-SNE plot are not always meaningful.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvXmY4NzdDiK"
      },
      "source": [
        "## 5.1 Demonstration: PCA Implementation\n",
        "\n",
        "## üéØ Interview Prep: Concepts Review\n",
        "\n",
        "**Context:** Projecting high-dimensional data into 2D for visualization while maximizing variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVibTTMOdDiK"
      },
      "outputs": [],
      "source": [
        "# PCA demonstration\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_iris)\n",
        "\n",
        "# Fit PCA\n",
        "pca = PCA(n_components=4)\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Explained variance\n",
        "explained_var = pd.DataFrame({\n",
        "    'PC': [f'PC{i+1}' for i in range(4)],\n",
        "    'Explained Variance': pca.explained_variance_ratio_,\n",
        "    'Cumulative': np.cumsum(pca.explained_variance_ratio_)\n",
        "})\n",
        "\n",
        "print(\"Explained Variance:\")\n",
        "print(explained_var)\n",
        "\n",
        "# Scree plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(explained_var['PC'], explained_var['Explained Variance'], color='steelblue', edgecolor='black')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Scree Plot')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(explained_var['PC'], explained_var['Cumulative'], marker='o', color='darkred')\n",
        "plt.axhline(y=0.95, color='black', linestyle='--', label='95% threshold')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Cumulative Variance')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úì First 2 components explain {explained_var.iloc[1]['Cumulative']:.1%} of variance!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFQsfIpHdDiK"
      },
      "source": [
        "## 5.2 Interview Strategy: Selecting Optimal Components\n",
        "\n",
        "## üéØ Interview Prep: Concepts Review\n",
        "**Context:** Defending your choice of dimensionality and explaining the trade-offs between information loss and model simplicity.\n",
        "\n",
        "### üìö Selection Strategies\n",
        "\n",
        "> **Interview Question:** *\"How do you choose the number of components?\"*\n",
        "\n",
        "#### 1. The Elbow Method\n",
        "Plot the explained variance for each component in a **Scree Plot**. Look for the \"elbow\"‚Äîthe point where the variance drop-off levels off. This indicates that additional components are likely capturing more noise than signal.\n",
        "\n",
        "\n",
        "#### 2. Cumulative Explained Variance\n",
        "Calculate the running total of variance captured. A common heuristic is to retain enough components to cover **90% to 95%** of the total variance in the dataset.\n",
        "\n",
        "\n",
        "#### 3. Downstream Model Performance (Cross-Validation)\n",
        "The most robust \"real-world\" method. Treat the number of components ($k$) as a **hyperparameter**. Use cross-validation to find the value of $k$ that yields the best performance on your specific target metric (e.g., F1-score or RMSE).\n",
        "\n",
        "\n",
        "### üöÄ Key Takeaways\n",
        "* **Parsimony:** If 2 components capture 85% of variance and 10 components capture 90%, the simpler 2-component model is often preferred to prevent overfitting and reduce computational cost.\n",
        "* **Context Matters:** If the goal is **visualization**, you are usually restricted to $k=2$ or $k=3$. If the goal is **compression**, follow the 95% variance rule.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Standardize features\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "pca = PCA(n_components=4)\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "loadings = pd.DataFrame(\n",
        "    pca.components_.T,\n",
        "    columns=[f'PC{i+1}' for i in range(4)],\n",
        "    index=feature_names\n",
        ")\n",
        "\n",
        "loadings\n"
      ],
      "metadata": {
        "id": "POQYfIMsvdjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJWqHqZodDiK"
      },
      "outputs": [],
      "source": [
        "# Reduce to 2D for visualization\n",
        "pca_2d = PCA(n_components=2)\n",
        "X_pca = pca_2d.fit_transform(X_scaled)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, species in enumerate(iris.target_names):\n",
        "    mask = y_iris == i\n",
        "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], label=species, alpha=0.6, s=50)\n",
        "\n",
        "plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%} variance)')\n",
        "plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%} variance)')\n",
        "plt.title('PCA: 4D ‚Üí 2D Projection')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úì Classes well-separated in just 2 dimensions!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA and Covariance\n",
        "\n",
        "**PCA finds the directions of maximum variance by performing an eigen-decomposition of the covariance matrix.**\n",
        "\n",
        "**Variance (Population)**:\n",
        "\n",
        "* $\\mathrm{Var}(X) = \\mathbb{E}\\!\\left[(X - \\mu)^2\\right]$\n",
        "\n",
        "**Variance (Sample)**:\n",
        "\n",
        "* $s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2$\n",
        "\n",
        "**Covariance (Population)**:\n",
        "\n",
        "* $\\mathrm{Cov}(X, Y) = \\mathbb{E}\\!\\left[(X - \\mu_X)(Y - \\mu_Y)\\right]$\n",
        "\n",
        "\n",
        "### Step-by-step relationship\n",
        "\n",
        "### 1. Center the data\n",
        "\n",
        "Given a data matrix ($X \\in \\mathbb{R}^{n \\times d}$):\n",
        "\n",
        "$\n",
        "\\tilde{X} = X - \\mu\n",
        "$\n",
        "\n",
        "where ($\\mu$) is the mean of each feature.\n",
        "\n",
        "### 2. Compute the covariance matrix\n",
        "\n",
        "$\n",
        "\\Sigma = \\frac{1}{n-1} \\tilde{X}^\\top \\tilde{X}\n",
        "$\n",
        "\n",
        "* Diagonal entries ‚Üí variances of features\n",
        "* Off-diagonal entries ‚Üí covariances between features\n",
        "\n",
        "This matrix encodes **how features vary together**.\n",
        "\n",
        "### 3. Eigen-decomposition of covariance\n",
        "\n",
        "**Eigen-decomposition is the process of expressing a matrix in terms of its eigenvectors and eigenvalues. In PCA, the eigenvectors define the directions of the new features, and the eigenvalues quantify their importance by how much variance they explain. The covariance matrix determines the eigenvectors, which define the new features in PCA.**\n",
        "\n",
        "$\n",
        "\\Sigma v_i = \\lambda_i v_i\n",
        "$\n",
        "\n",
        "* (v_i) ‚Üí **principal components** (directions)\n",
        "* (\\lambda_i) ‚Üí **variance explained** by each component\n",
        "\n",
        "### 4. Interpretation\n",
        "\n",
        "* **Principal components are the eigenvectors of the covariance matrix**\n",
        "* **Eigenvalues rank components by importance**\n",
        "* PCA rotates the coordinate system to:\n",
        "\n",
        "  * maximize variance\n",
        "  * remove covariance (decorrelate features)\n",
        "\n",
        "After PCA, the new features are **uncorrelated**.\n",
        "\n",
        "### Geometric intuition\n",
        "\n",
        "* Covariance defines the **shape of the data cloud**\n",
        "* PCA finds the **axes of the ellipsoid**\n",
        "* The longest axis = first principal component\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C7R6a4IzjFZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Our PCI Eigenvector**\n",
        "\n",
        "$\n",
        "\\mathbf v_1=\n",
        "\\begin{bmatrix}\n",
        "0.5210659\\\n",
        "-0.2693474\\\n",
        "0.5804131\\\n",
        "0.5648565\n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "If you standardized the features first (as in `StandardScaler`), then ($\\Sigma$) is the covariance of the standardized data (very close to the correlation matrix). For Iris (standardized), it is:\n",
        "\n",
        "$$\n",
        "\\Sigma \\approx\n",
        "\\begin{bmatrix}\n",
        "1.006711 & -0.118359 & 0.877604 & 0.823431 \\\\\n",
        "-0.118359 & 1.006711 & -0.431316 & -0.368583 \\\\\n",
        "0.877604 & -0.431316 & 1.006711 & 0.969328 \\\\\n",
        "0.823431 & -0.368583 & 0.969328 & 1.006711\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "## Step 1: Use the eigenvector equation\n",
        "\n",
        "$\n",
        "\\Sigma \\mathbf v_1 = \\lambda_1 \\mathbf v_1\n",
        "$\n",
        "\n",
        "Compute the left side:\n",
        "\n",
        "$\n",
        "\\Sigma \\mathbf v_1 =\n",
        "\\begin{bmatrix}\n",
        "1.530936\\\n",
        "-0.791366\\\n",
        "1.705303\\\n",
        "1.659597\n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "## Step 2: Use the Rayleigh quotient to get the eigenvalue\n",
        "\n",
        "Because (\\mathbf v_1) is unit-length in PCA, the eigenvalue is:\n",
        "\n",
        "$\n",
        "\\lambda_1 = \\mathbf v_1^\\top \\Sigma \\mathbf v_1\n",
        "$\n",
        "\n",
        "So:\n",
        "\n",
        "$$\n",
        "\\lambda_1\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.5210659 & -0.2693474 & 0.5804131 & 0.5648565\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1.530936 \\\\\n",
        "-0.791366 \\\\\n",
        "1.705303 \\\\\n",
        "1.659597\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "Compute the dot product:\n",
        "\n",
        "* (0.5210659(1.530936)=0.797)\n",
        "* ((-0.2693474)(-0.791366)=0.213)\n",
        "* (0.5804131(1.705303)=0.990)\n",
        "* (0.5648565(1.659597)=0.937)\n",
        "\n",
        "Sum:\n",
        "\n",
        "$\n",
        "\\lambda_1 \\approx 0.797 + 0.213 + 0.990 + 0.937 = 2.937 \\approx 2.94\n",
        "$\n",
        "\n",
        "That‚Äôs the ‚Äú2.92-ish‚Äù value (the exact number varies slightly depending on rounding and whether you use (n) vs (n-1) in covariance).\n",
        "\n",
        "## How to print the exact value in Python\n",
        "\n",
        "```python\n",
        "pca.explained_variance_[0]          # eigenvalue for PC1\n",
        "# and equivalently:\n",
        "v1 = pca.components_[0]\n",
        "Sigma = np.cov(X_scaled, rowvar=False, ddof=1)\n",
        "v1 @ Sigma @ v1\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "pfj7HmOMy9Sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.explained_variance_[0]          # eigenvalue for PC1\n",
        "# and equivalently:\n",
        "v1 = pca.components_[0]\n",
        "Sigma = np.cov(X_scaled, rowvar=False, ddof=1)\n",
        "v1 @ Sigma @ v1"
      ],
      "metadata": {
        "id": "yqy7DkyJ0ZzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Eigenvalues\n",
        "\n",
        "$\n",
        "[2.94, 0.91, 0.15, 0.02]\n",
        "$\n",
        "\n",
        "* One eigenvalue per eigenvector (PC1‚ÄìPC4)\n",
        "* Computed from the covariance (or correlation) matrix\n",
        "* Each equals the **variance captured** along that PC direction\n",
        "\n",
        "\n",
        "## Why eigenvalues = importance in PCA\n",
        "\n",
        "In PCA, **importance is defined as variance explained**.\n",
        "\n",
        "So:\n",
        "\n",
        "* Larger eigenvalue ‚Üí more variance ‚Üí more information\n",
        "* Smaller eigenvalue ‚Üí little variance ‚Üí mostly noise\n",
        "\n",
        "That is why PCs are ordered by eigenvalue.\n",
        "\n",
        "\n",
        "## Explained variance ratio (the key interpretation)\n",
        "\n",
        "Total variance (for standardized Iris data):\n",
        "\n",
        "$\n",
        "\\sum \\lambda_i = 4\n",
        "$\n",
        "\n",
        "So the explained variance ratios are:\n",
        "\n",
        "$\n",
        "\\text{PC1: } \\frac{2.94}{4} \\approx 73%\n",
        "$\n",
        "$\n",
        "\\text{PC2: } \\frac{0.91}{4} \\approx 23%\n",
        "$\n",
        "$\n",
        "\\text{PC3: } \\frac{0.15}{4} \\approx 4%\n",
        "$\n",
        "$\n",
        "\\text{PC4: } \\frac{0.02}{4} < 1%\n",
        "$\n",
        "\n",
        "This tells you:\n",
        "\n",
        "* PC1 is overwhelmingly important\n",
        "* PC2 still meaningful\n",
        "* PC3, PC4 contribute very little\n",
        "\n",
        "## How this connects to loadings\n",
        "\n",
        "* Loadings tell you **what each PC is made of**\n",
        "* Eigenvalues tell you **how much that PC matters**\n",
        "\n",
        "Example:\n",
        "\n",
        "* PC1 loadings ‚Üí mostly petal length & width\n",
        "* PC1 eigenvalue = 2.94 ‚Üí explains ~73% of variance\n",
        "\n",
        "So you can say:\n",
        "\n",
        "> **Petal features dominate PC1, and PC1 explains most of the dataset‚Äôs variance.**\n",
        "\n",
        "---\n",
        "\n",
        "## What importance does *not* mean (important distinction)\n",
        "\n",
        "Eigenvalues do **not** mean:\n",
        "\n",
        "* Feature importance (like Random Forest)\n",
        "* Causal importance\n",
        "* Predictive power for a target variable\n",
        "\n",
        "They mean **importance for representing variance in the data**.\n",
        "\n",
        "## Teaching-safe summary\n",
        "\n",
        "You can safely say:\n",
        "\n",
        "> **Eigenvalues quantify the importance of each principal component by measuring how much variance it captures.**\n",
        "\n",
        "\n",
        "## One-line takeaway\n",
        "\n",
        "**Eigenvectors tell you the direction; eigenvalues tell you how much that direction matters.**\n",
        "\n"
      ],
      "metadata": {
        "id": "0oHPHgtaxtWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Covariance vs correlation PCA\n",
        "\n",
        "* If features are on **different scales**, PCA on covariance can be misleading\n",
        "* Standardizing features first is equivalent to doing PCA on the **correlation matrix**\n",
        "\n",
        "\n",
        "### Equivalent formulation (SVD)\n",
        "\n",
        "**SVD (Singular Value Decomposition) is a fundamental matrix factorization that expresses any matrix as a product of three simpler matrices. It is the backbone of PCA and many methods in data science.**\n",
        "\n",
        "PCA can also be computed via SVD:\n",
        "\n",
        "$\n",
        "\\tilde{X} = U \\Sigma V^\\top\n",
        "$\n",
        "\n",
        "* Columns of (V) = principal directions\n",
        "* Singular values relate to covariance eigenvalues\n",
        "\n",
        "This avoids explicitly computing the covariance matrix and is numerically stable.\n",
        "\n",
        "### Why this matters in practice\n",
        "\n",
        "* Highly correlated features ‚Üí strong covariance ‚Üí PCA compresses them\n",
        "* PCA is a **systematic way to remove redundancy**\n",
        "* This connects directly to your earlier correlation analysis on the Iris data\n",
        "\n",
        "\n",
        "### One-line takeaway\n",
        "\n",
        "**Covariance is the object PCA diagonalizes; PCA is the transformation that turns covariance into variance.**"
      ],
      "metadata": {
        "id": "9bFeHfvtyIQB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c3VuofidDiK"
      },
      "source": [
        "## 5.3 LDA (Linear Discriminant Analysis)\n",
        "\n",
        "## üéØ Interview Prep: Concepts Review\n",
        "**Context:** Utilizing class labels to find a feature space that maximizes group distinctness.\n",
        "\n",
        "---\n",
        "\n",
        "### üìö Core Concepts\n",
        "\n",
        "#### Key Difference from PCA: Supervised Objective\n",
        "While PCA is unsupervised and focuses on **variance preservation**, LDA is **supervised**. It seeks to project data into a lower-dimensional space that maximizes the distance between class means while minimizing the variance within each class.\n",
        "\n",
        "\n",
        "\n",
        "* **PCA:** Finds directions of maximum variation.\n",
        "* **LDA:** Finds directions that maximize class separability.\n",
        "\n",
        "#### Mathematical Limitation: Number of Components\n",
        "A critical constraint of LDA is its dimensionality limit. You can project the data onto at most:\n",
        "$$(n_{classes} - 1) \\text{ dimensions}$$\n",
        "\n",
        "**Example:** If you are classifying the Iris dataset (3 classes: Setosa, Versicolor, Virginica), LDA can only provide a maximum of **2** linear discriminants, regardless of how many input features you have.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "> **Interview Tip:** Mention that because LDA uses labels, it is often more effective than PCA for dimensionality reduction specifically intended to improve the performance of a classification model.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz999gqGdDiK"
      },
      "outputs": [],
      "source": [
        "# LDA\n",
        "lda = LDA(n_components=2)\n",
        "X_lda = lda.fit_transform(X_scaled, y_iris)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, species in enumerate(iris.target_names):\n",
        "    mask = y_iris == i\n",
        "    plt.scatter(X_lda[mask, 0], X_lda[mask, 1], label=species, alpha=0.6, s=50)\n",
        "\n",
        "plt.xlabel('LD1')\n",
        "plt.ylabel('LD2')\n",
        "plt.title('LDA: Supervised Dimensionality Reduction')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úì Even better class separation than PCA!\")\n",
        "print(\"  (Because LDA uses class labels)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUMRmji0dDiK"
      },
      "source": [
        "## 5.4 t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
        "\n",
        "## üéØ Interview Prep: Concepts Review\n",
        "**Context:** A non-linear dimensionality reduction technique optimized for visualizing high-dimensional data in 2D or 3D.\n",
        "\n",
        "\n",
        "### üìö Core Concepts\n",
        "\n",
        "#### Use Case: Visualization ONLY\n",
        "Unlike PCA or LDA, t-SNE is **not** a preprocessing tool for machine learning. It is used exclusively to explore and visualize the underlying structure of high-dimensional datasets.\n",
        "\n",
        "#### Strengths:\n",
        "* **Local Structure:** It is exceptionally good at keeping points that are close together in high-dimensional space close together in 2D.\n",
        "* **Non-Linear:** Can unravel complex manifolds that linear methods like PCA might miss.\n",
        "* **Clustering:** Naturally creates tight, well-separated visual clusters.\n",
        "\n",
        "#### Weaknesses:\n",
        "* **Computational Expense:** Much slower than PCA, especially as the number of data points increases ($O(N^2)$ or $O(N \\log N)$ depending on implementation).\n",
        "* **Hyperparameter Sensitivity:** Results (and even the existence of clusters) can vary wildly based on the **Perplexity** setting.\n",
        "* **No Transformation:** You cannot \"fit\" a t-SNE model and then \"transform\" new, incoming data points. You must re-run the algorithm on the entire combined dataset.\n",
        "\n",
        "\n",
        "> **Interview Tip:** If asked why we don't use t-SNE for training models, mention that it does not preserve global distances (the distance between two distant clusters is often meaningless) and the lack of a `transform()` method makes it impossible to use in a production inference pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95jFj-_8dDiL"
      },
      "outputs": [],
      "source": [
        "# t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "X_tsne = tsne.fit_transform(X_scaled)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, species in enumerate(iris.target_names):\n",
        "    mask = y_iris == i\n",
        "    plt.scatter(X_tsne[mask, 0], X_tsne[mask, 1], label=species, alpha=0.6, s=50)\n",
        "\n",
        "plt.xlabel('t-SNE Dimension 1')\n",
        "plt.ylabel('t-SNE Dimension 2')\n",
        "plt.title('t-SNE: Non-linear Dimensionality Reduction')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úì Creates very tight, well-separated clusters\")\n",
        "print(\"  Perfect for exploratory visualization!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdjDERsGdDiL"
      },
      "source": [
        "## Comparison: PCA vs LDA vs t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tyf94ZQPdDiL"
      },
      "outputs": [],
      "source": [
        "# Side-by-side comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "methods = [\n",
        "    (X_pca, 'PCA (Unsupervised, Linear)', 'PC1', 'PC2'),\n",
        "    (X_lda, 'LDA (Supervised, Linear)', 'LD1', 'LD2'),\n",
        "    (X_tsne, 't-SNE (Unsupervised, Non-linear)', 't-SNE 1', 't-SNE 2')\n",
        "]\n",
        "\n",
        "for ax, (X_reduced, title, xlabel, ylabel) in zip(axes, methods):\n",
        "    for i, species in enumerate(iris.target_names):\n",
        "        mask = y_iris == i\n",
        "        ax.scatter(X_reduced[mask, 0], X_reduced[mask, 1],\n",
        "                   label=species, alpha=0.6, s=40)\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypXM4UqPdDiL"
      },
      "source": [
        "# Summary: Key Interview Takeaways\n",
        "\n",
        "## üéØ Final Concept Review\n",
        "**Context:** Consolidating the core principles of feature engineering and dimensionality reduction for technical interviews.\n",
        "\n",
        "\n",
        "### üìö Feature Engineering\n",
        "* **Always scale** before PCA, KNN, SVM, and Neural Networks.\n",
        "* **Drop first category** in one-hot encoding to avoid the **dummy variable trap** (perfect multicollinearity).\n",
        "* **Understand missingness type** (MCAR/MAR/MNAR) before choosing an imputation strategy.\n",
        "\n",
        "\n",
        "\n",
        "### üîç Feature Selection\n",
        "* **Hierarchy:** **Filter** (fast, model-agnostic) ‚Üí **Wrapper** (slow, accurate) ‚Üí **Embedded** (balanced).\n",
        "* **Mutual Information** is your go-to for capturing **non-linear** relationships.\n",
        "* **Ensembles:** Random Forests and XGBoost provide feature importance \"for free\" during training.\n",
        "\n",
        "### üß¨ Feature Extraction\n",
        "* **Computer Vision:** CNNs extract features hierarchically (Early = edges, Deep = complex objects).\n",
        "* **NLP:** Transformers use the **Attention mechanism** for dynamic, contextual feature selection.\n",
        "\n",
        "### üìè Dimensionality & Reduction\n",
        "* **Curse of Dimensionality:** Leads to sparse data, meaningless distance metrics, and overfitting.\n",
        "* **PCA:** Unsupervised; maximizes **variance preservation**.\n",
        "* **LDA:** Supervised; maximizes **class separation**.\n",
        "* **t-SNE:** Non-linear; use for **visualization only** (never for preprocessing!).\n",
        "\n",
        "\n",
        "## ‚ùì Common Interview Questions\n",
        "\n",
        "**1. \"When would you use StandardScaler vs MinMaxScaler?\"**\n",
        "* **StandardScaler:** The default choice; centers data around 0 with unit variance. Use when features follow a Gaussian distribution.\n",
        "* **MinMaxScaler:** Scales to a fixed range (usually [0,1]). Preferred for Neural Networks or algorithms that require a specific bounded range.\n",
        "\n",
        "**2. \"What's the dummy variable trap?\"**\n",
        "* It occurs when independent variables are highly correlated (multicollinear). If you have $N$ categories and include all $N$ as dummy variables, the $N^{th}$ variable is perfectly predictable from the others.\n",
        "* **Solution:** Use `drop_first=True`.\n",
        "\n",
        "**3. \"Difference between PCA and LDA?\"**\n",
        "* **PCA** is unsupervised and focuses on the \"spread\" of the data regardless of labels.\n",
        "* **LDA** is supervised and focuses on finding a path that makes classes as distinct as possible.\n",
        "\n",
        "\n",
        "\n",
        "**4. \"Why can't you use t-SNE for preprocessing?\"**\n",
        "* It is **non-deterministic** (results change with the seed).\n",
        "* It lacks a `.transform()` method for new data.\n",
        "* It does not preserve **global structure** (distances between distant clusters are arbitrary).\n",
        "\n",
        "**5. \"How do you handle missing data?\"**\n",
        "* **Simple:** Mean/Median/Mode (only for MCAR).\n",
        "* **Advanced:** KNN Imputation or MICE (Multiple Imputation by Chained Equations) to preserve relationships between variables.\n",
        "\n",
        "\n",
        "### üöÄ Next Steps\n",
        "* **Code:** Practice with `interview_prep_simulation_scenarios_4.ipynb`.\n",
        "* **Theory:** Review the `Interview_Prep_4_Terms_and_Concepts.pdf`.\n",
        "* **Focus:** Practice explaining the **\"why\"** (the trade-offs), not just the \"how\" (the code).\n",
        "\n",
        "**Remember:** In interviews, demonstrating that you know *when* a technique will fail is just as important as knowing how to implement it!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}